<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="强化学习, 方寸田园">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>强化学习 | 方寸田园</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="方寸田园" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>




<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">方寸田园</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">方寸田园</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/wanc97" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/wanc97" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/image/2022-09-29-18-04-39.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">强化学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%99%BA%E8%83%BD%E4%BD%93/">
                                <span class="chip bg-color">智能体</span>
                            </a>
                        
                            <a href="/tags/%E4%BB%B7%E5%80%BC/">
                                <span class="chip bg-color">价值</span>
                            </a>
                        
                            <a href="/tags/%E7%AD%96%E7%95%A5/">
                                <span class="chip bg-color">策略</span>
                            </a>
                        
                            <a href="/tags/%E6%A8%A1%E5%9E%8B/">
                                <span class="chip bg-color">模型</span>
                            </a>
                        
                            <a href="/tags/%E7%8E%AF%E5%A2%83/">
                                <span class="chip bg-color">环境</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                机器学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-08-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2022-10-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    62 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="强化学习基础"><a href="#强化学习基础" class="headerlink" title="强化学习基础"></a>强化学习基础</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>强化学习研究的是智能体在复杂、不确定的环境中最大化它能获得的奖励。<br>强化学习研究的是多序列决策问题。<br>状态满足可重复到达。<br><img src='/medias/image/2022-09-04-15-52-14.png' width="60%"></p>
<p>智能体与环境之间交互，并在这个过程中学习。具体的，智能体观察环境的状态，对环境发出动作，并从环境获得奖励且再次观察环境的状态。<br><img src='/medias/image/screenshot_1661425319742.png' width="60%"><br>智能体一般包含一个或多个：</p>
<ul>
<li>策略：根据策略选取下一步动作，也是最终所需要的</li>
<li>价值函数：对当前状态或状态动作对进行评估，是智能体对环境好坏的理解</li>
<li>模型：环境的状态转移及奖励函数，智能体对环境处境和变化的理解</li>
</ul>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略是智能体的动作模型，它其实是一个函数，决定了智能体的动作。<br>策略可以分为两种：</p>
<ul>
<li>随机性策略：就是 $\pi$， 即：$\pi(a|s)=P(a_t=a|s_t=s)$，输入一个状态$s$，输出各个动作的概率，通过采样可以得到具体采用的动作。</li>
<li>确定性（deterministic）策略：函数直接输出一个动作，可以看成$a^*=argmax_a\pi(a|s)$</li>
</ul>
<p>随机性策略具有许多优点，例如：随机性可以更好地探索环境；随机性带来的动作多样性有利于多智能体博弈，防止策略被对手预测。<br><img src='/medias/image/screenshot_1661425437051.png' width="60%"><br>基于策略的走迷宫，每一个状态得到动作</p>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>价值函数用于评估状态的好坏，反映我们可以得到的奖励的量，其中未来的奖励会通过一个折扣因子$γ$进行加权。</p>
<p>$$<br>V_π(s) = E_π[G_t|s_t=s]=E_π[\sum_{k=0}^∞γ^k_{t+k+1}|s_t=s]<br>$$</p>
<p>这种价值函数的输入只有状态，另一种价值函数叫Q函数，是将状态和动作共同作为输入：</p>
<p>$$<br>Q_π(s,a) = E_π[G_t|s_t=s,a_t=a]=E_π[\sum_{k=0}^∞γ^k_{t+k+1}|s_t=s,a_t=a]<br>$$</p>
<p>即可以获得的奖励是由当前状态和采取的动作共同决定的。<br><img src='/medias/image/screenshot_1661425532804.png' width="60%"><br>基于价值的走迷宫，会得到各个状态（和动作）的价值</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型决定了下一步的状态，由状态转移概率和奖励函数组成，其中状态转移概率为：</p>
<p>$p_{s s^{\prime}}^{a}=p(s_{t+1}=s^{\prime} | s_{t}=s, a_{t}=a)$<br>奖励函数为：<br>$R(s,a)=E[r_{t+1} | s_{t}=s, a_{t}=a]$</p>
<h3 id="On-policy-VS-Off-policy"><a href="#On-policy-VS-Off-policy" class="headerlink" title="On-policy VS Off-policy"></a>On-policy VS Off-policy</h3><ul>
<li>  如果要学习的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做<code>on-policy(同策略)</code>。</li>
<li>  如果要学习的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做<code>off-policy(异策略)</code>。</li>
</ul>
<p>比较拟人化的讲法是如果要学习的那个 agent，一边跟环境互动，一边做学习这个叫on-policy。 如果它在旁边看别人玩，通过看别人玩来学习的话，这个叫做 off-policy。</p>
<ul>
<li>on-policy稳定，容易陷入局部最优</li>
<li>off-policy探索性更好，但不过程严格，收敛慢，容易奔溃</li>
</ul>
<p>off-policy的不严格体现在时序差分的优化上：</p>
<ul>
<li>根据贝尔曼方程：$V(s)=R(s)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime} | s) V(s^{\prime})$；</li>
<li>当采用确定的策略时转移关系固定：$V(s)=R(s)+\gamma V(s^{\prime})$；</li>
<li>而off-policy采取的策略不一样，会使得下一个状态$s^{\prime}$不一致；</li>
<li>进而贝尔曼方程的更新就不严格。</li>
<li>在采用随机性策略时，也可以得到类似推导，即：off-policy的动作概率分布不一致。</li>
</ul>
<p>但是对于使用q值来更新的算法，off-policy则可以规避这个问题：</p>
<ul>
<li>根据q值的贝尔曼方程：$Q(s,a)=R(s,a)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime} | s,a) V(s^{\prime})$；</li>
<li>当状态和动作可以得到确定下一状态时：$Q(s,a)=R(s,a)+\gamma V(s^{\prime})$；</li>
<li>因为更新的$Q(s,a)$就是针对这一动作的更新，而不是针对$V(s)$更新；</li>
<li>所以这一过程在off-policy下是严谨的。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_on_policy_agent</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> agent<span class="token punctuation">,</span> num_episodes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    return_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> tqdm<span class="token punctuation">(</span>total<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>num_episodes<span class="token operator">/</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> desc<span class="token operator">=</span><span class="token string">'Iteration %d'</span> <span class="token operator">%</span> i<span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span>
            <span class="token keyword">for</span> i_episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>num_episodes<span class="token operator">/</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                episode_return <span class="token operator">=</span> <span class="token number">0</span>
                transition_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'states'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'actions'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'next_states'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'rewards'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'dones'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span>
                state <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
                done <span class="token operator">=</span> <span class="token boolean">False</span>
                <span class="token keyword">while</span> <span class="token keyword">not</span> done<span class="token punctuation">:</span>
                    action <span class="token operator">=</span> agent<span class="token punctuation">.</span>take_action<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
                    next_state<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> _ <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>next_state<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>reward<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>done<span class="token punctuation">)</span>
                    state <span class="token operator">=</span> next_state
                    episode_return <span class="token operator">+=</span> reward
                return_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>episode_return<span class="token punctuation">)</span>
                agent<span class="token punctuation">.</span>update<span class="token punctuation">(</span>transition_dict<span class="token punctuation">)</span>
                <span class="token keyword">if</span> <span class="token punctuation">(</span>i_episode<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'episode'</span><span class="token punctuation">:</span> <span class="token string">'%d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_episodes<span class="token operator">/</span><span class="token number">10</span> <span class="token operator">*</span> i <span class="token operator">+</span> i_episode<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'return'</span><span class="token punctuation">:</span> <span class="token string">'%.3f'</span> <span class="token operator">%</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>return_list<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
                pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> return_list

<span class="token keyword">def</span> <span class="token function">train_off_policy_agent</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> agent<span class="token punctuation">,</span> num_episodes<span class="token punctuation">,</span> replay_buffer<span class="token punctuation">,</span> minimal_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    return_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> tqdm<span class="token punctuation">(</span>total<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>num_episodes<span class="token operator">/</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> desc<span class="token operator">=</span><span class="token string">'Iteration %d'</span> <span class="token operator">%</span> i<span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span>
            <span class="token keyword">for</span> i_episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>num_episodes<span class="token operator">/</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                episode_return <span class="token operator">=</span> <span class="token number">0</span>
                state <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
                done <span class="token operator">=</span> <span class="token boolean">False</span>
                <span class="token keyword">while</span> <span class="token keyword">not</span> done<span class="token punctuation">:</span>
                    action <span class="token operator">=</span> agent<span class="token punctuation">.</span>take_action<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
                    next_state<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> _ <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
                    replay_buffer<span class="token punctuation">.</span>add<span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> done<span class="token punctuation">)</span>
                    state <span class="token operator">=</span> next_state
                    episode_return <span class="token operator">+=</span> reward
                    <span class="token keyword">if</span> replay_buffer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> minimal_size<span class="token punctuation">:</span>
                        b_s<span class="token punctuation">,</span> b_a<span class="token punctuation">,</span> b_r<span class="token punctuation">,</span> b_ns<span class="token punctuation">,</span> b_d <span class="token operator">=</span> replay_buffer<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
                        transition_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'states'</span><span class="token punctuation">:</span> b_s<span class="token punctuation">,</span> <span class="token string">'actions'</span><span class="token punctuation">:</span> b_a<span class="token punctuation">,</span> <span class="token string">'next_states'</span><span class="token punctuation">:</span> b_ns<span class="token punctuation">,</span> <span class="token string">'rewards'</span><span class="token punctuation">:</span> b_r<span class="token punctuation">,</span> <span class="token string">'dones'</span><span class="token punctuation">:</span> b_d<span class="token punctuation">&#125;</span>
                        agent<span class="token punctuation">.</span>update<span class="token punctuation">(</span>transition_dict<span class="token punctuation">)</span>
                return_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>episode_return<span class="token punctuation">)</span>
                <span class="token keyword">if</span> <span class="token punctuation">(</span>i_episode<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'episode'</span><span class="token punctuation">:</span> <span class="token string">'%d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_episodes<span class="token operator">/</span><span class="token number">10</span> <span class="token operator">*</span> i <span class="token operator">+</span> i_episode<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'return'</span><span class="token punctuation">:</span> <span class="token string">'%.3f'</span> <span class="token operator">%</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>return_list<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
                pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> return_list<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="与监督学习的区别"><a href="#与监督学习的区别" class="headerlink" title="与监督学习的区别"></a>与监督学习的区别</h3><ul>
<li>监督学习的数据是独立同分布的，而强化学习是存在前后关联的序列数据</li>
<li>强化学习的奖励是延迟的</li>
</ul>
<h2 id="智能体类型"><a href="#智能体类型" class="headerlink" title="智能体类型"></a>智能体类型</h2><h3 id="基于策略（policy-based）"><a href="#基于策略（policy-based）" class="headerlink" title="基于策略（policy-based）"></a>基于策略（policy-based）</h3><p>直接学习策略，输入环境状态，输出对应动作（的概率）。</p>
<blockquote>
<p>常见算法：Policy Gradient</p>
</blockquote>
<h3 id="基于价值（value-based）"><a href="#基于价值（value-based）" class="headerlink" title="基于价值（value-based）"></a>基于价值（value-based）</h3><p>直接学习价值函数，隐式学习策略。通过价值函数可以推算策略。</p>
<blockquote>
<p>基于价值的方法需要维护一个价值表格或价值函数，并通过价值表格或价值函数来选取价值最大的动作，这种迭代只能运用在离散的环境下，不适合动作集合庞大、动作连续的场景。<br>基于价值相比基于策略训练起来效果更好，更平稳，因为只要训练好Q函数，就可以保证策略是最好的，而训练Q函数是一个单纯的回归问题。<br>常见算法：Q-learning，Sarsa</p>
</blockquote>
<h3 id="演员-评论员（actor-critic）"><a href="#演员-评论员（actor-critic）" class="headerlink" title="演员-评论员（actor-critic）"></a>演员-评论员（actor-critic）</h3><p>既学习策略，又学习价值函数。</p>
<blockquote>
<p>根据策略做出动作，同时根据价值函数给出的价值来加速学习过程。</p>
</blockquote>
<h3 id="基于模型"><a href="#基于模型" class="headerlink" title="基于模型"></a>基于模型</h3><p>通过建立模型学习状态转移来采取行动。</p>
<blockquote>
<p>免模型方法一般采用数据驱动方法，当学习过程较为困难且有一定先验知识时，可以通过建立模型来降低难度，提升泛化效果，缓解数据稀疏等问题。</p>
</blockquote>
<img src='/medias/image/screenshot_1661487116676.png' width="60%">

<h3 id="算法类型"><a href="#算法类型" class="headerlink" title="算法类型"></a>算法类型</h3><p>不同算法有不同的适用场景：</p>
<table>
<thead>
<tr>
<th>动作空间</th>
<th>算法</th>
</tr>
</thead>
<tbody><tr>
<td>离散</td>
<td>Sarsa、Q-learning、DQN</td>
</tr>
<tr>
<td>连续</td>
<td>DDPG、TD3</td>
</tr>
<tr>
<td>both</td>
<td>PG、AC、A2C、TRPO、PPO、SAC</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>优化策略</th>
<th>算法</th>
</tr>
</thead>
<tbody><tr>
<td>在线</td>
<td>Sarsa、PG、AC、A2C、TRPO、PPO</td>
</tr>
<tr>
<td>离线</td>
<td>Q-learning、DQN、DDPG、TD3、SAC</td>
</tr>
</tbody></table>
<ul>
<li>注：此处的依据是是否更新Q值，是否可以严谨地使用历史数据；</li>
</ul>
<table>
<thead>
<tr>
<th>策略</th>
<th>算法</th>
</tr>
</thead>
<tbody><tr>
<td>随机性</td>
<td>PG、AC、A2C、TRPO、PPO</td>
</tr>
<tr>
<td>确定性</td>
<td>DDPG</td>
</tr>
</tbody></table>
<h2 id="探索-amp-利用"><a href="#探索-amp-利用" class="headerlink" title="探索&amp;利用"></a>探索&amp;利用</h2><h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><ul>
<li><p>探索：尝试不同的动作来得到最佳策略，不一定能获得最优收益</p>
</li>
<li><p>利用：采用已知可以得到很大奖励的动作</p>
</li>
<li><p>学习：在未知的环境中，通过学习与环境交互来改进策略</p>
</li>
<li><p>规划：基于当前状态规划并寻找最优动作</p>
</li>
</ul>
<h3 id="探索方式"><a href="#探索方式" class="headerlink" title="探索方式"></a>探索方式</h3><ul>
<li>添加噪声：$\epsilon-greedy$</li>
<li>积极初始化（optimistic initialization）：给各个动作一个较高的初始期待，增加智能体探索未使用动作的概率。</li>
<li>基于不确定性的度量：尝试具有不确定收益的策略，可能带来更高的收益<ul>
<li>上置信界法UCB：探索少的动作，不确定性高，可能的潜在收益高，可以将这个分数加到价值函数上。</li>
<li>汤普森采样</li>
</ul>
</li>
<li>概率匹配：随机性策略，根据概率采样</li>
</ul>
<h2 id="强化学习实验"><a href="#强化学习实验" class="headerlink" title="强化学习实验"></a>强化学习实验</h2><h3 id="gym"><a href="#gym" class="headerlink" title="gym"></a>gym</h3><a href="https://www.gymlibrary.dev/" title="" target="">gym</a>是由openAI开源的一个强化学习库，里面包含了很多的环境（游戏）

<p><strong>常见用法</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> gym
env <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">'环境名'</span><span class="token punctuation">)</span> <span class="token comment"># 构建实验环境</span>
observation <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 重置并获得初始状态</span>
<span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment"># 显示图形界面</span>
    action <span class="token operator">=</span> agent<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>
    observation<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>   <span class="token comment"># 提交动作并返回观察、奖励等信息</span>
env<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 关闭环境</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="stable-baselines3"><a href="#stable-baselines3" class="headerlink" title="stable_baselines3"></a>stable_baselines3</h3><p><strong>常见用法</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> gym
<span class="token keyword">from</span> stable_baselines3 <span class="token keyword">import</span> PPO
<span class="token keyword">from</span> stable_baselines3<span class="token punctuation">.</span>common<span class="token punctuation">.</span>env_util <span class="token keyword">import</span> make_atari_env<span class="token punctuation">,</span>make_vec_env
<span class="token keyword">from</span> stable_baselines3<span class="token punctuation">.</span>common<span class="token punctuation">.</span>atari_wrappers <span class="token keyword">import</span> AtariWrapper

env <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">"ALE/BattleZone-ram-v5"</span><span class="token punctuation">)</span>
<span class="token comment"># env = AtariWrapper("ALE/BattleZone-v5")</span>
<span class="token comment"># env = make_vec_env(env, n_envs=20)</span>
<span class="token comment"># env = make_atari_env("ALE/BattleZone-ram-v5", n_envs=20)</span>

model <span class="token operator">=</span> PPO<span class="token punctuation">(</span><span class="token string">"MlpPolicy"</span><span class="token punctuation">,</span> env<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># model.set_parameters("ppo_ALE/BattleZone-v5-1")</span>
model<span class="token punctuation">.</span>learn<span class="token punctuation">(</span>total_timesteps<span class="token operator">=</span><span class="token number">2000000</span><span class="token punctuation">,</span> log_interval<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"ppo_ALE/BattleZone-ram-v5-1"</span><span class="token punctuation">)</span>

obs <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
    action<span class="token punctuation">,</span> _states <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>obs<span class="token punctuation">,</span> deterministic<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    obs<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
    env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span>
    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> done<span class="token punctuation">:</span>
        <span class="token keyword">break</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="RAY-amp-rllib"><a href="#RAY-amp-rllib" class="headerlink" title="RAY&amp;rllib"></a>RAY&amp;rllib</h3><h4 id="RAY"><a href="#RAY" class="headerlink" title="RAY"></a>RAY</h4><p>以非常简洁的方式实现多进程、进程间通信、资源控制等<br><strong>多进程</strong>：<br><img src='/medias/image/2022-09-28-17-59-31.png' width="30%"><br><img src='/medias/image/2022-09-28-18-00-04.png' width="30%"><br>只需要通过装饰器对普通函数进行修饰，并在调用时加入remote，就可以新开后台进程执行</p>
<h4 id="tune"><a href="#tune" class="headerlink" title="tune"></a>tune</h4><p>基于RAY的调参工具，自动化，简洁</p>
<h4 id="RLLib"><a href="#RLLib" class="headerlink" title="RLLib"></a>RLLib</h4><p>基于RAY和tune实现强化学习</p>
<h2 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h2><p>智能体与环境交互，观察环境状态、采取动作改变环境，然后得到奖励并进一步观察环境状态的过程可以通过马尔科夫决策过程（MDP）来表示</p>
<h3 id="马尔科夫过程（MP）"><a href="#马尔科夫过程（MP）" class="headerlink" title="马尔科夫过程（MP）"></a>马尔科夫过程（MP）</h3><h4 id="马尔科夫性质"><a href="#马尔科夫性质" class="headerlink" title="马尔科夫性质"></a>马尔科夫性质</h4><p>随机过程中，在给定过去和现在状态的情况下，随机变量未来状态的条件概率分布仅依赖当前状态。<br>核心是：<strong>只取决于现在，而与过去独立</strong>：<br>$$<br> p(X_{t+1}=x_{t+1} |X_{0:t}=x_{0: t})=p(X_{t+1}=x_{t+1} |X_{t}=x_{t})<br>$$</p>
<blockquote>
<p>如果过程不满足马尔科夫性质，可以使用循环神经网络、注意力机制等</p>
</blockquote>
<h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><p>一组随机变量序列，满足马尔科夫性质，则称为<strong>马尔科夫过程</strong>。<br>离散时间的马尔科夫过程称为<strong>马尔科夫链</strong>：<br><img src='/medias/image/screenshot_1661743038453.png' width="60%"><br>因为每个状态转移只取决于一个状态，所以马尔科夫链的转移过程可以用状态转移矩阵来表示。</p>
<h3 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h3><ul>
<li>马尔科夫奖励过程是在马尔科夫链的基础上增加了奖励函数R。</li>
<li>奖励函数表示当达到某一个状态的时候可以获得多大的奖励。</li>
<li>当状态数有限的时候，R可以是一个向量。</li>
</ul>
<h4 id="回报与价值函数"><a href="#回报与价值函数" class="headerlink" title="回报与价值函数"></a>回报与价值函数</h4><p><strong>范围</strong>：一个回合的长度<br><strong>回报</strong>：奖励的逐步叠加<br>$$<br>G_{t}=r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\gamma^{3} r_{t+4}+\ldots+\gamma^{T-t-1} r_{T}<br>$$<br><strong>状态价值函数</strong>：回报的期望<br>$$<br>V^{t}(s) =E[G_{t} | s_{t}=s]<br>=E[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots+\gamma^{T-t-1} r_{T} | s_{t}=s]<br>$$</p>
<h4 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h4><p>$$<br>V(s)=R(s)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime} | s) V(s^{\prime})<br>$$<br>通过贝尔曼方程，可以将当下状态的价值表示成当下状态的奖励和转移状态的价值组成。<br>转移状态的价值需要加上一个折扣系数和转移概率。</p>
<h4 id="迭代计算马尔科夫奖励过程的价值"><a href="#迭代计算马尔科夫奖励过程的价值" class="headerlink" title="迭代计算马尔科夫奖励过程的价值"></a>迭代计算马尔科夫奖励过程的价值</h4><h5 id="蒙特卡洛法"><a href="#蒙特卡洛法" class="headerlink" title="蒙特卡洛法"></a>蒙特卡洛法</h5><p>产生大量的轨迹，计算回报的均值<br><img src='/medias/image/screenshot_1661745146222.png' width="60%"></p>
<h5 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h5><p>对于所有状态，不断通过贝尔曼方程进行更新迭代，直至状态的价值收敛<br><img src='/medias/image/screenshot_1661765542382.png' width="60%"></p>
<h3 id="马尔科夫决策过程-1"><a href="#马尔科夫决策过程-1" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h3><p>在马尔科夫奖励过程的基础上增加了决策（动作）<br>在状态转移方面增加了一个条件，变成了：<br>$$<br>p(s_{t+1}=s^{\prime} | s_{t}=s,a_{t}=a)<br>$$<br>不仅取决于状态，还取决于动作。<br>同时奖励函数也多了一个形式：<br>$$<br>R(s_{t}=s, a_{t}=a)=E[r_{t} | s_{t}=s, a_{t}=a]<br>$$<br>即动作和状态共同影响可能得到的奖励</p>
<h4 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a>策略</h4><p>动作由策略基于状态得到：<br>$$<br>\pi(a |s)=p(a_{t}=a | s_{t}=s)<br>$$<br>当策略确定的时候，每个状态会采取的动作概率一定，此时马尔科夫决策过程可以退化为马尔科夫奖励过程：<br>$$<br>P_{\pi}(s^{\prime} | s)=\sum_{a \in A} \pi(a | s) p(s^{\prime} | s, a)<br>$$<br>$$<br>r_{\pi}(s)=\sum_{a \in A} \pi(a | s) R(s, a)<br>$$</p>
<h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><img src='/medias/image/screenshot_1661766803148.png' width="60%">
马尔科夫过程和马尔科夫奖励过程可以看成是一种静态的情况，概率转移、奖励、价值都是固定的。
而马尔科夫决策过程增加了决策环节，智能体通过动作影响了状态的概率转移，进而影响了状态的价值。因此马尔科夫决策过程具有动态的特性，智能体可以改变环境，争取奖励最大化。

<h4 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a>Q函数</h4><p>马尔科夫决策过程中的价值函数需要考虑动作，即<strong>动作价值函数</strong>，又称<strong>Q函数</strong>：<br>$$<br>Q_{\pi}(s, a)=E_{\pi}[G_{t} |s_{t}=s, a_{t}=a]<br>$$<br>通过对Q函数中的动作进行加权，可以得到<strong>价值函数</strong>：<br>$$<br>V_{\pi}(s)=\sum_{a \in A} \pi(a | s) Q_\pi(s, a)<br>$$<br><strong>贝尔曼方程</strong>：<br>$$<br>Q(s,a)=R(s,a)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime} | s,a) V(s^{\prime})<br>$$</p>
<h4 id="最佳策略搜索"><a href="#最佳策略搜索" class="headerlink" title="最佳策略搜索"></a>最佳策略搜索</h4><p>就是找到一个最大的价值函数，使得：<br>$$<br>\pi^{*}(s)=\underset{\pi}{\arg \max } ~ V_{\pi}(s)<br>$$<br>常用策略有策略迭代和价值迭代</p>
<h4 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h4><ol>
<li>根据策略计算价值函数和Q函数；</li>
<li>最大化Q函数来优化策略。</li>
</ol>
<img src='/medias/image/screenshot_1661829627420.png' width="60%">
$$
Q_{\pi_{i}}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime} \mid s, a) V_{\pi_{i}}(s^{\prime})
$$
$$
\pi_{i+1}(s)=\underset{a}{\arg \max } ~Q_{\pi_{i}}(s, a)
$$
<img src='/medias/image/screenshot_1661829943836.png' width="60%">

<p><strong>Q学习迭代</strong>：</p>
<p>$$<br>Q^*(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime} | s, a) \max_{a^{\prime}} Q^*(s^{\prime}, a^{\prime})<br>$$</p>
<h4 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h4><ol>
<li>初始化价值；</li>
<li>根据价值计算Q函数；</li>
<li>最大化Q函数来得到价值<br>$$<br>Q_{k+1}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime} | s, a) V_{k}(s^{\prime})<br>$$<br>$$<br>V_{k+1}(s)=\max_{a} Q_{k+1}(s, a)<br>$$</li>
</ol>
<p>策略迭代是在价值函数收敛后更新策略，而价值迭代在每一次价值函数更新后都进行策略提升，效率更高。</p>
<h1 id="基于价值"><a href="#基于价值" class="headerlink" title="基于价值"></a>基于价值</h1><h2 id="表格型方法"><a href="#表格型方法" class="headerlink" title="表格型方法"></a>表格型方法</h2><p>通过一个表格记录每一种状态和对应动作可以得到的奖励，奖励包括直接奖励和折扣的未来奖励。<br>常见的表格型方法包括：蒙特卡洛、Q学习、Sarsa<br>其中蒙特卡罗法是按回合更新，而Q学习、Sarsa是按单步更新</p>
<h3 id="差分时序"><a href="#差分时序" class="headerlink" title="差分时序"></a>差分时序</h3><p><strong>强化</strong>：可以通过下一个状态的价值来更新当前状态的价值。这种单步更新的方法又称时序差分法。<br>时序差分目标：<br>$$<br>r_{t+1}+\gamma V(s_{t+1})<br>$$<br>时序差分误差：<br>$$<br>\delta=r_{t+1}+\gamma V(s_{t+1})-V(s_{t})<br>$$<br>时序差分法：<br>$$<br>V(s_{t}) \leftarrow V(s_{t})+\alpha(r_{t+1}+\gamma V(s_{t+1})-V(s_{t}))<br>$$</p>
<p>把时序差分方法进行进一步的推广，之前是只往前走一步，即<code>TD(1)</code>。<br>可以调整步数（step），变成<code>n步时序差分（n-step TD）</code>。比如<code>TD(2)</code>，即往前走两步，利用两步得到的回报，使用自举来更新状态的价值。<br><img src='/medias/image/screenshot_1661841461788.png' width="60%"></p>
<p>$$<br>n=1   \qquad G_{t}^{(1)}=r_{t+1}+\gamma V(s_{t+1})\\<br>n=2  \qquad  G_{t}^{(2)}= r_{t+1}+\gamma r_{t+2}+\gamma^{2} V(s_{t+2}) \\<br>\vdots \\<br>n=\infty \qquad G_{t}^{\infty}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_{T}<br>$$<br>如果时序差分方法需要更广度的更新，就变成了 动态规划方法（因为动态规划方法是把所有状态都考虑进去来进行更新）。<br>如果时序差分方法需要更深度的更新，就变成了蒙特卡洛方法。<br>图右下角是穷举搜索的方法（exhaustive search），穷举搜索的方法不仅需要很深度的信息，还需要很广度的信息。<br><img src='/medias/image/screenshot_1661841906623.png' width="60%"></p>
<h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h3><p>将差分时序中的价值函数改为了Q函数：<br>$$<br>Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t})+\alpha[r_{t+1}+\gamma Q(s_{t+1}, a_{t+1})-Q(s_{t}, a_{t})]<br>$$<br><img src='/medias/image/screenshot_1661854222340.png' width="100%"></p>
<p>流程：</p>
<ol>
<li>根据Q表格选择并输出动作；</li>
<li>获取Sarsa五元组，更新Q表格。</li>
</ol>
<img src='/medias/image/screenshot_1661854674324.png' width="100%">

<p>但是在Q表格更新后，$V(s_{t+1})$不一定等于$Q(s_{t+1}, a_{t+1})$；<br>所以Sarsa不能在off-policy场景下使用历史数据，只能在on-policy下用当下数据</p>
<h3 id="Q学习"><a href="#Q学习" class="headerlink" title="Q学习"></a>Q学习</h3><p>将Sarsa的时序差分目标改为了$r_{t+1}+\gamma\max_a Q(s_{t+1}, a)$：<br>$$<br>Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t})+\alpha[r_{t+1}+\gamma\max_a Q(s_{t+1}, a)-Q(s_{t}, a_{t})]<br>$$<br>这种调整使得Q学习可以使用历史数据。<br>Q学习是异策略的时序差分学习方法，Sarsa是同策略的时序差分学习方法。 Sarsa在更新 Q 表格的时候，它用到的是 $A’$ 。我们要获取下一个 Q 值的时候，$A’$是下一个步骤一定会执行的动作，这个动作有可能是$\varepsilon$-贪心方法采样出来的动作，也有可能是最大化 Q 值对应的动作，也有可能是随机动作，但这是它实际执行的动作。 但是 Q学习在更新 Q 表格的时候，它用到的是 Q 值 $Q(S’,a)$ 对应的动作 ，它不一定是下一个步骤会执行的实际的动作，因为我们下一个实际会执行的那个动作可能会探索。 Q学习默认的下一个动作不是通过行为策略来选取的，Q学习直接看Q表格，取它的最大化的值，它是默认 $A’$ 为最佳策略选取的动作，所以 Q学习在学习的时候，不需要传入 $A’$，即 $a_{t+1}$ 的值。<br><img src='/medias/image/screenshot_1661857748609.png' width="100%"></p>
<img src='/medias/image/screenshot_1661858063900.png' width="100%">

<p>直观来说，Q学习使用选取最大值的最大化操作，因此更加激进一些，而Sarsa则较为保守。</p>
<h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><h3 id="概念-2"><a href="#概念-2" class="headerlink" title="概念"></a>概念</h3><h4 id="价值函数近似"><a href="#价值函数近似" class="headerlink" title="价值函数近似"></a>价值函数近似</h4><p>传统的强化学习通过表格来记录价值函数或动作价值函数，这样有局限：</p>
<ul>
<li>状态空间较多无法存储；</li>
<li>连续状态空间无法存储；</li>
<li>泛化能力不足。</li>
</ul>
<p>可以通过<strong>Q网络</strong>进行<strong>价值函数近似</strong>：<br>$$<br>Q_{\phi}(\boldsymbol{s}, \boldsymbol{a}) \approx Q_{\pi}(\boldsymbol{s}, \boldsymbol{a})<br>$$<br>其中$Q_{\phi}(\boldsymbol{s}, \boldsymbol{a})$是一个参数为$\phi$的函数，比如神经网络，其输出为一个实数。</p>
<p><strong>深度Q网络</strong>（deep Q-network，<strong>DQN</strong>）是指基于深度学习的Q学习算法，主要结合了价值函数近似与神经网络技术，并采用目标网络和经历回放的方法进行网络的训练。</p>
<h4 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h4><img src='/medias/image/screenshot_1662013290118.png' width="60%">
状态价值函数输入状态，得到这个状态的价值。
有两种方法：蒙特卡罗法和时序差分法

<h5 id="蒙特卡罗法"><a href="#蒙特卡罗法" class="headerlink" title="蒙特卡罗法"></a>蒙特卡罗法</h5><img src='/medias/image/screenshot_1662013363393.png' width="60%">
蒙特卡罗法在一个回合的游戏结束后才能知道每个状态的价值，并更新网络。
一方面比较慢；另一方面方差较大，因为累计奖励是很多奖励的和，每一个状态都具有随机性。

<h5 id="时序差分法"><a href="#时序差分法" class="headerlink" title="时序差分法"></a>时序差分法</h5><img src='/medias/image/screenshot_1662013484348.png' width="60%">

<p>时序差分法在每一步就可以更新，让两个状态的差等于所获得的奖励，速度更快，且方差更小</p>
<h4 id="动作价值函数"><a href="#动作价值函数" class="headerlink" title="动作价值函数"></a>动作价值函数</h4><p>与状态价值函数类似的还有动作价值函数（Q函数），动作价值函数的输入是一个状态-动作对，其指在某一个状态采取某一个动作，假设我们都使用策略π，得到的累积奖励的期望值有多大。<br>Q函数有一个需要注意的问题是，策略$ \pi$在看到状态s的时候，它采取的动作不一定是a。Q函数假设在状态s强制采取动作a，而不管我们现在考虑的策略$ \pi$会不会采取动作a，这并不重要。在状态s强制采取动作a。接下来都用策略$ \pi$继续玩下去，就只有在状态s，我们才强制一定要采取动作a，接下来就进入自动模式，让策略$ \pi$继续玩下去，得到的期望奖励才是$ Q_{\pi}(s,a)$。<br><img src='/medias/image/screenshot_1662014401773.png' width="60%"></p>
<p>Q函数有两种写法：</p>
<ol>
<li>输入是状态与动作，输出就是一个标量。这种Q函数既适用于连续动作（动作是无法穷举的），又适用于离散动作。</li>
<li>输入是一个状态，输出就是多个动作值。这种Q函数只适用于离散动作。</li>
</ol>
<p>在使用Q函数进行策略改进的过程是根据策略调整Q函数，然后根据Q函数跟新策略，且新策略一定比以前的策略更好。<br><img src='/medias/image/screenshot_1662024851761.png' width="60%"><br>$$<br>\pi^{\prime}(s)=\underset{a}{\arg \max} Q_{\pi}(s, a)<br>$$</p>
<h4 id="目标网络"><a href="#目标网络" class="headerlink" title="目标网络"></a>目标网络</h4><p>通过时序差分法训练Q函数的时候的拟合目标为：<br>$$<br>Q_{\pi}(s_{t}, a_{t}) =r_{t}+\gamma Q_{\pi}(s_{t+1}, \pi(s_{t+1}))<br>$$<br>此时右边的目标是变动的，会导致训练过程不稳定。<br>目标网络的做法是先把右边冻住，只更新左边的网络，在更新一定轮次后用左边的新参数覆盖右边网络的参数。<br><img src='/medias/image/screenshot_1662084227422.png' width="60%"></p>
<h4 id="探索"><a href="#探索" class="headerlink" title="探索"></a>探索</h4><p>因为强化学习会加强有收益的动作，所以如果一个动作产生了收益，就会加强，使得其它没有出现的动作更不可能出现。<br>一种方法是在一定概率下采取随机动作，这个概率在训练初期较大，在训练后期较小：<br>$$<br>a= \begin{cases} \underset{a}{\arg \max} Q(s, a) &amp; \text {, 有 } 1-\varepsilon \text { 的概率 } \\ \text { 随机} &amp; \text {, 否则 }  \end{cases}<br>$$<br>另一种方式是玻尔兹曼探索：<br>$$<br>\pi(a \mid s)=\frac{\mathrm{e}^{Q(s, a) / T}}{\sum_{a^{\prime} \in A} \mathrm{e}^{Q(s, a^{\prime}) / T}}<br>$$<br>一方面将取max动作改为了按概率采样；另一方面通过温度系数T调整分布的平缓程度，以给小概率事件更大或更小的采样概率。</p>
<h4 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h4><ul>
<li>在强化学习中，跟环境交互获得数据是比较耗时的；</li>
<li>数据非常相似，缺乏多样性不利于训练。<img src='/medias/image/screenshot_1662085965797.png' width="100%">
经验回放可以设置一个缓冲区，存放过去的数据，里面可能包含不同策略的数据。</li>
<li>这样一方面增加了数据量；</li>
<li>同时也增加了数据的多样性。</li>
</ul>
<p>因为时序差分法学习Q函数，所以即使是不同的策略也没有什么关系。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ReplayBuffer</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' 经验回放池 '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> capacity<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span><span class="token builtin">buffer</span> <span class="token operator">=</span> collections<span class="token punctuation">.</span>deque<span class="token punctuation">(</span>maxlen<span class="token operator">=</span>capacity<span class="token punctuation">)</span>  <span class="token comment"># 队列,先进先出</span>

    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> done<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 将数据加入buffer</span>
        self<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> done<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">sample</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 从buffer中采样数据,数量为batch_size</span>
        transitions <span class="token operator">=</span> random<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        state<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> done <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span>transitions<span class="token punctuation">)</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>next_state<span class="token punctuation">)</span><span class="token punctuation">,</span> done

    <span class="token keyword">def</span> <span class="token function">size</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 目前buffer中数据的数量</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="深度Q网络"><a href="#深度Q网络" class="headerlink" title="深度Q网络"></a>深度Q网络</h4><p>将前面的思想融合就可以得到深度Q网络的算法<br><img src='/medias/image/screenshot_1662086332904.png' width="60%"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Qnet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' 只有一层隐藏层的Q网络 '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Qnet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 隐藏层使用ReLU激活函数</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
<span class="token keyword">class</span> <span class="token class-name">DQN</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' DQN算法 '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span>
                 epsilon<span class="token punctuation">,</span> target_update<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>action_dim <span class="token operator">=</span> action_dim
        self<span class="token punctuation">.</span>q_net <span class="token operator">=</span> Qnet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span>
                          self<span class="token punctuation">.</span>action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token comment"># Q网络</span>
        <span class="token comment"># 目标网络</span>
        self<span class="token punctuation">.</span>target_q_net <span class="token operator">=</span> Qnet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span>
                                 self<span class="token punctuation">.</span>action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        <span class="token comment"># 使用Adam优化器</span>
        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                          lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma  <span class="token comment"># 折扣因子</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> epsilon  <span class="token comment"># epsilon-贪婪策略</span>
        self<span class="token punctuation">.</span>target_update <span class="token operator">=</span> target_update  <span class="token comment"># 目标网络更新频率</span>
        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 计数器,记录更新次数</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device

    <span class="token keyword">def</span> <span class="token function">take_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># epsilon-贪婪策略采取动作</span>
        <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>epsilon<span class="token punctuation">:</span>
            action <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>self<span class="token punctuation">.</span>action_dim<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            action <span class="token operator">=</span> self<span class="token punctuation">.</span>q_net<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> action

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                              dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        actions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        rewards <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        next_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        dones <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                             dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        q_values <span class="token operator">=</span> self<span class="token punctuation">.</span>q_net<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> actions<span class="token punctuation">)</span>  <span class="token comment"># Q值</span>
        <span class="token comment"># 下个状态的最大Q值</span>
        max_next_q_values <span class="token operator">=</span> self<span class="token punctuation">.</span>target_q_net<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>
            <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        q_targets <span class="token operator">=</span> rewards <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> max_next_q_values <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> dones
                                                                <span class="token punctuation">)</span>  <span class="token comment"># TD误差目标</span>
        dqn_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>q_values<span class="token punctuation">,</span> q_targets<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 均方误差损失函数</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># PyTorch中默认梯度会累积,这里需要显式将梯度置为0</span>
        dqn_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 反向传播更新参数</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>count <span class="token operator">%</span> self<span class="token punctuation">.</span>target_update <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>target_q_net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>q_net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 更新目标网络</span>
        self<span class="token punctuation">.</span>count <span class="token operator">+=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><h4 id="double-DQN"><a href="#double-DQN" class="headerlink" title="double DQN"></a>double DQN</h4><p>在DQN中，当一个状态-动作对的值被高估了之后，会传导到使得其它的状态-动作对也被高估。<br><img src='/medias/image/screenshot_1662088624913.png' width="60%"><br>double DQN用一个网络来得到动作，用另一个网络来得到Q值，阻断被高估的Q值的传递。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DQN</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' DQN算法,包括Double DQN '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 下个状态的最大Q值</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>dqn_type <span class="token operator">==</span> <span class="token string">'DoubleDQN'</span><span class="token punctuation">:</span> <span class="token comment"># DQN与Double DQN的区别</span>
            max_action <span class="token operator">=</span> self<span class="token punctuation">.</span>q_net<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> 
            max_next_q_values <span class="token operator">=</span> self<span class="token punctuation">.</span>target_q_net<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> max_action<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span> <span class="token comment"># DQN的情况</span>
            max_next_q_values <span class="token operator">=</span> self<span class="token punctuation">.</span>target_q_net<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h4><p>Dueling DQN将网络的输出进行了拆分：<br>$$<br>Q(s,a) = V(s) + A(s,a)<br>$$<br>可以理解为拆分成了均值标量和动作效果向量：</p>
<ul>
<li>即Q值一部分是由状态贡献；</li>
<li>另一部分是由采取的动作贡献。</li>
</ul>
<p>通过这种方式类似于加入了先验知识：</p>
<ul>
<li>一方面提升学习效率；</li>
<li>另一方面让模型对没有采取过的动作也有泛化能力。</li>
<li>在动作空间较大的环境下非常有效<img src='/medias/image/screenshot_1662089472911.png' width="60%">
为了防止模型不优化状态标量，可以对动作向量去均值。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">VAnet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>VAnet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>  <span class="token comment"># 共享网络部分</span>
        self<span class="token punctuation">.</span>fc_A <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc_V <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        A <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_A<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        V <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_V<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        Q <span class="token operator">=</span> V <span class="token operator">+</span> A <span class="token operator">-</span> A<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Q值由V值和A值计算得到</span>
        <span class="token keyword">return</span> Q<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h4><p>在数据采样的时候从均匀采样改为按概率采样，给时序差分误差较大的样本更多权重。<br><img src='/medias/image/screenshot_1662090087948.png' width="60%"></p>
<h4 id="Balance-between-MC-and-TD"><a href="#Balance-between-MC-and-TD" class="headerlink" title="Balance between MC and TD"></a>Balance between MC and TD</h4><p>在蒙特卡罗和时序差分之间做了一个折中：<br><img src='/medias/image/screenshot_1662090245979.png' width="60%"></p>
<h4 id="Noisy-Net"><a href="#Noisy-Net" class="headerlink" title="Noisy Net"></a>Noisy Net</h4><p>在探索网络的参数上加噪声<br><img src='/medias/image/screenshot_1662090910463.png' width="60%"><br>在一个episode中探索网络的参数是固定了，随机也被固化下来，同一个状态会固定用同样的动作。<br>所以这种噪声是state-dependent exploration，而不像在动作上加噪声一样是随机乱试。</p>
<h4 id="Distributional-Q-function"><a href="#Distributional-Q-function" class="headerlink" title="Distributional Q-function"></a>Distributional Q-function</h4><p>又称C51，网络输出由各个动作的概率值变为各个动作的分布（包含动作取得各个收益的概率）<br><img src='/medias/image/screenshot_1662091553928.png' width="60%"><br>最后选动作的时候一般还是取平均收益最高的动作；<br>但是这个分布可以给训练和选择提供更多方案，例如判断风险。<br>同时也有防止高估状态值的效果。</p>
<h4 id="Rainbow"><a href="#Rainbow" class="headerlink" title="Rainbow"></a>Rainbow</h4><p>将各个技巧融合所得<br>下图上面每一个是只用一个技巧<br><img src='/medias/image/screenshot_1662091735217.png' width="60%"><br>下图上面每一个是只去一个技巧<br><img src='/medias/image/screenshot_1662092109698.png' width="60%"></p>
<h3 id="连续动作"><a href="#连续动作" class="headerlink" title="连续动作"></a>连续动作</h3><h4 id="设计网络"><a href="#设计网络" class="headerlink" title="设计网络"></a>设计网络</h4><img src='/medias/image/screenshot_1662099271298.png' width="60%">
让网络输出三个部分，其中均值$μ$就是能让Q值最大的动作，即：
$$
\mu(s)=\arg \max_{a} Q(s, a)
$$

<h1 id="基于策略"><a href="#基于策略" class="headerlink" title="基于策略"></a>基于策略</h1><h2 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h2><h3 id="策略梯度计算"><a href="#策略梯度计算" class="headerlink" title="策略梯度计算"></a>策略梯度计算</h3><p>智能体与环境交互的轨迹为：<br>$$<br>\tau={s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}}<br>$$<br>在参数为$\theta$的策略下轨迹的概率为：<br>$$<br>p_{\theta}(\tau) =p(s_{1}) p_{\theta}(a_{1} | s_{1}) p(s_{2} | s_{1}, a_{1}) p_{\theta}(a_{2} | s_{2}) p(s_{3} | s_{2}, a_{2}) \cdots \\<br>=p(s_{1}) \prod_{t=1}^{T} p_{\theta}(a_{t} | s_{t}) p(s_{t+1} | s_{t}, a_{t})<br>$$<br>其中$p(s_{t+1}|s_t,a_t)$是环境，无法控制；<br>而$p_\theta(a_t|s_t)$代表参数为$\theta$的智能体策略，是可以调整的。<br>在不同的轨迹下会有不同的奖励$R(\tau)$，而它的期望也就是在参数为$\theta$的智能体策略下的奖励期望：<br>$$<br>R_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)<br>$$</p>
<img src='/medias/image/screenshot_1661934842794.png' width="60%">

<p>想要让奖励越大越好，可以使用梯度上升来最大化期望奖励：<br>$$<br> R_{\theta}=\sum_{\tau} R(\tau)  p_{\theta}(\tau)<br>$$</p>
<p>即更新策略中的参数。<br>通过推导可得：</p>
<p>$$<br>\nabla R_{\theta}=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau) \\<br>=\sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\<br>=\sum_{\tau} R(\tau) p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\<br>=E_{\tau \sim p_{\theta}(\tau)}[R(\tau) \nabla \log p_{\theta}(\tau)]\\<br>\approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^{n}) \nabla \log p_{\theta}(\tau^{n}) \\<br>=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R(\tau^{n}) \nabla \log p_{\theta}(a_{t}^{n} \mid s_{t}^{n})<br>$$</p>
<p>从公式可以直观看出：<br>当在状态$s_{t}$下采取动作$a_{t}$得到的奖励$\tau$是正的时候，就会增大在状态$s_{t}$下采取动作$a_{t}$的概率，反之减小概率；<br>同时当奖励越大时，也会以更大幅度来增大在状态$s_{t}$下采取动作$a_{t}$的概率。<br>这里的梯度上升也可以用深度学习的优化器：<br>$$<br>\theta \leftarrow \theta+\eta R_{\theta}<br>$$<br><img src='/medias/image/screenshot_1661999624899.png' width="60%"><br><img src='/medias/image/screenshot_1661999773063.png' width="60%"></p>
<h3 id="实现技巧"><a href="#实现技巧" class="headerlink" title="实现技巧"></a>实现技巧</h3><h4 id="添加基线"><a href="#添加基线" class="headerlink" title="添加基线"></a>添加基线</h4><p>在有的环境中，奖励永远都是正，会使得采样到的动作的概率都会上升，而没有采样到的动作概率会下降，但没有采样到的动作不一定是不好的动作。<br>因此可以增加一个基线，使得奖励低于基线的动作概率下降：<br>$$<br>R_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}(R(\tau^{n})-b)  \log p_{\theta}(a_{t}^{n} \mid s_{t}^{n})<br>$$<br>基线可以采用奖励的平均值（期望值），也可以采用网络估计，让网络根据状态估计。这样$R-b$这一项就可以表示在某个状态下采取某个动作的相对优势，即采取这个动作是不是比采取别的动作好，也就是评论员（critic）。</p>
<h4 id="调整奖励分数"><a href="#调整奖励分数" class="headerlink" title="调整奖励分数"></a>调整奖励分数</h4><p>前面给每个动作对都使用整个轨迹的奖励，而这是不合理的：</p>
<ul>
<li>后面的动作对前面获得的奖励应该没有贡献；</li>
<li>现在的动作对未来的奖励的贡献应该衰减。<br>应对第一个问题，可以加上时间限制，每个动作只用现在到未来的奖励：<br>$$<br>R_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}(\sum_{t^{\prime}=t}^{T_{n}} r_{t^{\prime}}^{n}-b)  \log p_{\theta}(a_{t}^{n} \mid s_{t}^{n})<br>$$<br>应对第二个问题，可以加上折扣系数</li>
</ul>
<p>$$<br>R_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}(\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n}-b)  \log p_{\theta}(a_{t}^{n} \mid s_{t}^{n})<br>$$</p>
<h3 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h3><p>基于策略梯度的强化学习的经典算法，采用回合更新的模式：<br><img src='/medias/image/screenshot_1662001267683.png' width="60%"><br><img src='/medias/image/screenshot_1662001283381.png' width="60%"><br><img src='/medias/image/2022-09-14-18-23-04.png' width="100%"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PolicyNet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PolicyNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">REINFORCE</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span>
                 device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>policy_net <span class="token operator">=</span> PolicyNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span>
                                    action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>policy_net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                          lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>  <span class="token comment"># 使用Adam优化器</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma  <span class="token comment"># 折扣因子</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device

    <span class="token keyword">def</span> <span class="token function">take_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 根据动作概率分布随机采样</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        probs <span class="token operator">=</span> self<span class="token punctuation">.</span>policy_net<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        action_dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>probs<span class="token punctuation">)</span>
        action <span class="token operator">=</span> action_dist<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> action<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        reward_list <span class="token operator">=</span> transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span>
        state_list <span class="token operator">=</span> transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span>
        action_list <span class="token operator">=</span> transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span>
        G <span class="token operator">=</span> <span class="token number">0</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">reversed</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>reward_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 从最后一步算起</span>
            reward <span class="token operator">=</span> reward_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
            state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                 dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            action <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>action_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            log_prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>policy_net<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">)</span>
            G <span class="token operator">=</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> G <span class="token operator">+</span> reward
            loss <span class="token operator">=</span> <span class="token operator">-</span>log_prob <span class="token operator">*</span> G  <span class="token comment"># 每一步的损失函数</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 反向传播计算梯度</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 梯度下降</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h1 id="演员-评论员（AC）"><a href="#演员-评论员（AC）" class="headerlink" title="演员-评论员（AC）"></a>演员-评论员（AC）</h1><p>结合介于策略和基于价值的方法<br><img src='/medias/image/screenshot_1662099462188.png' width="60%"></p>
<p>Actor-Critic优点：</p>
<ol>
<li>相比以值函数为中心的算法，Actor - Critic应用了策略梯度的做法，这能让它在连续动作或者高维动作空间中选取合适的动作，而 Q-learning 做这件事会很困难甚至瘫痪。另一方面，由Actor决定动作，相比只使用Critic的方法，增加了随机性、探索性，可以更好地找到更优解。</li>
<li>相比单纯策略梯度，Actor - Critic应用了Q-learning或其他策略评估的做法，使得Actor Critic能进行单步更新而不是回合更新，比单纯的Policy Gradient的效率要高。</li>
</ol>
<h2 id="优势演员-评论员算法（A2C）"><a href="#优势演员-评论员算法（A2C）" class="headerlink" title="优势演员-评论员算法（A2C）"></a>优势演员-评论员算法（A2C）</h2><p><strong>演员-评论员算法</strong>是一种结合<strong>策略梯度</strong>和<strong>时序差分学习</strong>的强化学习方法，其中，演员是指策略函数$\pi_{\theta}(a|s)$，即学习一个策略以得到尽可能高的回报。评论员是指价值函数$V_{\pi}(s)$，对当前策略的值函数进行估计，即评估演员的好坏。借助于价值函数，演员-评论员算法可以进行单步参数更新，不需要等到回合结束才进行更新。在演员-评论员算法里面，最知名的算法就是异步优势演员-评论员算法。如果我们去掉异步，则为<strong>优势演员-评论员（advantage actor-critic，A2C）算法</strong>。<br><img src='/medias/image/screenshot_1662101051405.png' width="60%"><br>基本做法就是把策略梯度算法的累计奖励替换成价值函数，基本的价值函数有$V_{\pi_{\theta}}(s_{t}^{n})$和$Q_{\pi_{\theta}}(s_{t}^{n}, a_{t}^{n})$，还可以替换成优势函数$Q_{\pi_{\theta}}(s_{t}^{n}, a_{t}^{n})-V_{\pi_{\theta}}(s_{t}^{n})$。后者就是<strong>优势演员-评论员（advantage actor-critic，A2C）算法</strong>。<br>而A2C的公式也可以简化成只用一个网络：<br>$$<br>Q_{\pi_{\theta}}(s_{t}^{n}, a_{t}^{n})-V_{\pi_{\theta}}(s_{t}^{n})\\<br>=\mathbb{E}[r_{t}^{n}+\gamma V_{\pi}(s_{t+1}^{n})]-V_{\pi_{\theta}}(s_{t}^{n})\\<br>=r_{t}^{n}+\gamma V_{\pi}(s_{t+1}^{n})-V_{\pi_{\theta}}(s_{t}^{n})<br>$$<br>在训练的时候，演员和评论家是交替进行优化的。<br>所以策略梯度为：<br>$$<br> \nabla \bar{R}{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(r_{t}^{n}+\gamma V^{\pi}\left(s_{t+1}^{n}\right)-V^{\pi}\left(s_{t}^{n}\right)\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)<br>$$<br><img src='/medias/image/2022-09-04-21-41-55.png' width="60%"><br>因为演员和评论员的输入都是状态，所以这两个网络可以考虑共享底层网络来提升泛化性，降低训练难度。<br><img src='/medias/image/2022-09-05-10-20-42.png' width="60%"><br>还可以对策略的输出分布加以限制，让分布的熵不要太小，以增加探索性。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PolicyNet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PolicyNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">ValueNet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ValueNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">ActorCritic</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> actor_lr<span class="token punctuation">,</span> critic_lr<span class="token punctuation">,</span>
                 gamma<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 策略网络</span>
        self<span class="token punctuation">.</span>actor <span class="token operator">=</span> PolicyNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic <span class="token operator">=</span> ValueNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token comment"># 价值网络</span>
        <span class="token comment"># 策略网络优化器</span>
        self<span class="token punctuation">.</span>actor_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                lr<span class="token operator">=</span>actor_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 lr<span class="token operator">=</span>critic_lr<span class="token punctuation">)</span>  <span class="token comment"># 价值网络优化器</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device

    <span class="token keyword">def</span> <span class="token function">take_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        probs <span class="token operator">=</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        action_dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>probs<span class="token punctuation">)</span>
        action <span class="token operator">=</span> action_dist<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> action<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                              dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        actions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        rewards <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        next_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        dones <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                             dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        <span class="token comment"># 时序差分目标</span>
        td_target <span class="token operator">=</span> rewards <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span>
                                                                       dones<span class="token punctuation">)</span>
        td_delta <span class="token operator">=</span> td_target <span class="token operator">-</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span>  <span class="token comment"># 时序差分误差</span>
        log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> actions<span class="token punctuation">)</span><span class="token punctuation">)</span>
        actor_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span>log_probs <span class="token operator">*</span> td_delta<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 均方误差损失函数</span>
        critic_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>
            F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">,</span> td_target<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        actor_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 计算策略网络的梯度</span>
        critic_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 计算价值网络的梯度</span>
        self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新策略网络的参数</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新价值网络的参数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="异步优势演员-评论员算法（A3C）"><a href="#异步优势演员-评论员算法（A3C）" class="headerlink" title="异步优势演员-评论员算法（A3C）"></a>异步优势演员-评论员算法（A3C）</h2><p>为了提升训练速度，可以采用多进程的方式，每个子进程探索一个环节，并把得到的梯度回传个主节点的参数：<br><img src='/medias/image/2022-09-05-10-40-15.png' width="60%"></p>
<h2 id="路径衍生策略梯度算法"><a href="#路径衍生策略梯度算法" class="headerlink" title="路径衍生策略梯度算法"></a>路径衍生策略梯度算法</h2><p>优势演员-评论员算法中的Q网络只能评价动作好不好，不能提供好的方向，路径衍生策略梯度算法的思想类似于生成对抗网络，让Q网络同时输入状态和演员生成的动作，得到动作价值，进而可以给演员的优化提供方向。<br><img src='/medias/image/2022-09-05-11-03-33.png' width="60%"><br>路径衍生策略梯度算法也可以看成是在深度Q网络的基础上用策略梯度来选择动作，以解决动作空间过大或者动作连续的问题：<br><img src='/medias/image/2022-09-05-11-06-57.png' width="60%"><br>与生成对抗网络在技巧上的研究<br><img src='/medias/image/2022-09-05-11-09-12.png' width="60%"></p>
<h2 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h2><p>Policy gradient 是on-policy的做法，这样要收集很多数据，然后更新参数，再去收集数据，效率较低；<br>可以用一个policy去跟环境互动收集数据，另一个policy可以用收集到的数据执行梯度上升。<br>考虑到两个policy的参数不一样，动作分布不一样，因此梯度计算需要进行一些调整。<br>假设两个policy的差别不大的情况下，可以通过<strong>重要性采样</strong>，加入两个policy的概率值来调整：<br><img src='/medias/image/screenshot_1662004433769.png' width="60%"><br>其中跟环境互动收集数据的policy参数$\theta’$是常数，不用求梯度。</p>
<p>当两个policy的差别过大时，会使得算法过程不稳定，因此要控制这种差异：</p>
<ul>
<li>一种思路是控制参数空间，控制参数更新量；<ul>
<li>但是步长过小会造成效率低下；</li>
<li>步长过大算法就会不稳定；</li>
<li>即无法找到好的固定步长</li>
</ul>
</li>
<li>另一种思路是控制策略空间：<ul>
<li>让新旧策略在策略空间上的变化不大；</li>
</ul>
</li>
</ul>
<p>即使是on-policy做法，按照回合更新时，从后往前更新，也会出现策略不一致的情况</p>
<p><code>信任区域策略优化(Trust Region Policy Optimization，TRPO)</code>在梯度公式的基础上通过KL散度来限制两个参数的差距：<br>$$<br>\begin{aligned}<br>J_{T R P O}^{\theta^{\prime}}(\theta)=E_{(s_{t}, a_{t}) \sim \pi_{\theta^{\prime}}}[\frac{p_{\theta}(a_{t} | s_{t})}{p_{\theta^{\prime}}(a_{t} | s_{t})} A^{\theta^{\prime}}(s_{t}, a_{t})] \ \<br>\mathrm{KL}(\theta, \theta^{\prime})&lt;\delta<br>\end{aligned}<br>$$<br>这里的KL散度不是衡量参数的差异，而是衡量在相同状态下采取的行动差异。<br><img src='/medias/image/2022-09-14-18-22-39.png' width="100%"></p>
<h3 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h3><p>在优势函数方面，TRPO和后续的PPO采用<code>GAE(General Advantage Estimation)</code>的方式。<br>它的公式为：<br><img src='/medias/image/2022-09-21-14-58-56.png' width="100%"><br>可以看成是另一种N步TD，$\lambda$等于0时，等效为A2C中的优势函数：<br><img src='/medias/image/2022-09-21-14-50-19.png' width="70%"><br>而当$\lambda$等于1时，则变成了使用轨迹终点来计算：<br><img src='/medias/image/2022-09-21-14-50-35.png' width="60%"></p>
<p>即$\gamma$和$\lambda$越小时，更多地使用较近的奖励，因而方差更小，同时偏差大；<br>反之$\gamma$和$\lambda$越大时，更多地使用较远的奖励，因而方差更大，同时偏差小；</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TRPO</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" TRPO算法 """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> state_space<span class="token punctuation">,</span> action_space<span class="token punctuation">,</span> lmbda<span class="token punctuation">,</span>
                 kl_constraint<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> critic_lr<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state_dim <span class="token operator">=</span> state_space<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        action_dim <span class="token operator">=</span> action_space<span class="token punctuation">.</span>n
        <span class="token comment"># 策略网络参数不需要优化器更新</span>
        self<span class="token punctuation">.</span>actor <span class="token operator">=</span> PolicyNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic <span class="token operator">=</span> ValueNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 lr<span class="token operator">=</span>critic_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>lmbda <span class="token operator">=</span> lmbda  <span class="token comment"># GAE参数</span>
        self<span class="token punctuation">.</span>kl_constraint <span class="token operator">=</span> kl_constraint  <span class="token comment"># KL距离最大限制</span>
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> alpha  <span class="token comment"># 线性搜索参数</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device

    <span class="token keyword">def</span> <span class="token function">take_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        probs <span class="token operator">=</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        action_dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>probs<span class="token punctuation">)</span>
        action <span class="token operator">=</span> action_dist<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> action<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">hessian_matrix_vector_product</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> states<span class="token punctuation">,</span> old_action_dists<span class="token punctuation">,</span> vector<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 计算黑塞矩阵和一个向量的乘积</span>
        new_action_dists <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">)</span>
        kl <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>
            torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>kl<span class="token punctuation">.</span>kl_divergence<span class="token punctuation">(</span>old_action_dists<span class="token punctuation">,</span>
                                                 new_action_dists<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 计算平均KL距离</span>
        kl_grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>kl<span class="token punctuation">,</span>
                                      self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                      create_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        kl_grad_vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>grad<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> grad <span class="token keyword">in</span> kl_grad<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># KL距离的梯度先和向量进行点积运算</span>
        kl_grad_vector_product <span class="token operator">=</span> torch<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>kl_grad_vector<span class="token punctuation">,</span> vector<span class="token punctuation">)</span>
        grad2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>kl_grad_vector_product<span class="token punctuation">,</span>
                                    self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        grad2_vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>grad<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> grad <span class="token keyword">in</span> grad2<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> grad2_vector

    <span class="token keyword">def</span> <span class="token function">conjugate_gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad<span class="token punctuation">,</span> states<span class="token punctuation">,</span> old_action_dists<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 共轭梯度法求解方程</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>grad<span class="token punctuation">)</span>
        r <span class="token operator">=</span> grad<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
        p <span class="token operator">=</span> grad<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
        rdotr <span class="token operator">=</span> torch<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>r<span class="token punctuation">,</span> r<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 共轭梯度主循环</span>
            Hp <span class="token operator">=</span> self<span class="token punctuation">.</span>hessian_matrix_vector_product<span class="token punctuation">(</span>states<span class="token punctuation">,</span> old_action_dists<span class="token punctuation">,</span>
                                                    p<span class="token punctuation">)</span>
            alpha <span class="token operator">=</span> rdotr <span class="token operator">/</span> torch<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>p<span class="token punctuation">,</span> Hp<span class="token punctuation">)</span>
            x <span class="token operator">+=</span> alpha <span class="token operator">*</span> p
            r <span class="token operator">-=</span> alpha <span class="token operator">*</span> Hp
            new_rdotr <span class="token operator">=</span> torch<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>r<span class="token punctuation">,</span> r<span class="token punctuation">)</span>
            <span class="token keyword">if</span> new_rdotr <span class="token operator">&lt;</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">:</span>
                <span class="token keyword">break</span>
            beta <span class="token operator">=</span> new_rdotr <span class="token operator">/</span> rdotr
            p <span class="token operator">=</span> r <span class="token operator">+</span> beta <span class="token operator">*</span> p
            rdotr <span class="token operator">=</span> new_rdotr
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">compute_surrogate_obj</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> advantage<span class="token punctuation">,</span> old_log_probs<span class="token punctuation">,</span>
                              actor<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 计算策略目标</span>
        log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> actions<span class="token punctuation">)</span><span class="token punctuation">)</span>
        ratio <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>log_probs <span class="token operator">-</span> old_log_probs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>ratio <span class="token operator">*</span> advantage<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">line_search</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> advantage<span class="token punctuation">,</span> old_log_probs<span class="token punctuation">,</span>
                    old_action_dists<span class="token punctuation">,</span> max_vec<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 线性搜索</span>
        old_para <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>convert_parameters<span class="token punctuation">.</span>parameters_to_vector<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        old_obj <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_surrogate_obj<span class="token punctuation">(</span>states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> advantage<span class="token punctuation">,</span>
                                             old_log_probs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 线性搜索主循环</span>
            coef <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha<span class="token operator">**</span>i
            new_para <span class="token operator">=</span> old_para <span class="token operator">+</span> coef <span class="token operator">*</span> max_vec
            new_actor <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">)</span>
            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>convert_parameters<span class="token punctuation">.</span>vector_to_parameters<span class="token punctuation">(</span>
                new_para<span class="token punctuation">,</span> new_actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            new_action_dists <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>
                new_actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">)</span>
            kl_div <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>
                torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>kl<span class="token punctuation">.</span>kl_divergence<span class="token punctuation">(</span>old_action_dists<span class="token punctuation">,</span>
                                                     new_action_dists<span class="token punctuation">)</span><span class="token punctuation">)</span>
            new_obj <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_surrogate_obj<span class="token punctuation">(</span>states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> advantage<span class="token punctuation">,</span>
                                                 old_log_probs<span class="token punctuation">,</span> new_actor<span class="token punctuation">)</span>
            <span class="token keyword">if</span> new_obj <span class="token operator">></span> old_obj <span class="token keyword">and</span> kl_div <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>kl_constraint<span class="token punctuation">:</span>
                <span class="token keyword">return</span> new_para
        <span class="token keyword">return</span> old_para

    <span class="token keyword">def</span> <span class="token function">policy_learn</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> old_action_dists<span class="token punctuation">,</span> old_log_probs<span class="token punctuation">,</span>
                     advantage<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 更新策略函数</span>
        surrogate_obj <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_surrogate_obj<span class="token punctuation">(</span>states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> advantage<span class="token punctuation">,</span>
                                                   old_log_probs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">)</span>
        grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>surrogate_obj<span class="token punctuation">,</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        obj_grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>grad<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> grad <span class="token keyword">in</span> grads<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 用共轭梯度法计算x = H^(-1)g</span>
        descent_direction <span class="token operator">=</span> self<span class="token punctuation">.</span>conjugate_gradient<span class="token punctuation">(</span>obj_grad<span class="token punctuation">,</span> states<span class="token punctuation">,</span>
                                                    old_action_dists<span class="token punctuation">)</span>

        Hd <span class="token operator">=</span> self<span class="token punctuation">.</span>hessian_matrix_vector_product<span class="token punctuation">(</span>states<span class="token punctuation">,</span> old_action_dists<span class="token punctuation">,</span>
                                                descent_direction<span class="token punctuation">)</span>
        max_coef <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>kl_constraint <span class="token operator">/</span>
                              <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>descent_direction<span class="token punctuation">,</span> Hd<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        new_para <span class="token operator">=</span> self<span class="token punctuation">.</span>line_search<span class="token punctuation">(</span>states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> advantage<span class="token punctuation">,</span> old_log_probs<span class="token punctuation">,</span>
                                    old_action_dists<span class="token punctuation">,</span>
                                    descent_direction <span class="token operator">*</span> max_coef<span class="token punctuation">)</span>  <span class="token comment"># 线性搜索</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>convert_parameters<span class="token punctuation">.</span>vector_to_parameters<span class="token punctuation">(</span>
            new_para<span class="token punctuation">,</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 用线性搜索后的参数更新策略</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                              dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        actions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        rewards <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        next_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        dones <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                             dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        td_target <span class="token operator">=</span> rewards <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span>
                                                                       dones<span class="token punctuation">)</span>
        td_delta <span class="token operator">=</span> td_target <span class="token operator">-</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span>
        advantage <span class="token operator">=</span> compute_advantage<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>lmbda<span class="token punctuation">,</span>
                                      td_delta<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        old_log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>
                                                            actions<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        old_action_dists <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        critic_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>
            F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">,</span> td_target<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        critic_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新价值函数</span>
        <span class="token comment"># 更新策略函数</span>
        self<span class="token punctuation">.</span>policy_learn<span class="token punctuation">(</span>states<span class="token punctuation">,</span> actions<span class="token punctuation">,</span> old_action_dists<span class="token punctuation">,</span> old_log_probs<span class="token punctuation">,</span>
                          advantage<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h2><p>而<code>近端策略优化(Proximal Policy Optimization，简称 PPO)</code>是A2C的一个变形，它是现在OpenAI默认的强化学习算法。</p>
<img src='/medias/image/screenshot_1662005511162.png' width="60%">
TRPO是通过约束使得KL散度不要太大，但是这个比较难算，PPO是在计算梯度的时候把约束融合进去，包含两个变种。

<h3 id="PPO-Penalty"><a href="#PPO-Penalty" class="headerlink" title="PPO-Penalty"></a>PPO-Penalty</h3><img src='/medias/image/screenshot_1662005837900.png' width="60%">
通过自适应KL散度，让两个参数输出的动作差别减小

<h3 id="PPO-Clip"><a href="#PPO-Clip" class="headerlink" title="PPO-Clip"></a>PPO-Clip</h3><img src='/medias/image/screenshot_1662005960602.png' width="60%">
通过clip让参数的优化不要太过。
<img src='/medias/image/2022-09-14-18-22-18.png' width="100%">
截断的效果比较好，而且非常简单；
通过观察梯度可以发现，截断就是限制重要性采样的比值，让新旧策略差别大的样本不起作用。

<p>在实际中往往使用分布式强化学习，同时在多个环境采集数据，供模型训练，因此必然存在模型不同步的问题，而PPO可以较好的解决这个问题，因此PPO的应用非常广泛。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PPO</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' PPO算法,采用截断方式 '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> actor_lr<span class="token punctuation">,</span> critic_lr<span class="token punctuation">,</span>
                 lmbda<span class="token punctuation">,</span> epochs<span class="token punctuation">,</span> eps<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>actor <span class="token operator">=</span> PolicyNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic <span class="token operator">=</span> ValueNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                lr<span class="token operator">=</span>actor_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 lr<span class="token operator">=</span>critic_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>lmbda <span class="token operator">=</span> lmbda
        self<span class="token punctuation">.</span>epochs <span class="token operator">=</span> epochs  <span class="token comment"># 一条序列的数据用来训练轮数</span>
        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps  <span class="token comment"># PPO中截断范围的参数</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device

    <span class="token keyword">def</span> <span class="token function">take_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        probs <span class="token operator">=</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        action_dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>probs<span class="token punctuation">)</span>
        action <span class="token operator">=</span> action_dist<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> action<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                              dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        actions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        rewards <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        next_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        dones <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                             dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        td_target <span class="token operator">=</span> rewards <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span>
                                                                       dones<span class="token punctuation">)</span>
        td_delta <span class="token operator">=</span> td_target <span class="token operator">-</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span>
        advantage <span class="token operator">=</span> rl_utils<span class="token punctuation">.</span>compute_advantage<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>lmbda<span class="token punctuation">,</span>
                                               td_delta<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        old_log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>
                                                            actions<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> actions<span class="token punctuation">)</span><span class="token punctuation">)</span>
            ratio <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>log_probs <span class="token operator">-</span> old_log_probs<span class="token punctuation">)</span>
            surr1 <span class="token operator">=</span> ratio <span class="token operator">*</span> advantage
            surr2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>ratio<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">,</span>
                                <span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span> <span class="token operator">*</span> advantage  <span class="token comment"># 截断</span>
            actor_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span>torch<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>surr1<span class="token punctuation">,</span> surr2<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># PPO损失函数</span>
            critic_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>
                F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">,</span> td_target<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            actor_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            critic_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="其它技巧"><a href="#其它技巧" class="headerlink" title="其它技巧"></a>其它技巧</h3><p>此处参考<a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/" title="" target="">The 37 Implementation Details of Proximal Policy Optimization</a>，<a href="https://zhuanlan.zhihu.com/p/512327050" title="" target="">影响PPO算法性能的10个关键技巧</a></p>
<h4 id="Advantage-Normalization"><a href="#Advantage-Normalization" class="headerlink" title="Advantage Normalization"></a>Advantage Normalization</h4><p>在使用GAE计算完一个batch的优势值后，对优势值进行减均值除标准差的Normalization；<br>可以在整个batch上做，也可以在minibatch上做，一般前者更好，因为后者波动更大。</p>
<h4 id="State-Normalization"><a href="#State-Normalization" class="headerlink" title="State Normalization"></a>State Normalization</h4><p>对输入的状态值做减均值除标准差的Normalization；<br>因为强化学习的数据并非一开始就获得全部，因此不能像监督学习一样在数据预处理阶段统一进行；<br>可以动态维护所有获得过的state的mean和std，然后再对当前的state做normalization。</p>
<h4 id="Reward-Normalization-amp-Reward-Scaling"><a href="#Reward-Normalization-amp-Reward-Scaling" class="headerlink" title="Reward Normalization &amp; Reward Scaling"></a>Reward Normalization &amp; Reward Scaling</h4><p>Reward Normalization：</p>
<ul>
<li>state normalization的操作类似，也是动态维护所有获得过的reward的mean和std，然后再对当前的reward做normalization</li>
</ul>
<p>Reward Scaling：</p>
<ul>
<li>通过指数滑动平均的方式维护标准差，然后只对reward除标准差</li>
</ul>
<p>一般Reward Scaling的效果要好于Reward Normalization，Reward Normalization可以会破坏reward的结构</p>
<h4 id="Policy-Entropy"><a href="#Policy-Entropy" class="headerlink" title="Policy Entropy"></a>Policy Entropy</h4><p>用熵来衡量策略给各个动作预测的概率值：<br><img src='/medias/image/2022-09-22-10-56-23.png' width="80%"><br>熵越大，说明策略给各个动作的预测更加平均，探索性更强，更有利于找到更优解<br>可以将熵加到loss上</p>
<h4 id="Learning-Rate-Decay"><a href="#Learning-Rate-Decay" class="headerlink" title="Learning Rate Decay"></a>Learning Rate Decay</h4><p>通过学习率衰减，提升训练后期的平稳性</p>
<h4 id="Gradient-clip"><a href="#Gradient-clip" class="headerlink" title="Gradient clip"></a>Gradient clip</h4><p>加入Gradient clip防止训练的时候出现梯度爆炸</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> self<span class="token punctuation">.</span>use_grad_clip<span class="token punctuation">:</span> <span class="token comment"># Trick 7: Gradient clip</span>
    torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span> 
self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Orthogonal-Initialization"><a href="#Orthogonal-Initialization" class="headerlink" title="Orthogonal Initialization"></a>Orthogonal Initialization</h4><p><code>正交初始化（Orthogonal Initialization）</code>是为了防止训练开始出现梯度消失和梯度爆炸的问题</p>
<ul>
<li>用均值为0，标准差为1的高斯分布初始化权重矩阵；</li>
<li>对这个权重矩阵进行奇异值分解，得到两个正交矩阵，取其中之一作为该层神经网络的权重矩阵。</li>
</ul>
<h4 id="Adam-Optimizer-Epsilon-Parameter"><a href="#Adam-Optimizer-Epsilon-Parameter" class="headerlink" title="Adam Optimizer Epsilon Parameter"></a>Adam Optimizer Epsilon Parameter</h4><p>将pytorch中Adam优化器默认的eps=1e-8改为eps=1e-5，可以提升训练性能</p>
<h4 id="Tanh-Activation-Function"><a href="#Tanh-Activation-Function" class="headerlink" title="Tanh Activation Function"></a>Tanh Activation Function</h4><p>把PPO的激活函数从relu改为tanh</p>
<h4 id="done信号区分"><a href="#done信号区分" class="headerlink" title="done信号区分"></a>done信号区分</h4><p>环境结束除了输赢外还有达到最大步长的情况；<br>在计算目标时，输赢的结束只考虑奖励，而达到最大步长的结束还应该考虑下一个状态的值。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> done <span class="token keyword">and</span> episode_steps <span class="token operator">!=</span> args<span class="token punctuation">.</span>max_episode_steps<span class="token punctuation">:</span>
    dw <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    dw <span class="token operator">=</span> <span class="token boolean">False</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
deltas <span class="token operator">=</span> r <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> dw<span class="token punctuation">)</span> <span class="token operator">*</span> vs_ <span class="token operator">-</span> vs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="动作空间分布"><a href="#动作空间分布" class="headerlink" title="动作空间分布"></a>动作空间分布</h4><p>将连续动作空间的分布从Gaussian分布改为Beta分布。<br>因为前者是无界分布，还需要clip的操作，这会带来不好的影响</p>
<h2 id="深度确定性策略梯度（DDPG）"><a href="#深度确定性策略梯度（DDPG）" class="headerlink" title="深度确定性策略梯度（DDPG）"></a>深度确定性策略梯度（DDPG）</h2><p>在连续的动作控制空间，Q-learning、DQN 等算法是没有办法处理的。这个时候可以使用策略网络。</p>
<ul>
<li>在离散动作的场景下，有几个动作，神经网络就输出几个概率值，我们用$\pi_\theta(a_t|s_t)$来表示这个随机性的策略。</li>
<li>在连续的动作场景下，比如机器人手臂弯曲的角度，可以输出一个具体的浮点数。我们用$\mu_{\theta}(s_t)$来代表这个确定性的策略。<img src='/medias/image/2022-09-05-13-07-43.png' width="80%"></li>
</ul>
<p>DDPG 的特点可以从它的名字当中拆解出来，拆解成：</p>
<ul>
<li>Deep：深度网络</li>
<li>Deterministic：输出确定的动作</li>
<li>Policy Gradient：策略网络<img src='/medias/image/2022-09-05-13-08-45.png' width="70%"></li>
</ul>
<p>同时借鉴了DQN的目标网络和经验回放<br><img src='/medias/image/2022-09-05-13-13-40.png' width="80%"><br>也属于AC网络。<br>Q网络和策略网络都有target network<br><img src='/medias/image/2022-09-05-13-15-04.png' width="80%"><br>因为DDPG是使用的确定性策略，因此可能不会尝试足够多的action来找有用的学习信号；<br>可以通过加入时间相关的OU噪声或者高斯噪声来提高探索性。<br><img src='/medias/image/2022-09-14-18-20-53.png' width="100%"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PolicyNet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> action_bound<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PolicyNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>action_bound <span class="token operator">=</span> action_bound  <span class="token comment"># action_bound是环境可以接受的动作最大值</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>action_bound


<span class="token keyword">class</span> <span class="token class-name">QValueNet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>QValueNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim <span class="token operator">+</span> action_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        cat <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span> a<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 拼接状态和动作</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>cat<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc_out<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">DDPG</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' DDPG算法 '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> action_bound<span class="token punctuation">,</span> sigma<span class="token punctuation">,</span> actor_lr<span class="token punctuation">,</span> critic_lr<span class="token punctuation">,</span> tau<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>actor <span class="token operator">=</span> PolicyNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> action_bound<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic <span class="token operator">=</span> QValueNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>target_actor <span class="token operator">=</span> PolicyNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> action_bound<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>target_critic <span class="token operator">=</span> QValueNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        <span class="token comment"># 初始化目标价值网络并设置和价值网络相同的参数</span>
        self<span class="token punctuation">.</span>target_critic<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 初始化目标策略网络并设置和策略相同的参数</span>
        self<span class="token punctuation">.</span>target_actor<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>actor_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>critic_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>sigma <span class="token operator">=</span> sigma  <span class="token comment"># 高斯噪声的标准差,均值直接设为0</span>
        self<span class="token punctuation">.</span>tau <span class="token operator">=</span> tau  <span class="token comment"># 目标网络软更新参数</span>
        self<span class="token punctuation">.</span>action_dim <span class="token operator">=</span> action_dim
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device

    <span class="token keyword">def</span> <span class="token function">take_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        action <span class="token operator">=</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 给动作添加噪声，增加探索</span>
        action <span class="token operator">=</span> action <span class="token operator">+</span> self<span class="token punctuation">.</span>sigma <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>action_dim<span class="token punctuation">)</span>
        <span class="token keyword">return</span> action

    <span class="token keyword">def</span> <span class="token function">soft_update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> net<span class="token punctuation">,</span> target_net<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> param_target<span class="token punctuation">,</span> param <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>target_net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            param_target<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>param_target<span class="token punctuation">.</span>data <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tau<span class="token punctuation">)</span> <span class="token operator">+</span> param<span class="token punctuation">.</span>data <span class="token operator">*</span> self<span class="token punctuation">.</span>tau<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        actions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        rewards <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        next_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        dones <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        next_q_values <span class="token operator">=</span> self<span class="token punctuation">.</span>target_critic<span class="token punctuation">(</span>next_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>target_actor<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span><span class="token punctuation">)</span>
        q_targets <span class="token operator">=</span> rewards <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> next_q_values <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> dones<span class="token punctuation">)</span>
        critic_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">,</span> actions<span class="token punctuation">)</span><span class="token punctuation">,</span> q_targets<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        critic_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        actor_loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        actor_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>soft_update<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">,</span> self<span class="token punctuation">.</span>target_actor<span class="token punctuation">)</span>  <span class="token comment"># 软更新策略网络</span>
        self<span class="token punctuation">.</span>soft_update<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">,</span> self<span class="token punctuation">.</span>target_critic<span class="token punctuation">)</span>  <span class="token comment"># 软更新价值网络</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="Twin-Delayed-DDPG"><a href="#Twin-Delayed-DDPG" class="headerlink" title="Twin Delayed DDPG"></a>Twin Delayed DDPG</h2><p>DDPG有时表现很好，但它在超参数和其他类型的调整方面经常很敏感，<code>双延迟深度确定性策略梯度(Twin Delayed DDPG，简称 TD3)</code>引入三个关键技巧来解决</p>
<p><strong>截断的双 Q 学习</strong>：同时训练两个Q函数，并使用Q值较小的来作为Q目标<br>$$<br>y\left(r, s^{\prime}, d\right)=r+\gamma(1-d) \min_{i=1,2} Q_{\phi_{i, t a r g}}\left(s^{\prime}, a_{T D 3}\left(s^{\prime}\right)\right)<br>$$<br><strong>延迟的策略更新</strong>：降低策略网络的更新频率<br><strong>目标策略平滑</strong>：在目标动作中加入噪声，以平滑误差<br>$$<br>a_{T D 3}\left(s^{\prime}\right)=\operatorname{clip}\left(\mu_{\theta, t a r g}\left(s^{\prime}\right)+\operatorname{clip}(\epsilon,-c, c), a_{\text {low }}, a_{\text {high }}\right)<br>$$<br><img src='/medias/image/2022-09-14-18-20-27.png' width="100%"></p>
<h2 id="soft-Actor-Critic"><a href="#soft-Actor-Critic" class="headerlink" title="soft Actor Critic"></a>soft Actor Critic</h2><img src='/medias/image/2022-09-15-12-11-52.png' width="100%">

<h1 id="基于模型-1"><a href="#基于模型-1" class="headerlink" title="基于模型"></a>基于模型</h1><p>基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。<br>但是环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。<br><img src='/medias/image/2022-09-16-15-18-49.png' width="60%"></p>
<h2 id="Dyna-Q"><a href="#Dyna-Q" class="headerlink" title="Dyna-Q"></a>Dyna-Q</h2><p>在Q-learning的基础上，将历史样本数据保存下来，每进行一次Q-learning更新后会进行n次Q-planning。<br>Q-planning会通过历史数据来更新。</p>
<h2 id="模型预测控制"><a href="#模型预测控制" class="headerlink" title="模型预测控制"></a>模型预测控制</h2><p>模型预测控制（MPC）包含两个迭代：</p>
<ul>
<li>根据历史数据学习环境模型$P(s,a)$；</li>
<li>运用模型来选择动作。</li>
</ul>
<p>打靶：生成候选序列的过程。<br>目的：生成多个序列，找到一个序列，使得序列的累计奖励最大。<br><img src='/medias/image/2022-09-19-11-40-09.png' width="60%"><br>区别：推演多步，而非一步。</p>
<h3 id="随机打靶法"><a href="#随机打靶法" class="headerlink" title="随机打靶法"></a>随机打靶法</h3><p>生成多个序列时，动作是从动作空间中随机选取的<br>适用于简单环境</p>
<h3 id="交叉熵法"><a href="#交叉熵法" class="headerlink" title="交叉熵法"></a>交叉熵法</h3><p>交叉熵方法（cross entropy method，CEM）是一种进化策略方法：在一个带参数的分布中采样动作，并用选择的最优动作序列来更新分布<br><img src='/medias/image/2022-09-19-11-38-37.png' width="60%"></p>
<h3 id="PETS"><a href="#PETS" class="headerlink" title="PETS"></a>PETS</h3><p>带有轨迹采样的概率集成（probabilistic ensembles with trajectory sampling，PETS）：采用集成学习的思想，生成多个环境模型，最后使用CEM预测动作，每个序列的每次动作都是用不同的模型来预测。<br>每个子模型输入状态动作对，输出下一个状态的分布：<br><img src='/medias/image/2022-09-19-11-53-33.png' width="60%"><br>损失函数为：<br><img src='/medias/image/2022-09-19-11-53-56.png' width="100%"></p>
<h2 id="基于模型的策略优化"><a href="#基于模型的策略优化" class="headerlink" title="基于模型的策略优化"></a>基于模型的策略优化</h2><p>PETS等算法可以构建环境模型用于推演<br>Dyna算法可以通过历史数据来优化策略，减少交互数据的需求<br>因此<code>基于模型的策略优化(MBPO)</code>结合两者可以使用环境模型来生成数据，减少交互数据的需求</p>
<p>具体的，MBPO算法会把真实环境样本作为分支推演的起点，使用模型进行一定步数的推演，并用推演得到的模型数据用来训练模型。<br><img src='/medias/image/2022-09-19-12-20-18.png' width="100%"><br>在复杂环境中，MBPO的表现远远好于PETS算法</p>
<p>当模型误差、策略偏移程度较小时，可以采用较大的分支推演步数；反之要采用较小的分支推演步数，甚至不适合采用分支推演。</p>
<h1 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h1><h2 id="稀疏奖励"><a href="#稀疏奖励" class="headerlink" title="稀疏奖励"></a>稀疏奖励</h2><p>很多场景下奖励和惩罚出现的频率很低，即稀疏，这样训练网络非常困难。</p>
<h3 id="Reward-Shaping"><a href="#Reward-Shaping" class="headerlink" title="Reward Shaping"></a>Reward Shaping</h3><p>可以自己设计一个合适的环境reward，来引导智能体往合适的方向探索。</p>
<h4 id="Curiosity"><a href="#Curiosity" class="headerlink" title="Curiosity"></a>Curiosity</h4><p>还可以设计一个好奇分数，将好奇分数加到价值上，增加智能体的探索：<br><img src='/medias/image/2022-09-05-11-47-12.png' width="60%"></p>
<h3 id="curriculum-learning"><a href="#curriculum-learning" class="headerlink" title="curriculum learning"></a>curriculum learning</h3><p>让智能体从简单的场景、任务、数据学起，逐渐提高难度，这样可以更好地学习。<br><strong>reverse curriculum learning</strong>可以控制智能体的初始状态，开始的时候离最终状态近，更加容易学习。</p>
<h3 id="分层强化学习"><a href="#分层强化学习" class="headerlink" title="分层强化学习"></a>分层强化学习</h3><p>将一个复杂的强化学习问题分解成多个小的、简单的子问题，每个子问题都可以单独用马尔可夫决策过程来建模。这样，我们可以将智能体的策略分为高层次策略和低层次策略，高层次策略根据当前状态决定如何执行低层次策略。这样，智能体就可以解决一些非常复杂的任务。</p>
<h2 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h2><p>模仿学习（imitation learning，IL）讨论连奖励都没有的场景，或者不能直接在环境中学习的场景，又叫：示范学习（learning from demonstration），学徒学习（apprenticeship learning），观察学习（learning by watching）<br>如：聊天机器人、自动驾驶、推荐系统。</p>
<h3 id="行为克隆"><a href="#行为克隆" class="headerlink" title="行为克隆"></a>行为克隆</h3><p>类似于监督学习，让机器根据数据学习人的行为<br><img src='/medias/image/2022-09-05-12-05-05.png' width="60%"></p>
<ul>
<li>但是人的数据非常有限，很多情况覆盖不到，会出现分布偏移的情况<ul>
<li>智能体的决策和专家有区别，使得智能体发展到专家未覆盖到的场景</li>
<li>可以通过数据融合：让人类来标注新的环境；较为困难</li>
</ul>
</li>
<li>另外机器可能（过拟合）完全模范专家；<ul>
<li>学了一些没有必要的动作</li>
</ul>
</li>
</ul>
<h3 id="逆强化学习"><a href="#逆强化学习" class="headerlink" title="逆强化学习"></a>逆强化学习</h3><ol>
<li>根据专家行为来学习奖励函数；</li>
<li>根据奖励函数找到最优演员；</li>
<li>调整奖励函数使得专家奖励优于演员；</li>
<li>回到第二步继续优化演员，直到专家和演员的分数无法调整出差别即收敛。<img src='/medias/image/2022-09-05-12-17-09.png' width="60%"></li>
</ol>
<p>与生成对抗网络的区别：<br><img src='/medias/image/2022-09-05-12-19-35.png' width="60%"></p>
<p>优势：往往不需要太多数据，因为只需要少数示范。</p>
<h2 id="离线强化学习"><a href="#离线强化学习" class="headerlink" title="离线强化学习"></a>离线强化学习</h2><p>在智能体学习的早期，所做的决策还非常不好的时候，不适合将智能体的决策直接反馈到真实环境中，此时可以使用离线强化学习。<br><img src='/medias/image/2022-09-19-14-21-40.png' width="100%"><br>外推误差：是指由于当前策略可能访问到的状态动作对与从数据集中采样得到的状态动作对的分布不匹配而产生的误差。<br>外推误差在离线强化学习中非常明显，会直接导致优化失败。</p>
<h3 id="批量限制策略"><a href="#批量限制策略" class="headerlink" title="批量限制策略"></a>批量限制策略</h3><p>通过选择与最优可能的数据来减小外推误差</p>
<h2 id="目标导向的强化学习"><a href="#目标导向的强化学习" class="headerlink" title="目标导向的强化学习"></a>目标导向的强化学习</h2><ul>
<li>当环境有小幅改变时，经典的算法需要重新训练</li>
<li>许多环境的奖励信号非常稀疏</li>
</ul>
<p>目标导向的强化学习会在策略网络、值网络的输入加上一个目标，这样可以给经验库中的数据加上合适的目标使得奖励信号变得稠密；同时可以应对目标变动的情况。</p>
<h3 id="HER"><a href="#HER" class="headerlink" title="HER"></a>HER</h3><p>事后经验回放（hindsight experience replay，HER）是在数据层面应用目标导向的思想。<br><img src='/medias/image/2022-09-19-14-58-43.png' width="100%"></p>
<h2 id="多智能体"><a href="#多智能体" class="headerlink" title="多智能体"></a>多智能体</h2><p>当环境中存在多个智能体时，存在许多困难：</p>
<ul>
<li>每个智能体所面对的环境是非稳态的；</li>
<li>多个智能体的训练可能是多目标的</li>
<li>训练评估的复杂度增加</li>
</ul>
<h3 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h3><ul>
<li>完全中心化：将多个智能体进行决策当作一个超级智能体在进行决策，即把所有智能体的状态聚合在一起当作一个全局的超级状态，把所有智能体的动作连起来作为一个联合动作。<ul>
<li>环境稳态，可以保证收敛性</li>
<li>不能应对智能体数量很多或者环境很大的情况</li>
</ul>
</li>
<li>完全去中心化：每个智能体单独学习，不考虑其它智能体<ul>
<li>环境不稳态</li>
<li>有利于扩展</li>
</ul>
</li>
<li>中心化训练去中心化执行：训练的时候采用中心化，智能体可以得到一些全局信息，而执行时采用去中心化，得不到全局信息</li>
</ul>
<h3 id="IPPO"><a href="#IPPO" class="headerlink" title="IPPO"></a>IPPO</h3><p>独立 PPO（Independent PPO，IPPO）：每个智能体使用一个PPO算法单独训练<br>当不同的智能体完全同质时，可以对不同智能体进行参数共享</p>
<h3 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h3><p>采用DDPG算法，每个智能体采用不同的actor，共用相同的critic</p>
<h1 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h1><h2 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h2><p>强化学习包含许多基本模块和思路，这些模块和思路分别被提出用以解决不同问题，当所面对的场景符合时，可以尝试拿来主义，将合适的额思想进行有机的结合。</p>
<h3 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h3><ul>
<li>值函数：相对容易优化，可以评估状态或者状态动作对的价值</li>
<li>策略网络：可以直接根据状态输出动作，可用于连续动作空间</li>
<li>模型：可以用于推演预测、或生成数据，以提升智能体的泛化性、采取更优解的概率</li>
<li>离线训练缓存：可以减少对交互数据的需求</li>
</ul>
<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><h2 id="调参总结"><a href="#调参总结" class="headerlink" title="调参总结"></a>调参总结</h2><h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><ul>
<li>奖励函数的设置非常重要：<ul>
<li>要设置合适的奖励，智能体才能找到学习方向；</li>
<li>奖励的设置要平衡，不能过与密集；</li>
<li>奖励过于稀疏的话智能体很难探索到；</li>
</ul>
</li>
</ul>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><ul>
<li>多线程&amp;矢量化：提升数据采集效率</li>
<li>对环境做一些合理地包装，使得环境提供的状态信息量更充足，信息密度更大；奖励函数更加合适</li>
</ul>
<h3 id="状态空间"><a href="#状态空间" class="headerlink" title="状态空间"></a>状态空间</h3><ul>
<li>做合适的Normalize</li>
</ul>
<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><ul>
<li>及时保存较好的模型</li>
<li>将测试保存成视频<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token punctuation">,</span> cv2
<span class="token keyword">def</span> <span class="token function">test_avi</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> env<span class="token punctuation">,</span> video_folder<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    fps <span class="token operator">=</span> <span class="token number">6</span>
    img_size <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span><span class="token number">210</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>video_folder<span class="token punctuation">)</span><span class="token punctuation">:</span>
        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>video_folder<span class="token punctuation">)</span>
    video_dir <span class="token operator">=</span> video_folder <span class="token operator">+</span><span class="token string">'/'</span><span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.avi'</span>
    fourcc <span class="token operator">=</span> cv2<span class="token punctuation">.</span>VideoWriter_fourcc<span class="token punctuation">(</span><span class="token operator">*</span><span class="token string">'MJPG'</span><span class="token punctuation">)</span> <span class="token comment">#opencv3.0</span>
    videoWriter <span class="token operator">=</span> cv2<span class="token punctuation">.</span>VideoWriter<span class="token punctuation">(</span>video_dir<span class="token punctuation">,</span> fourcc<span class="token punctuation">,</span> fps<span class="token punctuation">,</span> img_size<span class="token punctuation">)</span>
    obs <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
    score <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        img_data <span class="token operator">=</span> env<span class="token punctuation">.</span>render<span class="token punctuation">(</span>mode<span class="token operator">=</span><span class="token string">'rgb_array'</span><span class="token punctuation">)</span>
        videoWriter<span class="token punctuation">.</span>write<span class="token punctuation">(</span>img_data<span class="token punctuation">)</span>
        action<span class="token punctuation">,</span> _states <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>obs<span class="token punctuation">,</span> deterministic<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        obs<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
        score <span class="token operator">+=</span> reward
        <span class="token keyword">if</span> done<span class="token punctuation">:</span>
          <span class="token keyword">break</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"score: "</span><span class="token punctuation">,</span> score<span class="token punctuation">)</span>
    env<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">万川</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://notego.top/2022/08/25/qiang-hua-xue-xi/">http://notego.top/2022/08/25/qiang-hua-xue-xi/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">万川</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%99%BA%E8%83%BD%E4%BD%93/">
                                    <span class="chip bg-color">智能体</span>
                                </a>
                            
                                <a href="/tags/%E4%BB%B7%E5%80%BC/">
                                    <span class="chip bg-color">价值</span>
                                </a>
                            
                                <a href="/tags/%E7%AD%96%E7%95%A5/">
                                    <span class="chip bg-color">策略</span>
                                </a>
                            
                                <a href="/tags/%E6%A8%A1%E5%9E%8B/">
                                    <span class="chip bg-color">模型</span>
                                </a>
                            
                                <a href="/tags/%E7%8E%AF%E5%A2%83/">
                                    <span class="chip bg-color">环境</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你好，同行的有缘人</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2022/09/22/fen-bu-shi-xun-lian/">
                    <div class="card-image">
                        
                        <img src="/medias/image/2022-09-22-15-00-17.png" class="responsive-img" alt="分布式训练">
                        
                        <span class="card-title">分布式训练</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            众人拾柴火焰高
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-09-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    机器学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/05/30/pytorch/">
                    <div class="card-image">
                        
                        <img src="/medias/image/PyTorch.jpg" class="responsive-img" alt="PyTorch">
                        
                        <span class="card-title">PyTorch</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            深度学习编程框架
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" class="post-category">
                                    编程语言
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Python/">
                        <span class="chip bg-color">Python</span>
                    </a>
                    
                    <a href="/tags/PyTorch/">
                        <span class="chip bg-color">PyTorch</span>
                    </a>
                    
                    <a href="/tags/numpy/">
                        <span class="chip bg-color">numpy</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('20')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 方寸田园<br />'
            + '文章作者: 万川<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h1, h2, h3, h4, h5'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4, h5').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2021-2022</span>
            
            <span id="year">2021</span>
            <a href="/about" target="_blank">万川</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">127k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2021";
                    var startMonth = "3";
                    var startDate = "31";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/wanc97" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:wanc97@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=3221927185" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 3221927185" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz=MzI4MzEzNjIxNw==#wechat_redirect" class="tooltipped" target="_blank" data-tooltip="微信联系我: W3221927185" data-position="top" data-delay="50">
        <i class="fab fa-weixin"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

    <!-- 雪花特效 --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</body>

</html>
