<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="自然语言处理, 方寸田园">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>自然语言处理 | 方寸田园</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="方寸田园" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>




<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">方寸田园</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">方寸田园</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/wanc97" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/wanc97" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/image/2022-09-27-10-11-30.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">自然语言处理</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/">
                                <span class="chip bg-color">词嵌入</span>
                            </a>
                        
                            <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">
                                <span class="chip bg-color">文本分类</span>
                            </a>
                        
                            <a href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                                <span class="chip bg-color">语言模型</span>
                            </a>
                        
                            <a href="/tags/seq2seq/">
                                <span class="chip bg-color">seq2seq</span>
                            </a>
                        
                            <a href="/tags/transformer/">
                                <span class="chip bg-color">transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                机器学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-09-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2022-11-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="经典方法"><a href="#经典方法" class="headerlink" title="经典方法"></a>经典方法</h1><h2 id="TF—IDF"><a href="#TF—IDF" class="headerlink" title="TF—IDF"></a>TF—IDF</h2><ul>
<li>TF（Term Frequency）：在文章中反复出现的词重要；统计各个词的词频；</li>
<li>IDF（Inverse Document Frequency）：在各个文章中都出现的词不重要，统计一个词在多少文章中出现n，总共的文章数N；$IDF=log\frac{N}{n+1}$，为词的逆文档权重；</li>
<li>将词的TF与IDF相乘得到词的TF—IDF权重<img src='/medias/image/2022-09-27-11-36-27.png' width="60%"></li>
</ul>
<h1 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><ul>
<li><p>将序列向量化：</p>
<img src='/medias/image/2022-09-27-10-43-50.png' width="60%">
学习上下文信息，表示学习，不仅仅可以用于文本，还可以拓展到各种学习上下文信息的场景中；</li>
<li><p>Look-up Table：</p>
<ul>
<li><p>通过输入词的编号从表中查找词的向量：</p>
<img src='/medias/image/2022-09-27-10-46-32.png' width="60%"></li>
<li><p>低频、不重要的词，可以统一用一个$UNK$符号表示：</p>
<img src='/medias/image/2022-09-27-10-47-55.png' width="50%"></li>
</ul>
</li>
<li><p>词的含义很大程度上可以从它所使用的场景得到：</p>
<ul>
<li>一个没有见过的新词，可以通过有相近用法的词来推测含义<img src='/medias/image/2022-09-27-10-56-44.png' width="70%"></li>
<li>词的含义与它所在的上下文有非常强的关系</li>
</ul>
</li>
</ul>
<p><strong>one-hot编码缺点</strong></p>
<ul>
<li>占空间</li>
<li>相互独立，无法计算相似度</li>
<li>编码没有任何泛化能力，无法看出词之间的关联</li>
</ul>
<h2 id="基于计数的方法"><a href="#基于计数的方法" class="headerlink" title="基于计数的方法"></a>基于计数的方法</h2><h3 id="正点互信息"><a href="#正点互信息" class="headerlink" title="正点互信息"></a>正点互信息</h3><p><code>Co-Occurence Counts</code>：计算一个词为中心词，其它词出现的数量<br><img src='/medias/image/2022-09-27-11-30-32.png' width="60%"><br><img src='/medias/image/2022-09-27-11-30-38.png' width="40%"></p>
<p><code>PMI（Pointwise Mutual Information）点互信息</code>：这一指标用来衡量两个事物之间的相关性。<br><img src='/medias/image/2022-09-27-11-28-58.png' width="40%"></p>
<p><code>Positive Pointwise Mutual Information (PPMI)</code>：为了避免PMI中为负的情况，设计了PPMI。<br><img src='/medias/image/2022-09-27-11-32-49.png' width="40%"></p>
<ul>
<li>互信息MI为0代表独立没有关系，负值同样是有关系；</li>
<li>但是语言中负关系往往不明显：<ul>
<li>即一个词和少数词的互信息是正的；</li>
<li>和绝大多数词的互信息都是很微小的负值；</li>
</ul>
</li>
<li>因此可以对这种负值截断。</li>
</ul>
<p>词向量可以看成是通过神经网络方法对PMI进行一个隐式的因式分解</p>
<h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>通过预测的方式来拉近中心词和共线背景词的联合概率或条件概率<br><img src='/medias/image/2022-09-27-11-42-03.png' width="60%"></p>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>似然：<br>$$<br>\mbox{Likelihood} = L(\theta)=\prod\limits_{t=1}^T\prod\limits_{-m\le j \le m, j\neq 0}P(w_{t+j}|{w_t}, \theta)<br>$$<br>损失函数：<br><img src='/medias/image/2022-09-27-11-45-05.png' width="60%"></p>
<p>给每个词准备两个向量，分别在词作为中心词和背景词时使用<br><img src='/medias/image/2022-09-27-11-47-02.png' width="60%"><br>可得：<br><img src='/medias/image/2022-09-27-11-47-19.png' width="60%"></p>
<p>具体其中一对词的损失函数为：<br><img src='/medias/image/2022-09-27-12-08-53.png' width="100%"></p>
<p>通过给每个词两个向量，可以使得每个向量的计算都是线性的，更加容易计算和优化</p>
<h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><h4 id="常规负采样"><a href="#常规负采样" class="headerlink" title="常规负采样"></a>常规负采样</h4><p>前面计算概率需要计算中心词或背景词与所有词的概率，计算开销大，负采样是从非背景词中选择一部分进行计算：<br>$$<br>J_{t,j}(\theta)=<br>    -\log\sigma(u_{cute}^Tv_{cat}) -<br>    \sum\limits_{w\in {w_{i_1},\dots, w_{i_K}}}\log\sigma({-u_w^Tv_{cat}}) \\<br>    -\log\sigma(u_{cute}^Tv_{cat}) -<br>    \sum\limits_{w\in {w_{i_1},\dots, w_{i_K}}}\log(1-\sigma({u_w^Tv_{cat}}\color{black}))<br>$$</p>
<p>Word2Vec按修正的词频$U^{3/4}(w)$进行采样</p>
<h4 id="层序softmax"><a href="#层序softmax" class="headerlink" title="层序softmax"></a>层序softmax</h4><p>通过二叉树来组合生成条件概率，避免了分母概率的计算。<br>可以看成是对概率进行了因式分解。<br><img src='/medias/image/2022-10-28-18-09-50.png' width="80%"><br>每个词对应二叉树的一个叶节点，此时有：<br>$$<br>P(w_3 \mid w_c) = \sigma(u_{n(w_3, 1)}^\top v_c) \cdot \sigma(-u_{n(w_3, 2)}^\top v_c) \cdot \sigma(u_{n(w_3, 3)}^\top v_c)<br>$$<br>因为：$\sigma(x)+\sigma(-x) = 1$，所以：<br>$$<br>\sum_{w \in \mathcal{V}} P(w \mid w_c) = 1<br>$$<br>计算复杂度下降到了对数级。</p>
<h4 id="NCE-loss"><a href="#NCE-loss" class="headerlink" title="NCE loss"></a>NCE loss</h4><ul>
<li>原来优化的是条件概率$P(w_o|w_c)$，是预测、重构的方式，现在优化的是互信息$\frac{P(w_o|w_c)}{P(w_o)}$，对比式。</li>
<li>非归一化</li>
<li>二分类</li>
</ul>
<h3 id="变种类型"><a href="#变种类型" class="headerlink" title="变种类型"></a>变种类型</h3><h4 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h4><ul>
<li>跳字模型</li>
<li>在给定中心词时，计算窗口内背景词的联合条件概率：$\prod_{t=1}^TP(w^{t-m},…,w^{t-1},w^{t+1},…,w^{t+m}|w^t)$；</li>
<li>联合条件概率加入朴素假设：背景词概率条件独立，最大化这个概率。最后的概率表示为：$\prod_{t=1}^T\prod_{-m\leq j \leq m,j\neq 0}P(w^{t+j}|w^{t})$</li>
<li>通过加入对数得到损失函数：$-\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j \leq m,j\neq 0}log P(w^{t+j}|w^t)$</li>
<li>每个词有两组向量，分别是作为中心词和作为背景词时的表示。用$v$表示作为中心词时的表示，用$u$表示作为背景词时的表示，可得：$P(w_o|w_c)=\frac{e^{u^T_ov_c}}{\sum_{i\in V}e^{u^T_iv_c}}$，其中$V$是所有节点。</li>
<li>带负采样的跳字模型其实是在隐式的估算了一个带偏移的点互信息的因式分解。</li>
</ul>
<h4 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h4><ul>
<li>连续词袋模型<br>在已知背景词时，计算中心词的条件概率，背景词的向量求和与中心词向量求內积，对应的有：<ul>
<li>联合条件概率：$\prod_{t=1}^TP(w^t|w^{t-m},…,w^{t-1},w^{t+1},…,w^{t+m})$</li>
<li>损失函数：$-\frac{1}{T}\sum_{t=1}^TlogP(w^t|w^{t-m},…,w^{t-1},w^{t+1},…,w^{t+m})$</li>
<li>$P(w_c|w_{O1},…,w_{O2m})=\frac{e^{u^T_c(v_{o1}+…+v_{o2m})/(2m)}}{\sum_{i\in V}e^{u^T_i(v_{o1}+…+v_{o2m})/(2m)}}$</li>
</ul>
</li>
</ul>
<img src='/medias/image/2022-09-27-12-21-01.png' width="80%">

<h3 id="其他讨论"><a href="#其他讨论" class="headerlink" title="其他讨论"></a>其他讨论</h3><h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><p>超参数的选择取决于具体任务，常见的选择：<br><img src='/medias/image/2022-09-27-12-30-10.png' width="90%"></p>
<h4 id="窗口长度"><a href="#窗口长度" class="headerlink" title="窗口长度"></a>窗口长度</h4><p>更长的窗口长度使得词中更多包含主题相似<br>而更短的窗口长度使得词包含更多词法上的相似<br>还可以尝试给更近的共现对更高的权重</p>
<h4 id="词的重要性"><a href="#词的重要性" class="headerlink" title="词的重要性"></a>词的重要性</h4><p>词频越高的词，提供的信息量越少，如介词<br>因此还可以使用基于词频的采样，以概率：$P(w_i)=1 - \sqrt{\frac{thr}{f(w_i)}}$在训练中忽略。<br>其中$f(w_i)$是词频，$thr$是选择概率，原论文中$thr=10^{-5}$，词频大于$thr$的词汇会以概率$P(w_i)$被忽略。<br>可以训练的更快、效果更好</p>
<h3 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h3><p>任何有上下文关系的离散数据都可以用Word2vec学习表示<br><strong>学习段落/文章表示</strong></p>
<ul>
<li>让一部分隐向量随段落/文章变化，这部分就可以表示段落文章<ul>
<li>Distributed representations of sentences and documents</li>
</ul>
</li>
</ul>
<p><strong>网络Embedding</strong></p>
<ul>
<li>将网络中的节点类比文本中的词，学习节点的表示<ul>
<li>LINE: Large-scale Information Network Embedding</li>
</ul>
</li>
</ul>
<p><strong>关键词/网页</strong></p>
<ul>
<li>在查询关键词和用户点击的网页之间建立上下文关系，使得Word2Vec模型可以学习到查询关键词以及网页的隐含向量。<ul>
<li>Context and Content-aware Embeddings for Query Rewriting in Sponsored Search</li>
</ul>
</li>
</ul>
<h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><p>Global Vectors for Word Representation是基于计数方法和基于预测方法的结合：<br><img src='/medias/image/2022-09-27-17-48-21.png' width="80%"><br>信息来源使用基于计数的方法得到全局信息，向量优化采用梯度下降。</p>
<p>具体的损失函数如下：<br><img src='/medias/image/2022-09-27-17-49-40.png' width="80%"></p>
<p>后者代表优化目标：</p>
<ul>
<li>即共线频数越高的组合，向量内积越大；同时学习偏置值；</li>
<li>前者代表组合权重，降低低频组合的权重，同时不过度提高高频组合的权重；</li>
</ul>
<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><img src='/medias/image/2022-09-27-17-56-42.png' width="60%">

<h3 id="直观评估"><a href="#直观评估" class="headerlink" title="直观评估"></a>直观评估</h3><p>通过词向量查看相似词，从语义上直观判断词向量效果，如：</p>
<ul>
<li>相似度高的词，语义应该相似</li>
<li>相似度高的词，类型应该相似<img src='/medias/image/2022-09-27-18-08-13.png' width="60%"></li>
<li>好的Embedding可以表示类比关系，如：国王-男人+女人$\approx$王后；<img src='/medias/image/2022-09-27-18-07-48.png' width="60%"></li>
<li>好的Embedding还可以表示不同语言之间的映射关系<img src='/medias/image/2022-09-27-18-05-46.png' width="100%">
相似有多种角度，如直接计算内积、距离范数、可视化等</li>
</ul>
<h3 id="任务评估"><a href="#任务评估" class="headerlink" title="任务评估"></a>任务评估</h3><p>通过对比实验比较不同词向量方法下实际任务的效果，来判断词向量的好坏</p>
<h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>将一个词的n-gram词都作为这个词的词向量，可以优化效果和速度<br><img src='/medias/image/2022-09-27-18-27-34.png' width="80%"></p>
<h1 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h1><h2 id="引入-1"><a href="#引入-1" class="headerlink" title="引入"></a>引入</h2><h3 id="常见类型"><a href="#常见类型" class="headerlink" title="常见类型"></a>常见类型</h3><p>文本分类常常作为很多业务的前置环节<br>常见类型：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src='/medias/image/2022-09-29-14-51-59.png' width="100%"></td>
<td><img src='/medias/image/2022-09-29-14-52-21.png' width="100%"></td>
</tr>
<tr>
<td><img src='/medias/image/2022-09-29-14-52-39.png' width="100%"></td>
<td><img src='/medias/image/2022-09-29-14-52-56.png' width="100%"></td>
</tr>
</tbody></table>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>常见的分类数据集：<br><img src='/medias/image/2022-09-29-14-49-05.png' width="60%"></p>
<h3 id="任务流程"><a href="#任务流程" class="headerlink" title="任务流程"></a>任务流程</h3><p>一般包括特征提取和分类两个环节<br><img src='/medias/image/2022-09-29-15-09-31.png' width="60%"></p>
<h3 id="模型类型"><a href="#模型类型" class="headerlink" title="模型类型"></a>模型类型</h3><p>分类模型可以是生成的，也可以是判别的<br><img src='/medias/image/2022-09-29-15-08-58.png' width="100%"></p>
<h2 id="经典方法-1"><a href="#经典方法-1" class="headerlink" title="经典方法"></a>经典方法</h2><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><img src='/medias/image/2022-09-29-15-15-17.png' width="100%">
<img src='/medias/image/2022-09-29-15-15-25.png' width="100%">

<p>因为概率乘积一般不稳定，且最终只考虑大小，因此可以通过对数进行优化：<br>$$<br>\log P(x, y=k)=\log P(y=k) + \sum\limits_{t=1}^n\log P(x_t|y=k)<br>$$</p>
<h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><p>其中类别的先验概率容易确定。<br>而具体类别下的文档概率为：<br>$$<br>P(x| y=k)=P(x_1, \dots, x_n|y=k)<br>$$</p>
<p>而朴素贝叶斯假设是：</p>
<ul>
<li>Bag of Words假设：词序无关紧要，</li>
<li>条件独立假设：特征（词）在给定类的情况下是独立的。</li>
</ul>
<p>进而有：<br>$$<br>P(x| y=k)=P(x_1, \dots, x_n|y=k)=\prod\limits_{t=1}^nP(x_t|y=k)<br>$$<br>同时可以通过计数的方式得到标签下每个词的概率：<br>$$<br>P(x_i|y=k)=\frac{N(x_i, y=k)}{\sum\limits_{t=1}^{|V|}N(x_t, y=k)}<br>$$</p>
<p>为了避免词没有出现的情况，可以加上一个较小的值来实现拉普拉斯平滑：<br>$$<br>P(x_i|y=k)=\frac{\color{red}{\delta} +\color{black} N(x_i, y=k)<br>    }{\sum\limits_{t=1}^{|V|}(\color{red}{\delta} +\color{black}N(x_t, y=k))} =<br>    \frac{\color{red}{\delta} +\color{black} N(x_i, y=k)<br>    }{\color{red}{\delta\cdot |V|}\color{black}  + \sum\limits_{t=1}^{|V|}\color{black}N(x_t, y=k)}<br>$$</p>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><p>在计算以上概率后，便可以进行预测：<br><img src='/medias/image/2022-09-29-15-26-04.png' width="100%"></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在朴素贝叶斯方法中：</p>
<ul>
<li>通过one-hot对词进行表示；</li>
<li>进而通过<code>词袋（Bag of Words）</code>来表示文本；</li>
<li>最终通过朴素贝叶斯的推导得到预测结果<img src='/medias/image/2022-09-29-15-33-38.png' width="80%"></li>
</ul>
<h4 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h4><ol>
<li>由于朴素的假设，可能使得词汇组成完全相同的文本意思完全不同：<img src='/medias/image/2022-09-29-17-12-12.png' width="50%"></li>
</ol>
<ul>
<li>可以尝试将一些频繁的N-Grams作为单词使用：<img src='/medias/image/2022-09-29-17-13-32.png' width="70%"></li>
</ul>
<ol start="2">
<li>不使用不重要的词：<img src='/medias/image/2022-09-29-17-15-26.png' width="60%"></li>
</ol>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><p>逻辑回归，又称最大熵分类，则是一种判别模型，直接关系$P(y=k|x)$，而不在意联合概率分布。<br>具体的：</p>
<ul>
<li>将文本转化为特征表示；</li>
<li>特征表示跟权重矩阵或向量计算各个类别的概率。</li>
</ul>
<p>$$<br>P(class=k|{h})=\frac{\exp(w^{(k)}{h})}{\sum\limits_{i=1}^K\exp(w^{(i)}{h})}<br>$$<br><img src='/medias/image/2022-09-29-15-41-06.png' width="100%"></p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>基本原则是对数最大似然：<br>$$<br>w^{\ast}=\arg \max\limits_{w}\sum\limits_{i=1}^N\log P(y=y^i|x^i)<br>$$</p>
<p>等价为最小化交叉熵损失函数：<br><img src='/medias/image/2022-09-29-15-48-24.png' width="100%"></p>
<h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><img src='/medias/image/2022-09-29-15-49-46.png' width="100%">

<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><img src='/medias/image/2022-09-29-16-09-10.png' width="80%">

<h2 id="神经网络方法"><a href="#神经网络方法" class="headerlink" title="神经网络方法"></a>神经网络方法</h2><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p>主要特点是通过神经网络来获取文本的特征表示<br><img src='/medias/image/2022-09-29-16-12-50.png' width="100%"><br>最后的分类部分属于逻辑回归</p>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>神经网络的模型和逻辑回归没有本质不同，因此优化过程也是由最大似然得到的交叉熵损失函数。<br><img src='/medias/image/2022-09-29-16-18-59.png' width="100%"></p>
<h3 id="模型类型-1"><a href="#模型类型-1" class="headerlink" title="模型类型"></a>模型类型</h3><h4 id="经典方式"><a href="#经典方式" class="headerlink" title="经典方式"></a>经典方式</h4><p><code>词袋嵌入（Bag of Embeddings）</code>通过将输入文本所有词的独热编码求和，或加权求和得到特征表示：<br><img src='/medias/image/2022-09-29-16-23-07.png' width="100%"><br>加权的权重可以使用tf-idf</p>
<h4 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h4><ul>
<li><p>多层：</p>
<img src='/medias/image/2022-09-29-16-27-36.png' width="60%"></li>
<li><p>双向：</p>
<img src='/medias/image/2022-09-29-16-28-06.png' width="90%"></li>
</ul>
<h4 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h4><p>一般流程：</p>
<ul>
<li>通过卷积网络对输入文本的嵌入进行特征提取，每个卷积核提取一种特征；</li>
<li>对不同位置上的卷积结果进行全局池化，得到一个固定长度的向量；</li>
<li>不同卷积核得到的全局池化结果可以拼接起来，得到最终的特征表示。</li>
</ul>
<img src='/medias/image/2022-09-29-16-34-34.png' width="100%">
卷积网络也可以使用多层。

<ul>
<li>卷积的作用类似于ngram，一个卷积核处理一类ngram家族；</li>
<li>池化可以起到阈值过滤的作用，保留最有代表性的特征，而忽略低于阈值的特征。</li>
</ul>
<h2 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h2><p>与单标签相比，只有在输出的时候需要做一些调整：</p>
<ol>
<li><p>将输出激活函数由Softmax改为Element-wise Sigmoid，使得每个类别的概率独立。</p>
<img src='/medias/image/2022-09-29-16-45-06.png' width="60%"></li>
<li><p>将损失函数由单个交叉熵函数改为多个二元交叉熵函数的和。</p>
<img src='/medias/image/2022-09-29-16-45-12.png' width="100%"></li>
</ol>
<h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="词嵌入-1"><a href="#词嵌入-1" class="headerlink" title="词嵌入"></a>词嵌入</h3><p>词嵌入有多种获得方式：</p>
<ol>
<li>重新训练；</li>
<li>通过Word2Vec、GloVe等提前获得并固定；</li>
<li>通过Word2Vec、GloVe等提前获得并继续训练。<img src='/medias/image/2022-10-08-12-03-35.png' width="100%"></li>
</ol>
<ul>
<li>由于文本分类的数据集相对较小，因此第一种方式往往信息不充足；</li>
<li>而后两种方式相当于是通过其它的海量预料进行了预训练，可以利用外部信息；</li>
<li>第3中方式通过微调训练，一般要优于第二种：<ul>
<li>因为词嵌入主要学习共现关系；</li>
<li>很多时候反义词如good和bad共现概率大，因此嵌入向量接近</li>
<li>这不利于文本分类；</li>
<li>通过微调可以缓解这个问题。</li>
</ul>
</li>
</ul>
<h3 id="数据增广"><a href="#数据增广" class="headerlink" title="数据增广"></a>数据增广</h3><h4 id="word-dropout"><a href="#word-dropout" class="headerlink" title="word dropout"></a>word dropout</h4><p>随机将一些文本标记为UNK或随机词，使得模型不要过度依赖某些词。<br><img src='/medias/image/2022-09-29-16-52-52.png' width="100%"></p>
<h4 id="同义词替换"><a href="#同义词替换" class="headerlink" title="同义词替换"></a>同义词替换</h4><p>通过外部词库，替换文本中的同义词：<br><img src='/medias/image/2022-09-29-16-54-16.png' width="100%"></p>
<h4 id="跨语言翻译"><a href="#跨语言翻译" class="headerlink" title="跨语言翻译"></a>跨语言翻译</h4><p>通过成熟的翻译模型将文本翻译成其它语言，再翻译回来：<br><img src='/medias/image/2022-09-29-16-55-52.png' width="100%"></p>
<h1 id="语言建模"><a href="#语言建模" class="headerlink" title="语言建模"></a>语言建模</h1><h2 id="引入-2"><a href="#引入-2" class="headerlink" title="引入"></a>引入</h2><ul>
<li>词的概率可以通过在语料中出现的频率计算；</li>
<li>但句子不行，没有语料可以包含所有句子；</li>
<li>所以即使很合理的句子也会在频率计算出0概率；<img src='/medias/image/2022-09-30-10-45-27.png' width="100%"></li>
</ul>
<p>语言建模对文本概况进行条件概率分解，每次计算在给定文本下，下一个词的概率：<br>$$<br>P(y_1, y_2, \dots, y_n)=P(y_1)\cdot P(y_2|y_1)\cdot P(y_3|y_1, y_2)\cdot\dots\cdot P(y_n|y_1, \dots, y_{n-1})=<br>        \prod \limits_{t=1}^n P(y_t|y_{\mbox{&lt;}t})<br>$$<br>在得到语言模型后，就可以逐个预测下一个文本。</p>
<h2 id="N-gram语言模型"><a href="#N-gram语言模型" class="headerlink" title="N-gram语言模型"></a>N-gram语言模型</h2><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>N-gram是在频率计数法$P(y_t|y_1, \dots, y_{t-1}) = \frac{N(y_1, \dots, y_{t-1}, y_t)}{N(y_1, \dots, y_{t-1})}$的基础上增加N-gram的限制，假设语言模型具有马尔科夫性$P(y_t|y_1, \dots, y_{t-1}) = \frac{N(y_1, \dots, y_{t-1}, y_t)}{N(y_1, \dots, y_{t-1})}$。<br><img src='/medias/image/2022-09-30-11-25-14.png' width="70%"></p>
<h3 id="平滑"><a href="#平滑" class="headerlink" title="平滑"></a>平滑</h3><p>为了避免分子分母为0的情况，需要做一些处理</p>
<h4 id="Backoff"><a href="#Backoff" class="headerlink" title="Backoff"></a>Backoff</h4><p>当N-gram分母为0时，可以尝试N-1-gram，持续迭代<br><img src='/medias/image/2022-09-30-11-29-06.png' width="30%"></p>
<h4 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a>线性插值</h4><p>给各个概率加权平均<br><img src='/medias/image/2022-09-30-11-41-53.png' width="40%"></p>
<h4 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h4><p>当分子为0时，给每种情况一个小概率：<br><img src='/medias/image/2022-09-30-11-43-45.png' width="40%"></p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul>
<li>因为N-gram的马尔可夫性只使用了短文本，没有考虑长文本，因此N-gram生成的文本往往不流畅。</li>
</ul>
<h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p>通过神经网络对所有过去的文本进行特征提取，得到向量表示，并预测下一个词。<br><img src='/medias/image/2022-09-30-11-51-46.png' width="80%"><br>与文本分类的流程非常相似，只是这里的类别数非常高，与词表大小相同。</p>
<p>语言模型的输出层也可以看成是词嵌入：<br><img src='/medias/image/2022-09-30-12-00-10.png' width="80%"><br>$$<br>p(y_t| y_{\mbox{&lt;}t}) = \frac{exp({h_t^T}{e_{y_t}})}{\sum\limits_{w\in V}exp({h_t^T}{e_{w}})}<br>$$<br>因此也可以通过让输入词嵌入和输出层参数共享，来降低参数量，提升模型的泛化能力。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>让输出概率分布等于设定独热分布：<br><img src='/medias/image/2022-09-30-12-05-31.png' width="60%"><br>因为交叉熵等价于KL散度$D_{KL}(p^{\ast}|| p)$：<br>$$<br>Loss(p^{\ast}, p^{})= - p^{\ast} \log(p) = -\sum\limits_{i=1}^{|V|}p_i^{\ast} \log(p_i) \\<br>    = -\log(p_{y_t})=-\log(p(y_t| y_{\mbox{&lt;}t}))<br>$$</p>
<h3 id="建模模型"><a href="#建模模型" class="headerlink" title="建模模型"></a>建模模型</h3><h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><img src='/medias/image/2022-09-30-12-08-15.png' width="40%">

<h4 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h4><p>与文本分类不同，在语言模型中：</p>
<ul>
<li><p>CNN一般不用池化层；</p>
</li>
<li><p>并在开头用padding：</p>
</li>
<li><p>要保证前面的预测不会用到后面的信息；</p>
</li>
<li><p>堆叠多层来获得足够的感受野；</p>
<img src='/medias/image/2022-09-30-12-09-48.png' width="100%"></li>
<li><p>还可以通过残差连接来提升训练效果</p>
<img src='/medias/image/2022-09-30-12-13-32.png' width="60%"></li>
</ul>
<h2 id="生成策略"><a href="#生成策略" class="headerlink" title="生成策略"></a>生成策略</h2><p>贪婪的生成策略往往不能保证语言的连贯性和多样性，需要在生成过程中加入一些处理。</p>
<h3 id="贪婪解码"><a href="#贪婪解码" class="headerlink" title="贪婪解码"></a>贪婪解码</h3><p>不做任何处理</p>
<h3 id="标准采样"><a href="#标准采样" class="headerlink" title="标准采样"></a>标准采样</h3><p>不一定采用概率最大的值，而是依据生成的分布采样</p>
<h3 id="带温度的采样"><a href="#带温度的采样" class="headerlink" title="带温度的采样"></a>带温度的采样</h3><p>在softmax前除以一个温度值，可以调整输出的分布：<br><img src='/medias/image/2022-09-30-12-18-42.png' width="60%"></p>
<ul>
<li>温度越高时，分布越平均，随机性和多样性越高，越发散没意义</li>
<li>温度越低时，分布越极端，更加确定和重复，<img src='/medias/image/2022-09-30-12-17-57.png' width="100%">
<img src='/medias/image/2022-09-30-14-00-54.png' width="60%"></li>
</ul>
<h3 id="Top-K采样"><a href="#Top-K采样" class="headerlink" title="Top-K采样"></a>Top-K采样</h3><p>只在概率值Top-K的词中采样</p>
<ul>
<li>可以去掉非常不可能的词</li>
<li>但是固定的K不一定好<img src='/medias/image/2022-09-30-14-05-15.png' width="80%"></li>
</ul>
<h3 id="Top-p采样"><a href="#Top-p采样" class="headerlink" title="Top-p采样"></a>Top-p采样</h3><p>只在概率和大于p%的词中采样，可采样数是动态的</p>
<img src='/medias/image/2022-09-30-14-06-55.png' width="80%">

<h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>除了在单步的词选择上进行优化外，还可以在多步之间进行优化<br>单步贪婪选择时存在一下问题：<br><img src='/medias/image/2022-09-30-14-56-59.png' width="50%"><br>即贪心解并不一定是最优解</p>
<p>Beam Search在每个时刻会保存概率最高的N个序列，可以更大概率的取到更优解<br><img src='/medias/image/2022-09-30-15-02-31.png' width="60%"><br>beam size 一般设为4-10</p>
<h2 id="评估方式"><a href="#评估方式" class="headerlink" title="评估方式"></a>评估方式</h2><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>反应模型对于文本是否流畅的认可度<br><img src='/medias/image/2022-09-30-14-10-49.png' width="80%"></p>
<h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p><code>困惑度（Perplexity）</code>：$Perplexity(y_{1:M})=2^{-\frac{1}{M}L(y_{1:M})}$</p>
<p>是损失函数的一种变化：</p>
<ul>
<li>最佳的困惑度是1，也就是模型总是以1的概率正确预测词</li>
<li>最坏的困惑度是$|V|$<ul>
<li>表面上困惑度的值还可以增加到正无穷，但其实这种情况是学习到了某种反向的相关性<br>$$<br>Perplexity(y_{1:M})=2^{-\frac{1}{M}L(y_{1:M})} = 2^{-\frac{1}{M}\sum\limits_{t=1}^M\log_2 p(y_t|y_{1:t-1})}=2^{-\frac{1}{M}\cdot M \cdot \log_2\frac{1}{|V|}}=2^{\log_2 |V|} =|V|<br>$$</li>
</ul>
</li>
</ul>
<h1 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h1><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>从一个序列到另一个序列的任务，一般包含编码器和解码器两个组件：<br><img src='/medias/image/2022-09-30-14-40-17.png' width="60%"></p>
<p>最常见的seq2seq任务是机器翻译<br><img src='/medias/image/2022-09-30-14-38-52.png' width="80%"></p>
<p>seq2seq任务可以看成是条件语言模型<br><img src='/medias/image/2022-09-30-14-41-17.png' width="60%"><br><img src='/medias/image/2022-09-30-14-42-53.png' width="100%"><br>当然，条件语言模型中的条件$x$并不一定要是文本，也可以是图像等其它形式</p>
<p><strong>损失函数</strong>和<strong>生成推理</strong>的解码问题与语言模型一致</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="RNN-1"><a href="#RNN-1" class="headerlink" title="RNN"></a>RNN</h3><p>最经典的结构<br><img src='/medias/image/2022-09-30-14-45-52.png' width="100%"><br>RNN编码器将输入的条件句转化为一个向量表示；<br>通过可视化可以发现，有相似意思的输入句子，在向量特征空间中比较接近：<br><img src='/medias/image/2022-09-30-14-47-48.png' width="100%"></p>
<h3 id="注意力"><a href="#注意力" class="headerlink" title="注意力"></a>注意力</h3><h4 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h4><p>RNN编码器将输入句子的所有信息都压缩为一个向量；<br>这个过程会存在损失信息，且不相关的信息难以压缩的问题。<br>因此编码器与解码器之间的特征向量成了瓶颈。<br>注意力机制用待解码的特征向量与所有输入位置的特征向量计算出一个注意力分数：<br><img src='/medias/image/2022-09-30-15-11-56.png' width="100%"><br>这个注意力分数进一步归一化得到权重，与所有输入位置的特征向量加权求和得到最终的待解码特征向量：<br><img src='/medias/image/2022-09-30-15-12-09.png' width="80%"></p>
<h4 id="注意力计算"><a href="#注意力计算" class="headerlink" title="注意力计算"></a>注意力计算</h4><p>注意力分数有多种方式可以计算，包括：</p>
<ul>
<li>点积；</li>
<li>双线性bilinear函数，又称Luong attention，原论文中用的单向RNN；</li>
<li>多层感知机multi-layer perceptron，又称Bahdanau attention，原论文中用了双向RNN。<img src='/medias/image/2022-09-30-15-16-14.png' width="100%"></li>
</ul>
<h4 id="注意力对齐"><a href="#注意力对齐" class="headerlink" title="注意力对齐"></a>注意力对齐</h4><p>通过观察注意力分数，可以提供可解释性<br>例如在翻译任务中可以知道是什么翻译为了什么<br><img src='/medias/image/2022-09-30-15-22-31.png' width="100%"></p>
<h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><h4 id="总览-1"><a href="#总览-1" class="headerlink" title="总览"></a>总览</h4><img src='/medias/image/2022-09-30-15-52-41.png' width="100%">
只使用注意力，不使用RNN
<img src='/medias/image/2022-09-30-15-36-28.png' width="80%">

<ul>
<li>其中编码器部分包含自注意力</li>
<li>解码器包含自注意力和注意力</li>
</ul>
<img src='/medias/image/2022-09-30-15-39-07.png' width="90%">

<p>相比RNN：</p>
<ul>
<li>并行度高，更快的速度；</li>
<li>可以捕捉长时间依赖</li>
</ul>
<img src='/medias/image/2022-09-30-15-41-29.png' width="60%">

<p>计算复杂度：<br><img src='/medias/image/2022-10-31-18-26-54.png' width="100%"></p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><img src='/medias/image/2022-09-30-15-43-30.png' width="100%">
<img src='/medias/image/2022-09-30-15-43-44.png' width="60%">

<h4 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self-Attention"></a>Masked Self-Attention</h4><p>在解码器中，各个时间步也可以并行；<br>但是需要通过Masked阻止模型看到未来的信息。</p>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p>让模型同时关注不同的角度<br>$$<br>\mbox{head}_i=\mbox{Attention}(QW_Q^i, KW_K^i, VW_V^i) \\<br>\mbox{MultiHead}(Q, K, V) = \mbox{Concat}(\mbox{head}_1, \dots, \mbox{head}_n)W_o<br>$$<br><img src='/medias/image/2022-09-30-15-51-00.png' width="40%"></p>
<p>随着训练的进行，还可以去掉一些没有学到东西的头，以降低复杂度</p>
<h4 id="前馈块"><a href="#前馈块" class="headerlink" title="前馈块"></a>前馈块</h4><p>$$<br>FFN(x) = ReLU(xW_1+b_1)W_2+b_2<br>$$<br><img src='/medias/image/2022-09-30-15-54-12.png' width="30%"></p>
<h4 id="残差连接-amp-层标准化"><a href="#残差连接-amp-层标准化" class="headerlink" title="残差连接&amp;层标准化"></a>残差连接&amp;层标准化</h4><table>
<thead>
<tr>
<th>残差连接</th>
<th>层标准化</th>
</tr>
</thead>
<tbody><tr>
<td><img src='/medias/image/2022-09-30-15-56-49.png' width="100%"></td>
<td><img src='/medias/image/2022-09-30-15-56-58.png' width="100%"></td>
</tr>
</tbody></table>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>transformer不能得到词在序列中的位置，而这是十分重要的<br>位置编码主动记录这种信息<br>$$<br>PE_{pos, 2i}=\sin(pos/10000^{2i/d_{model}}) \\<br>PE_{pos, 2i+1}=\cos(pos/10000^{2i/d_{model}})<br>$$<br><img src='/medias/image/2022-09-30-16-01-23.png' width="50%"></p>
<h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="引入-3"><a href="#引入-3" class="headerlink" title="引入"></a>引入</h3><p>很多场景下数据量较小，难以训练出较好地模型，可以考虑将其它场景下训练好的包好丰富信息的模型迁移到目标场景。</p>
<h3 id="类别"><a href="#类别" class="headerlink" title="类别"></a>类别</h3><img src='/medias/image/2022-10-08-12-10-05.png' width="80%">

<h2 id="词嵌入-2"><a href="#词嵌入-2" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>如前文提到的，将词嵌入应用到文本分类等场景中：可以利用大量无标签文本对词嵌入进行预训练，然后将词嵌入应用到文本分类任务中微调，可以同时使用两个场景的数据信息。</p>
<h2 id="基于语言模型的词嵌入"><a href="#基于语言模型的词嵌入" class="headerlink" title="基于语言模型的词嵌入"></a>基于语言模型的词嵌入</h2><p>词嵌入方法得到的词表示仅仅包含的词的使用信息，而通过语言模型得到的词表示可以包含词所在的上下文信息。</p>
<p>因此基于语言模型的词嵌入：</p>
<ul>
<li>不是简单地训练并迁移词的嵌入向量；</li>
<li>而是训练并迁移一个语言模型，使得新场景可以直接得到上下文的表示。<img src='/medias/image/2022-10-08-14-16-26.png' width="80%"></li>
</ul>
<h3 id="CoVe"><a href="#CoVe" class="headerlink" title="CoVe"></a>CoVe</h3><p>CoVe通过翻译的预训练任务来得到上下文词向量，具体的：</p>
<ul>
<li><p>设计一个机器翻译的任务：</p>
<img src='/medias/image/2022-10-08-14-21-50.png' width="60%"></li>
<li><p>将训练好的模型的编码器部分拿出来用在下游任务中，具体的将编码器的输出和词嵌入结合，其中包含了丰富的信息：</p>
<img src='/medias/image/2022-10-08-14-22-41.png' width="40%"></li>
</ul>
<h3 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h3><p>在CoVe的基础上：</p>
<ul>
<li>将翻译任务改为了语言模型；</li>
<li>不是直接查表得到词的嵌入而是使用一个字符级别的网络学习，可以扩展到不认识的词。<img src='/medias/image/2022-10-08-14-29-58.png' width="100%"></li>
</ul>
<p>最后将各个层的表示加权级联：<br><img src='/medias/image/2022-10-08-14-30-33.png' width="80%"></p>
<h2 id="基于模型的迁移"><a href="#基于模型的迁移" class="headerlink" title="基于模型的迁移"></a>基于模型的迁移</h2><p>预训练环境不是只提供词表示，而是提供整个模型。<br>下游任务不需要设计各自的网络，只需要处理输入输出即可微调。<br><img src='/medias/image/2022-10-08-14-50-46.png' width="60%"></p>
<h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><p>生成式预训练语言模型，相当于transformer的解码器部分，在预训练后可以用于各种任务：<br><img src='/medias/image/2022-10-08-14-59-32.png' width="100%"><br>图中的<code>start</code>等是特殊符号<br>预训练阶段的损失函数为语言模型的损失函数：$L_{xent}=-\sum\limits_{t=1}^n\log(p(y_t| y_{\mbox{&lt;}t}))$<br>微调阶段的损失函数为语言模型的损失函数加上任务的损失函数：$L = L_{xent} + \lambda \cdot L_{task}$</p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>来自transformer的预训练双向编码器表示，相当于transformer的编码器部分。特点：</p>
<ul>
<li>transformer的encode；</li>
<li>预训练+微调；</li>
<li>双向；</li>
<li>不同任务上只需要改最后一层；</li>
</ul>
<p>BERT有两种预训练任务：NSP、MLM</p>
<h4 id="NSP"><a href="#NSP" class="headerlink" title="NSP"></a>NSP</h4><p>分类任务，输入两个句子的连接，判断两个句子是否是前后句。<br>输入由句子的嵌入、位置嵌入、段落嵌入组成<br>正例和负例的比例各占一半</p>
<h4 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h4><p>随机将一部分词改为mask、其他词或不变，在输出预测这些词<br><img src='/medias/image/2022-10-08-15-15-07.png' width="80%"><br>即：完形填空</p>
<h2 id="Adapter"><a href="#Adapter" class="headerlink" title="Adapter"></a>Adapter</h2><p>为了减少微调的参数量，适配器在预训练模型中插入一些参数量小的模块，并只训练这些模块：<br><img src='/medias/image/2022-10-08-15-29-27.png' width="80%"><br><img src='/medias/image/2022-10-08-15-29-38.png' width="100%"></p>
<h1 id="聊天机器人"><a href="#聊天机器人" class="headerlink" title="聊天机器人"></a>聊天机器人</h1><h2 id="开放聊天"><a href="#开放聊天" class="headerlink" title="开放聊天"></a>开放聊天</h2><h3 id="规则型"><a href="#规则型" class="headerlink" title="规则型"></a>规则型</h3><p><strong>正则化匹配</strong>：<code>人工智能标记语言（AIML）</code>是一种基于XML标准的声明式语言，它规定了可以在机器人中使用的编程构想和数据结构。<br>可以基于<code>人工智能标记语言（AIML）</code>来定义聊天机器人的模式和回复，构建模式匹配聊天机器人<br><img src='/medias/image/2022-11-04-10-48-03.png' width="100%"><br>有答案就直接返回；<br>没有答案可以通过一定的规则进行泛化匹配；</p>
<h3 id="主题知识问答"><a href="#主题知识问答" class="headerlink" title="主题知识问答"></a>主题知识问答</h3><p>包括：</p>
<ul>
<li>主题知识整理；</li>
<li>主题实体词识别；</li>
<li>主题知识点匹配；</li>
<li>主题无答案个性化回复；</li>
</ul>
<img src='/medias/image/2022-11-04-10-53-06.png' width="60%">
中文命名实体识别（Named Entity Recognition，NER）是指识别中文文本中实体的边界和类别。命名实体识别是文本处理中的基础技术，广泛应用在自然语言处理、推荐系统、知识图谱等领域，比如推荐系统中的基于实体的用户画像、基于实体召回等。

<h3 id="图谱问答"><a href="#图谱问答" class="headerlink" title="图谱问答"></a>图谱问答</h3><img src='/medias/image/2022-11-04-11-03-44.png' width="100%">
无歧义实体答案合成：当问题不直接匹配时，可以根据图谱找到合适的节点
歧义实体答案合成：当问题不直接匹配且实体存在多个可能选项时，可以反问用户

<h3 id="生成式问答"><a href="#生成式问答" class="headerlink" title="生成式问答"></a>生成式问答</h3><p>在seq2seq的基础上加入合适的信息，如主题、情绪、性别等等，得到基于情绪的回答：<br><img src='/medias/image/2022-11-04-12-07-28.png' width="100%"></p>
<p>在输入上加入风格特征，得到基于风格化的回答：<br><img src='/medias/image/2022-11-04-12-12-06.png' width="100%"></p>
<h2 id="机器阅读"><a href="#机器阅读" class="headerlink" title="机器阅读"></a>机器阅读</h2><p>阅读材料，根据提问给出答案</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">万川</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://notego.top/2022/09/27/zi-ran-yu-yan-chu-li/">http://notego.top/2022/09/27/zi-ran-yu-yan-chu-li/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">万川</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/">
                                    <span class="chip bg-color">词嵌入</span>
                                </a>
                            
                                <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">
                                    <span class="chip bg-color">文本分类</span>
                                </a>
                            
                                <a href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                                    <span class="chip bg-color">语言模型</span>
                                </a>
                            
                                <a href="/tags/seq2seq/">
                                    <span class="chip bg-color">seq2seq</span>
                                </a>
                            
                                <a href="/tags/transformer/">
                                    <span class="chip bg-color">transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你好，同行的有缘人</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2022/10/22/ren-de-quan-jing-bi-ji/">
                    <div class="card-image">
                        
                        <img src="/medias/image/2022-10-22-11-56-25.png" class="responsive-img" alt="《人的全景》笔记">
                        
                        <span class="card-title">《人的全景》笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            认识我自己
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-10-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-category">
                                    读书笔记
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%85%A8%E5%9B%A0%E6%A8%A1%E5%9E%8B/">
                        <span class="chip bg-color">全因模型</span>
                    </a>
                    
                    <a href="/tags/%E5%B9%B3%E8%A1%A1%E6%80%81/">
                        <span class="chip bg-color">平衡态</span>
                    </a>
                    
                    <a href="/tags/%E6%80%9D%E7%BB%B4%E4%BD%93%E6%93%8D/">
                        <span class="chip bg-color">思维体操</span>
                    </a>
                    
                    <a href="/tags/%E8%B4%9F%E5%8F%8D%E9%A6%88/">
                        <span class="chip bg-color">负反馈</span>
                    </a>
                    
                    <a href="/tags/%E6%AF%94%E8%BE%83/">
                        <span class="chip bg-color">比较</span>
                    </a>
                    
                    <a href="/tags/%E7%BB%8F%E9%AA%8C%E4%BB%B7%E5%80%BC%E6%B8%85%E5%8D%95/">
                        <span class="chip bg-color">经验价值清单</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/09/22/fen-bu-shi-xun-lian/">
                    <div class="card-image">
                        
                        <img src="/medias/image/2022-09-22-15-00-17.png" class="responsive-img" alt="分布式训练">
                        
                        <span class="card-title">分布式训练</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            众人拾柴火焰高
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-09-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    机器学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/">
                        <span class="chip bg-color">数据并行</span>
                    </a>
                    
                    <a href="/tags/%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C/">
                        <span class="chip bg-color">模型并行</span>
                    </a>
                    
                    <a href="/tags/Ring-Allreduce/">
                        <span class="chip bg-color">Ring-Allreduce</span>
                    </a>
                    
                    <a href="/tags/horovod/">
                        <span class="chip bg-color">horovod</span>
                    </a>
                    
                    <a href="/tags/deepspeed/">
                        <span class="chip bg-color">deepspeed</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('20')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 方寸田园<br />'
            + '文章作者: 万川<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h1, h2, h3, h4, h5'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4, h5').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2021-2022</span>
            
            <span id="year">2021</span>
            <a href="/about" target="_blank">万川</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">134.7k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2021";
                    var startMonth = "3";
                    var startDate = "31";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/wanc97" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:wanc97@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=3221927185" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 3221927185" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz=MzI4MzEzNjIxNw==#wechat_redirect" class="tooltipped" target="_blank" data-tooltip="微信联系我: W3221927185" data-position="top" data-delay="50">
        <i class="fab fa-weixin"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

    <!-- 雪花特效 --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</body>

</html>
