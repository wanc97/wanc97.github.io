<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="机器学习, 方寸田园">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>机器学习 | 方寸田园</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="方寸田园" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>




<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">方寸田园</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">方寸田园</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/wanc97" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/wanc97" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/image/ml2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">机器学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%AE%97%E6%B3%95/">
                                <span class="chip bg-color">算法</span>
                            </a>
                        
                            <a href="/tags/%E6%A8%A1%E5%9E%8B/">
                                <span class="chip bg-color">模型</span>
                            </a>
                        
                            <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">监督学习</span>
                            </a>
                        
                            <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">无监督学习</span>
                            </a>
                        
                            <a href="/tags/%E7%89%B9%E5%BE%81/">
                                <span class="chip bg-color">特征</span>
                            </a>
                        
                            <a href="/tags/%E8%BF%87%E6%8B%9F%E5%90%88/">
                                <span class="chip bg-color">过拟合</span>
                            </a>
                        
                            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">机器学习</span>
                            </a>
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                            <a href="/tags/%E6%A6%82%E7%8E%87/">
                                <span class="chip bg-color">概率</span>
                            </a>
                        
                            <a href="/tags/%E6%B3%9B%E5%8C%96/">
                                <span class="chip bg-color">泛化</span>
                            </a>
                        
                            <a href="/tags/%E5%81%8F%E5%B7%AE/">
                                <span class="chip bg-color">偏差</span>
                            </a>
                        
                            <a href="/tags/%E6%96%B9%E5%B7%AE/">
                                <span class="chip bg-color">方差</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                机器学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-03-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2022-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    23.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><ul>
<li>向量&amp;矩阵&amp;张量：一种组织数据的方式，如：类别、语音信号、图像、视频等。</li>
<li>范数：度量单个向量的大小；<ul>
<li>p-范数：当p为0时，范数值为非零元素个数；p为1时，范数值为各个元素的绝对值之和，p为2时，范数值为常见的欧几里得长度；p为无穷时，范数值为各个元素的最大值；即p越大时，绝对值越大的元素起到的作用越大。</li>
</ul>
</li>
<li>内积：两个向量之间的关系。</li>
<li>矩阵运算是一种线性变换，既可以看成是对数据（向量进行变换），也可以看成是对坐标系进行变换。</li>
<li>矩阵的特征向量表示线性变换的方向，从左（右）特征向量的方向变换到右（左）特征向量的方向；</li>
<li>特征值则代表了具体某个方向变换的伸缩系数。</li>
</ul>
<h3 id="概率-amp-统计"><a href="#概率-amp-统计" class="headerlink" title="概率&amp;统计"></a>概率&amp;统计</h3><table>
<thead>
<tr>
<th></th>
<th>概率</th>
<th>统计</th>
</tr>
</thead>
<tbody><tr>
<td>集中趋势</td>
<td>分布期望</td>
<td>样本均值、中位数、众数</td>
</tr>
<tr>
<td>分散度</td>
<td>分布方差/标准差、分位点</td>
<td>样本方差/标准差、全距、四分距</td>
</tr>
</tbody></table>
<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p><strong>统计模型</strong></p>
<ul>
<li>$p(x;\theta)$：随机变量$x$受参数$\theta$影响；</li>
</ul>
<p><strong>随机样本</strong></p>
<ul>
<li>从统计模型中独立同分布的生成的实例；</li>
</ul>
<p><strong>统计集中趋势</strong></p>
<ul>
<li>均值：为这批数据找到它们的“代表”<ul>
<li>局限：把众多的数据用一个数据表示出来必然会丢失许多信息，如：数据分布情况会因异常值而产生巨大偏离。（张家有钱一千万，邻居九个穷光蛋，平均各个张百万）</li>
</ul>
</li>
<li>中位数：中点数，中值；是按顺序排列的一组数据中居于中间位置的数。<ul>
<li>局限：当数据有多个不同的部分组成时，中位数也没有太多的代表性。</li>
</ul>
</li>
<li>众数：样本观测值在频数分布表中频数最多的那一组的组中值。</li>
</ul>
<p><strong>统计分散度</strong><br>平均数可以表征一批数据的典型值，但是仅凭平均数还不能给我们提供足够的信息，平均数无法表征一组数据的分散程度。</p>
<ul>
<li>全距=max-min：也叫极差。它是一组数据中最大值与最小值之差。可以用于度量数据的分散程度。<ul>
<li>局限：全距虽然求解方便快捷，但是它的局限性在于“若数据中存在异常值的情况，会产生偏差。为了摆脱异常值带来的干扰，比如我们看一下下面的两组数据。只是增加了一个异常值，两组数据的全距产生了巨大的差异。</li>
</ul>
</li>
<li>四分位数：所有观测值从小到大排序后四等分，处于三个分割点位置的数值就是四分位数：Q1，Q2和Q3。</li>
<li>迷你距：也叫四分位距；一组数据中较小四分位数与较大四分位数之差；迷你距= 上四分位数 - 下四分位数。</li>
<li>全距，四分位距，箱形图可以表征一组数据极大和极小值之间的差值跨度，一定程度上反应了数据的分散程度，但是却无法精准的告诉我们，这些数值具体出现的频率；我们度量每批数据中数值的“变异”程度时，可以通过观察每个数据与均值的距离来确定，各个数值与均值距离越小，变异性越小数据越集中，距离越大数据约分散，变异性越大。<ul>
<li>方差：方差是度量数据分散性的一种方法，是数值与均值的距离的平方数的平均值。</li>
<li>标准差：方差的开方。</li>
</ul>
</li>
<li>标准分：$Z=\frac{X-\mu}{\sigma}$，表征了距离均值的标准差的个数，可以用于有不同均值和不同标准差的多个数据集之间的数据比较。</li>
</ul>
<p><strong>概率&amp;似然</strong></p>
<ul>
<li>概率是描述在确定分布下某个事件发生可能性的大小；</li>
<li>似然是描述在发生某些事件的情况下分布取某组参数的可能性。</li>
</ul>
<p><strong>P-value</strong></p>
<ul>
<li>一个事件的P-value不是这个事件的概率值，而是所有概率小于等于该事件概率值得概率和（积分），即反应该事件发生的稀有程度。</li>
</ul>
<p><strong>划分</strong></p>
<ul>
<li>互斥且并集为整个样本空间的一组事件称为样本空间的一个划分</li>
</ul>
<p><strong>条件概率</strong></p>
<ul>
<li>在事件A发生时事件B发生的条件概率为：事件A和事件B共同发生时的联合概率除以事件A发生的概率；</li>
<li>当事件A和事件B独立时，其联合概率等于各自发生概率的乘积，因此事件B发生的条件概率也等于事件B本身的概率。</li>
</ul>
<p><strong>全概率公式</strong></p>
<ul>
<li>一个事件的概率可以分为在一个划分下不同事件的概率与该在该事件下的条件概率乘积的和。</li>
</ul>
<p><strong>矩</strong></p>
<ul>
<li>即一种关于随机变量的期望；</li>
<li>原点矩：n阶原点矩就是随机变量的n次方的期望，如期望是一阶原点矩；</li>
<li>中心距：即随机变量通过期望去中心化后的期望，如方差是二阶中心距；</li>
</ul>
<p><strong>不相关&amp;独立</strong></p>
<ul>
<li>不相关：$E(x,y)=E(x)E(y)$，主要是指没有线性关系，统计上的没有关系。</li>
<li>独立：$P(x,y)=\frac{P(x,y)P(y)}{P(y)}=P(x|y)P(y)=P(x)P(y)$，没有任何关系，一个事件的情况不影响另一个事件的概率。</li>
</ul>
<p><strong>自变量/协变量/因变量</strong></p>
<ul>
<li>在函数$y=f(x,b)$中$x$和$b$都可以影响$y$，我们操纵$x$作为输入，而$b$独立存在不控制；称$x$为自变量，$b$为协变量，$y$为因变量；</li>
</ul>
<h4 id="概率分布"><a href="#概率分布" class="headerlink" title="概率分布"></a>概率分布</h4><ul>
<li>随机事件发生的概率符合一定分布；</li>
<li>期望、方差等是描述分布的指标，但不等于分布；</li>
<li>$分布=分布描述+参数$，分布描述如：高斯分布、二项分布等，分布描述决定分布的基本属性，参数决定分布的具体细节；</li>
<li>连续分布：高斯分布、指数分布；离散分布：0-1/伯努利分布、二项分布、几何分布、泊松分布；</li>
</ul>
<h5 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h5><ul>
<li>一维：$P(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$</li>
<li>高维：$P(x)=\frac{1}{\sqrt{(2\pi)^d|\Sigma|}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$<ul>
<li>其中$\Sigma$是对称的协方差矩阵，所以一定可以相似对角化，得到几个不相关的高斯分布；</li>
<li>等密度点为超椭球形；</li>
<li>边缘分布与条件分布都是高斯分布</li>
<li>高斯分布的不相关等于独立</li>
<li>高斯分布的线性变换/组合仍然是高斯分布；</li>
</ul>
</li>
</ul>
<h5 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h5><ul>
<li>$P(x=1;\theta)=\theta,P(x=0;\theta)=1-\theta$</li>
</ul>
<h5 id="指数分布族"><a href="#指数分布族" class="headerlink" title="指数分布族"></a>指数分布族</h5><ul>
<li>意义：保凸性</li>
<li>基本形式：$p(x;\theta)=h(x)e^{\eta(\theta)T(x)-A(\theta)}$</li>
<li>伯努利分布：$p(x;\theta)=e^{I(x=1)ln(\theta)+I(x=0)ln(1-\theta)}$</li>
</ul>
<h3 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h3><p><strong>凸优化问题</strong></p>
<ul>
<li>凸函数在凸集上优化</li>
<li>凸集要求：<ul>
<li>等式约束：仿射函数<ul>
<li>仿射函数：最高次数为1的多项式函数。</li>
<li>常数项为零的仿射函数称为线性函数。</li>
</ul>
</li>
<li>不等式约束为:凸函数&lt;=0 或 凹函数&gt;=0</li>
</ul>
</li>
<li>具体形式如下：<br>$$min_{\vec{w}}f(\vec{w})$$<br>$$s.t.\qquad g_j(\vec{w})\le0,j=1,2,…,J\qquad h_k(\vec{w})=0,k=1,2,…,K$$</li>
<li>其中$f(\vec{w})$和$g_j(\vec{w})$都是$R^n$上的连续可微凸函数；$h_k(\vec{w})$是仿射函数。</li>
</ul>
<p><strong>拉格朗日乘子法</strong></p>
<ul>
<li>可以将带约束求极值的问题转化为无约束求极值问题</li>
<li>通过引入松弛变量$\lambda\ge0$和拉格朗日乘子$\mu$可设计函数：<br>$$L(\vec{w},\lambda,\mu)=f(\vec{w})+\sum_j\lambda_jg_j(\vec{w})+\sum_k\mu_kh_k(\vec{w})$$</li>
<li>同时由于一种奇妙的性质：<br>$$max_{\lambda,\mu}L(\vec{w},\lambda,\mu)={f(\vec{w}),\vec{w}满足约束 \atop \infty,\vec{w}不满足约束}$$</li>
<li>使得原问题转化为了无约束的问题：<br>$$min_{\vec{w}}max_{\lambda,\mu}L(\vec{w},\lambda,\mu)$$</li>
<li>对偶问题即通过交换极小、极大地顺序而来,通过对偶问题往往能降低难度：<br>$$max_{\lambda,\mu}min_{\vec{w}}L(\vec{w},\lambda,\mu)$$</li>
<li>弱对偶性：<br>$$max_{\lambda,\mu}min_{\vec{w}}L(\vec{w},\lambda,\mu)\le min_{\vec{w}}max_{\lambda,\mu}L(\vec{w},\lambda,\mu)$$</li>
<li>强对偶性：<br>$$max_{\lambda,\mu}min_{\vec{w}}L(\vec{w},\lambda,\mu)=min_{\vec{w}}max_{\lambda,\mu}L(\vec{w},\lambda,\mu)$$</li>
<li>对于满足强对偶性的情况，对偶问题得到的解就是原问题的解；对于不满足强对偶性，对偶问题的解不一定是原问题的解。</li>
<li>对于凸优化问题，KKT条件与强对偶性互为充要；对于非凸优化问题，KKT条件是强对偶性的必要条件，还需要加上二阶导数正定判断。</li>
</ul>
<p><strong>KKT条件原理</strong></p>
<ul>
<li>KKT条件为：<ul>
<li>$\nabla_{\vec{w}}{L(\vec{w},\lambda,\mu)}=0$</li>
<li>$\lambda_j\ge 0,j=1,2,…,J$</li>
<li>$\lambda_jg_j(\vec{w})=0,j=1,2,…,J$</li>
<li>$g_j(\vec{w})\le0,j=1,2,…,J$</li>
<li>$h_k(\vec{w})=0,k=1,2,…,K$</li>
</ul>
</li>
<li>对于等式约束：<ul>
<li>等同于将可行域限定在等式约束所代表的超平面上；</li>
<li>如果极值点处的梯度与超平面不垂直，则此处还有优化空间，即此处不是极值点；</li>
<li>所以极值点处函数的梯度一定与超平面垂直，即函数的梯度一定与超平面的梯度共线；</li>
<li>如果有多个等式约束，则函数的梯度在各个超平面梯度组成的子空间中。</li>
<li>由此可以得到等式约束极值必要条件（第一条）。</li>
</ul>
</li>
<li>对于不等式约束（统一采用小于或等于）<ul>
<li>可以将约束分成起作用的约束和不起作用的约束；</li>
<li>对于起作用的不等式约束，分析方法同等式约束，即函数的梯度一定与约束函数的梯度反向（不仅要共线，因为如果梯度同向则此约束暂时不起作用）</li>
<li>如果多个不等式约束同时作用，则函数的负梯度一定在各个约束的梯度正向子空间内，即加权系数一定要是正的。</li>
<li>对于不起作用的约束，加权系数为0。</li>
<li>由此分析可以得到不等式约束极值必要条件KKT条件（前三条）（凸优化时是充要条件）</li>
</ul>
</li>
<li>另外加上原约束条件和极值的正定约束条件，可得全部条件；</li>
<li>等式约束要求梯度一定共线但不要求同反向，不等式约束不起作用不要求，起作用必反向；</li>
<li>等式约束可以看成是两个不等式约束来分析，此时必起作用。</li>
</ul>
<h2 id="机器学习基本知识"><a href="#机器学习基本知识" class="headerlink" title="机器学习基本知识"></a>机器学习基本知识</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p><strong>模式</strong></p>
<ul>
<li>存在于时间和空间中可以观察的事物，如果可以用来区分它们是否相同或相似，就可以称为模式，模式不是事物本身，而是从事物中获得的信息。</li>
<li>世界上的事物都具有特殊性，不存在绝对的相同，但是人们为了认识世界，必须对事物加以分类，以更好地研究各个类别的规律。</li>
<li>每一个事物都可以看成是一些模式/特性的集合，机器学习就是找到这些模式和事物的模式表达。</li>
</ul>
<p><strong>机器学习</strong></p>
<ul>
<li>对象：具有一定统计规律的数据。</li>
<li>过程：找出数据中有泛化能力的模式/规律，以应用在新的场景下。</li>
</ul>
<p><strong>场景</strong></p>
<ul>
<li>数学/映射关系复杂；</li>
<li>难以编程实现；</li>
<li>有足够的数据。</li>
</ul>
<p><strong>模式识别</strong></p>
<ul>
<li>将带有空间、时间分布的事物的信息向类别映射。</li>
<li>可以分为统计模式识别方法和结构模式识别方法。</li>
<li>统计模式识别使用计算机对数据进行建模和分类，包括：数据获取，特征提取，分类器设计，系统实现</li>
</ul>
<p><strong>机器学习任务类型</strong>：</p>
<ul>
<li><strong>监督学习</strong>是在有明确的标签的数据上进行的，属于<strong>预测</strong>模型。<ul>
<li>其中分类问题输出离散的类别标签，包括图片分类、诊断等等；</li>
<li>回归问题输出连续的回归值，包括股市预测、气温预测、点击率预估等等。</li>
</ul>
</li>
<li>而<strong>无监督学习</strong>使用没有标签的数据，属于<strong>描述</strong>模型，揭示数据内在规律。其中包括聚类、降维。</li>
<li>半监督学习任务：用大量的未标记训练数据和少量的已标记数据来训练模型。</li>
<li>强化学习任务：从系统与环境的大量交互知识中训练模型。</li>
</ul>
<p><strong>机器学习算法类型</strong>：</p>
<ul>
<li>传统统计学习：基于数学模型的机器学习方法。包括SVM、逻辑回归、决策树等。<ul>
<li>这一类算法基于严格的数学推理，具有可解释性强、运行速度快、可应用于小规模数据集的特点。</li>
</ul>
</li>
<li>深度学习：基于神经网络的机器学习方法。包括前馈神经网络、卷积神经网络、循环神经网络等。<ul>
<li>这一类算法基于神经网络，可解释性较差，强烈依赖于数据集规模。这类算法在语音、视觉、自然语言等领域非常成功。</li>
</ul>
</li>
<li>集成学习：将多种机器学习方法结合使用的策略，包括：bagging、boosting、stacking等。<ul>
<li>一些常见的使用形式：随机森林（决策树+bagging）、adaboost、梯度提升树等。</li>
</ul>
</li>
</ul>
<p><strong>可解释性</strong></p>
<ul>
<li>可解释性与问题难度本身就是矛盾的，当一个问题有明确的规则可以解决时，本身就具有可解释性，就不需要用机器学习；所以不是机器学习、深度学习可解释性差，而是它们能够应对可解释性差的问题；可解释性好的问题就是人提前在数据中挖掘出了信息、于是就可以用简单模型。</li>
</ul>
<p><strong>没有免费的午餐定理</strong>：</p>
<ul>
<li>对于一个学习算法A，如果在某些问题上它比算法B好，那么必然存在另一些问题，在那些问题中B比A更好。</li>
<li>因此不存在这样的算法：它在所有的问题上都取得最佳的性能。因此要谈论算法的优劣必须基于具体的学习问题。</li>
<li>每一个模型都是在学习一个分布，当分布变化时，模型自然不能取得更好的效果。</li>
</ul>
<p><strong>拒绝策略</strong><br>在判决结果置信度低时，在实际中有必要使用拒绝决策，即不对数据的类别进行判定或预测。</p>
<h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><h4 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h4><ul>
<li>从不太可能发生的事件中能学到更多的有用信息。<ul>
<li>发生可能性较大的事件包含较少的信息，发生可能性较小的事件包含较多的信息;</li>
<li>独立事件包含额外的信息。</li>
</ul>
</li>
</ul>
<h4 id="自信息self-information"><a href="#自信息self-information" class="headerlink" title="自信息self-information"></a>自信息self-information</h4><ul>
<li>对于事件 ，定义自信息self-information为该事件发生概率的负对数，即概率越大的事件信息越少。</li>
<li>自信息仅仅处理单个输出，但是如果计算自信息的期望，它就是熵。</li>
</ul>
<h4 id="熵函数"><a href="#熵函数" class="headerlink" title="熵函数"></a>熵函数</h4><ul>
<li>用来对q这个分布的不确定性进行编码所需的信息量；</li>
</ul>
<h4 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL散度）</h4><ul>
<li>给定分布q之后，还需要多少信息来编码分布p；<ul>
<li>假如两个分布完全一样，相对熵就是0；</li>
<li>两个分布不一样时相对熵增加至无穷大；</li>
</ul>
</li>
</ul>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><ul>
<li>度量两个分布的距离，在分类问题中对应最大似然估计。范围为真实分布的熵到无穷大；</li>
<li>p与q的交叉熵=p的熵+p与q的相对熵。</li>
</ul>
<h3 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h3><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p>原始数据（如文章、图像）的维度、特征数往往极高，为了实现有效地识别，需要从数据中找到有效特征。</p>
<h4 id="特征空间"><a href="#特征空间" class="headerlink" title="特征空间"></a>特征空间</h4><ul>
<li>输入空间：所有输入的可能取值；</li>
<li>输出空间：所有输出的可能取值。</li>
<li>特征空间：经过特征工程处理过的输入空间<ul>
<li>特征向量表示每个具体的输入，所有特征向量构成特征空间。</li>
<li>特征空间的每一个维度对应一种特征。</li>
<li>可以将输入空间等同于特征空间，但是也可以不同。绝大多数情况下，输入空间等于特征空间。</li>
<li>模型是定义在特征空间上的。</li>
</ul>
</li>
</ul>
<h4 id="泛化"><a href="#泛化" class="headerlink" title="泛化"></a>泛化</h4><ul>
<li>更加一般、更加普遍的规律、特征；如PCA分解时，对应奇异值/方差越大的分量是越普遍的分量，也越具有泛化性。</li>
<li>越具有泛化能力的信息越具有普适性，也越是机器学习和人学习的重点。</li>
<li>泛化特征与所研究问题有关，如不同的分类问题中，同一个特征的重要程度不一样。</li>
</ul>
<h4 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h4><ul>
<li>代表模型的函数集合。这也称作模型的表示容量representational capacity。</li>
<li>不同的模型（算法）有不同的假设空间（函数空间），因此需要考虑所需要的找到的假设函数是不是在假设空间中；</li>
<li>由于额外的限制因素（比如优化算法的限制），模型的有效容量effective capacity一般会小于模型的表示容量。</li>
<li>通常在模型的假设空间中找出最佳的函数是非常困难的优化问题，实际应用中只是挑选一个使得训练误差足够低的函数即可。</li>
</ul>
<h3 id="分布"><a href="#分布" class="headerlink" title="分布"></a>分布</h3><h4 id="独立同分布"><a href="#独立同分布" class="headerlink" title="独立同分布"></a>独立同分布</h4><ul>
<li>机器学习中的训练数据和测试数据要求是在同一个分布中进行独立同分布产生。</li>
<li>学习过程中，假定这个分布存在，但是具体参数未知。</li>
</ul>
<h4 id="参数模型-amp-非参数模型"><a href="#参数模型-amp-非参数模型" class="headerlink" title="参数模型&amp;非参数模型"></a>参数模型&amp;非参数模型</h4><ul>
<li>参数模型假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型；</li>
<li>非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</li>
<li>参数模型和非参数模型中的“参数”并不是模型中的参数，而是数据分布的参数。</li>
</ul>
<h4 id="生成模型-amp-判别模型"><a href="#生成模型-amp-判别模型" class="headerlink" title="生成模型&amp;判别模型"></a>生成模型&amp;判别模型</h4><p>区别在于有没有显式计算分布。</p>
<ul>
<li>判别模型是得到决策边界，判断样本为不同类别的概率，不显式计算分布，隐式学习到数据分布信息；</li>
<li>生成模型是显式计算样本数据与类别的分布信息，判别时计算样本为哪一类的概率更大。<img src='/medias/image/机器学习-生成判别.jpeg' width="80%"></li>
</ul>
<h4 id="误差可以分解为偏差、方差和噪声之和"><a href="#误差可以分解为偏差、方差和噪声之和" class="headerlink" title="误差可以分解为偏差、方差和噪声之和"></a>误差可以分解为偏差、方差和噪声之和</h4><ul>
<li>偏差：描述模型对于特定数据（训练集）的拟合效果,度量了学习算法的期望预测与真实结果之间的偏离程度，刻画了学习算法本身的拟合能力。</li>
<li>方差：描述模型在不同数据（训练集与测试集）上拟合效果的差别，度量了训练集的变动所导致的学习性能的变化，刻画了数据扰动造成的影响。</li>
<li>噪声：度量了在当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度。</li>
<li>偏差-方差分解表明：泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定的。</li>
<li>偏差-方差分解中，噪声也可以称作最优误差或者贝叶斯误差。如：在图像识别的问题中，人眼识别的错误率可以视作最优误差。</li>
<li>欠拟合：高偏差，低方差；</li>
<li>过拟合：低偏差，高方差。</li>
</ul>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p><strong>机器学习三要素</strong>：</p>
<ul>
<li>模型：机器学习的目的，通过少的模型参数去表征大量数据的信息；</li>
<li>策略：评价模型的好坏，如：极大似然、最大后验等，具体到神经网络中就是损失函数；</li>
<li>算法：如何更好的去优化、训练模型，降低时间空间复杂度。</li>
</ul>
<p><strong>经验风险最小化&amp;结构风险最小化&amp;奥卡姆剃刀原理</strong></p>
<ul>
<li>机器学习的目标是<strong>期望风险</strong>最小化，也就是让模型拟合真实分布；</li>
<li>但真实分布无法完全得到，只能获得分布的一些数据，<strong>经验风险</strong>最小化的目的是使模型更好地拟合数据样本；</li>
<li>有限的采样数据与真实分布之间存在gap，<strong>结构风险</strong>是在经验风险最小的基础上加入先验知识，使得模型往更加合理的方向优化；</li>
<li>结构风险最小化策略符合<strong>奥卡姆剃刀原理</strong>的先验观点：能够很好地解释已知数据，且十分简单才是最好的模型。</li>
</ul>
<p><strong>参数估计</strong></p>
<ul>
<li><strong>极大似然估计</strong>就是经验风险最小化的例子，拟合样本，使得在样本固定时，参数的似然值极大；<ul>
<li>神经网络优化时，分类问题采用交叉熵损失，因为假设分类问题的类别服从伯努力分布；回归问题采用均方损失函数，因为假设回归问题的输出服从高斯分布。</li>
</ul>
</li>
<li><strong>最大后验估计</strong>就是结构风险最小化的例子，考虑到不同参数的先验概率不一样，用参数的先验概率乘以似然值得到后验概率，并最大化后验概率。<ul>
<li>L1、L2正则化先验的假设参数值应该不大。</li>
</ul>
</li>
</ul>
<p><strong>过拟合</strong></p>
<ul>
<li>过拟合的原因是：将训练样本本身的一些特点当作了所有潜在样本都具有的一般性质，这会造成泛化能力下降。</li>
<li>过拟合无法避免，只能缓解，因为没有办法得到总体分布的全部信息，有限的样本中总是会有噪声。</li>
</ul>
<p><strong>缓解过拟合</strong></p>
<ul>
<li>正则化：通过加入先验知识，限制不合适的假设空间，如权重衰减，对应加入贝叶斯先验后的最大后验概率估计；</li>
<li>数据集增广：通过人工规则产生虚假数据来创造更多的训练数据，新产生出来的数据包含正确的重要信息，同时次要信息相互抵消。</li>
<li>噪声注入：包括输入噪声注入、输出噪声注入、权重噪声注入。将噪声分别注入到输入/输出/权重参数中。</li>
<li>早停：当验证集上的误差没有进一步改善时，算法提前终止。</li>
</ul>
<p><strong>PAC&amp;&amp;VC维</strong></p>
<ul>
<li>PAC理论认为如果精度可以无限小称为强可学习，如果仅比猜测好一点则是弱可学习，集成学习boosting证明强可学习与弱可学习等价。</li>
<li>训练误差与泛化误差之间差异的上界随着模型容量增长而增长，随着训练样本增多而下降；因为随着模型容量增加，会把更多噪声识别成特征，而随着样本增加，噪声会被平滑。</li>
<li>VC维理论对于机器学习算法有很好的指导作用，但是它在深度学习很难应用。因为边界太宽泛，且难以确定深度学习的容量。由于深度学习模型的有效容量受限于优化算法，因此确定深度学习模型的容量特别困难。</li>
</ul>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p><strong>泛化能力评估</strong></p>
<ul>
<li>留出法：直接将数据切分为三个互斥的部分（也可以切分成两部分，此时训练集也是验证集），然后在训练集上训练模型，在验证集上选择模型，最后用测试集上的误差作为泛化误差的估计。</li>
<li>K折交叉验证法：数据随机划分为K个互不相交且大小相同的子集，利用K-1个子集数据训练模型，利用余下的一个子集测试模型。</li>
<li>留一法：假设数据集中存在N个样本，令K=N则得到了K折交叉验证的一个特例。</li>
<li>自助采样法：放回采样，随机森林选用此策略，会改变数据分布。</li>
</ul>
<p><strong>性能度量</strong></p>
<ul>
<li>混淆矩阵</li>
</ul>
<table>
<thead>
<tr>
<th>真实/预测</th>
<th>正类</th>
<th>反类</th>
</tr>
</thead>
<tbody><tr>
<td>正类</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr>
<td>反类</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody></table>
<ul>
<li><p>准确率 (accuracy)：$A=\frac{TP+TN}{TP+TN+FP+FN}$，正确预测样本的占比；</p>
</li>
<li><p>查准率(precision)：$P=\frac{TP}{TP+FP}$，预测的正例的有多大比例是正例；</p>
</li>
<li><p>查全率(recall):$R=\frac{TP}{TP+FN}$，正例有多大比例被预测出来；</p>
</li>
<li><p>$F_1$分数：$F_1=2*\frac{P * R}{P+R}$，对查准率和查全率进行一个调和；</p>
</li>
<li><p>$F_\beta$分数：$F_\beta=(1+\beta^2)\frac{P * R}{\beta^2*P+R}$，可以调整查准率和查全率的权重，$\beta$越大时，查全率权重越大。</p>
</li>
<li><p>查准率和查全率都是越大越好；但是对于已有模型，这两个值是一对负相关的；他们的大小由区分正反类的阈值确定，阈值设得越高时模型越倾向于预测为反类，此时recall下降，precision上升，反之亦然。</p>
</li>
</ul>
<p><strong>P-R曲线</strong><br>调整阈值可以得到不同的precision-recall对，进而得到P-R曲线。</p>
<ul>
<li>P-R曲线从左上角(0,1) 到右下角(1,0) 。</li>
<li>开始时第一个样本（最可能为正例的）预测为正例，其它样本都预测为负类。此时：查准率很高，几乎为1；查全率很低，几乎为0，大量的正例没有找到；</li>
<li>结束时所有的样本都预测为正类。此时：查全率很高，正例全部找到了，查全率为1；查准率很低，大量的负类被预测为正类。</li>
</ul>
<p><strong>ROC曲线</strong></p>
<ul>
<li><p>正确报警率（真正例率）：$TPR=\frac{TP}{TP+FN}$</p>
</li>
<li><p>误警率（假正例率）：$FPR=\frac{FP}{FP+TN}$ </p>
</li>
<li><p>ROC曲线从左下角$(0,0)$到右上角$(1,1)$。</p>
</li>
<li><p>开始时第一个样本（最可能为正例的）预测为正例，其它样本都预测为负类。此时：真正例率很低，几乎为0，因为大量的正例未预测到；假正例率很低，几乎为0，因为此时预测为正类的样本很少，所以几乎没有错认的正例；</p>
</li>
<li><p>结束时所有的样本都预测为正类。此时：真正例率很高，几乎为1，因为所有样本都预测为正类；假正例率很高，几乎为1，因为所有的负样本都被错认为正类。</p>
</li>
<li><p>对角线对应于随机猜想模型。点$(0,1)$对应于理想模型；通常ROC曲线越靠近点$(0,1)$越好。</p>
</li>
<li><p>P-R曲线和ROC曲线上的每一个点都对应了一个阈值的选择，该点就是在该阈值下的(查准率，查全率) /(真正例率，假正例率) 。</p>
</li>
<li><p>相对P-R曲线，ROC曲线在正负样本变化时更加稳定，因为ROC曲线的两个指标是分开使用正样本和负样本的数据，因此当比例发生变化时，每一个指标使用的比值不变。</p>
</li>
</ul>
<h2 id="深度学习基础知识"><a href="#深度学习基础知识" class="headerlink" title="深度学习基础知识"></a>深度学习基础知识</h2><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p><strong>人工智能发展</strong></p>
<ul>
<li>人工智能：泛指与智能有关的技术</li>
<li>规则学习：硬编码知识。计算机不需要从数据中学习知识。</li>
<li>机器学习：不用显式编程的学习<ul>
<li>经典的机器学习：人工设计特征。计算机从数据中学习到了“特征–&gt;label”之间的映射。</li>
</ul>
</li>
<li>特征学习：机器从数据中自动学习到特征，然后学习到了“特征 –&gt; label”之间的映射。</li>
<li>深度学习：机器从数据中自动学习到了多层特征（深层特征由浅层特征来表达），然后学习到了“特征 –&gt; label”之间的映射。<ul>
<li>深度学习是端到端学习，自动学习特征和映射，不用像以前一样进行手工的特征设计。但目前的发展程度还很低，人工设计网络结构一定程度上还是属于特征设计，还是需要通过先验信息来限定特征结构、模型容量。</li>
<li>通过组合简单的概念（concept）来构建复杂的概念。如：在图片识别任务中，通过比较相邻像素的亮度，则容易地识别边缘；通过识别边的集合，则容易识别角和轮廓；通过识别轮廓和角的特点集合，则容易识别物体整体。</li>
</ul>
</li>
</ul>
<p><strong>数据集&amp;参数量</strong></p>
<ul>
<li>在大的数据集（数据中信息多）上，训练更大的网络更有可能得到更好的效果，而在小数据集上，模型的发挥空间小，效果更多取决于特征工程（人工先验知识）的能力。</li>
<li>数据量有限的情况下，模型容量并非越大越好；因为数据可以看成是用冲激函数在原分布上采样，随着模型容量的增大，一定会学到更多的噪声。</li>
<li>随着数据集的增大，一方面是数据提供的信息更多了，有利于提升泛化性能；另一方面是使得优化曲面更加平滑，降低优化难度。直观理解一个样本对于优化平面带来的是一个冲击，样本多了之后可以相互平滑，从数学上看，代价函数是多个样本的loss的平均，样本数越多时代价函数曲面更平滑，更有利于优化，但一个batch的样本数适当少一点可以加快收敛速度和提供随机性。</li>
<li>现在深度模型的参数量越来越大，但并不是参数越多越好，应该和数据量、场景相匹配，尽量用更少的参数达到相同的效果以增加模型的泛化能力。</li>
</ul>
<p><strong>可辨识性</strong></p>
<ul>
<li>如果一个训练集可以唯一确定一组模型参数，则该模型称作可辨认的。</li>
<li>带有隐变量的模型通常是不可辨认的。因为可以批量交换隐变量，从而得到等价的模型。如：交换隐单元和的权重向量。也可以放大权重和偏置倍，然后缩小输出倍，从而保持模型等价。</li>
<li>模型可辨认性问题意味着：<br>神经网络的代价函数具有非常多、甚至是无限多的局部极小解。<br>由可辨认性问题产生的局部极小解都具有相同的代价函数值，它并不是代价函数非凸性带来的问题。<br>假如存在一组参数使得模型、数据达到全局最小，可以通过一些变换得到无数组全局最优参数。</li>
</ul>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p><strong>激活函数</strong></p>
<ul>
<li>对输入做非线性变换，如果神经网络中没有非线性变换，则最终退化为简单线性变换（矩阵运算）；</li>
<li>一般解释sigmoid函数在深度学习上的劣势通常是从梯度消失的角度来解释，也可以尝试另一个角度：sigmoid函数就不是为深度学习而生的，它原本就是在浅层模型（逻辑斯蒂回归）中将数值从无穷区间映射到0-1之间的工具。现在这一工作基本由softmax替代，所以替代sigmoid函数的不是relu，而是softmax，另外softmax本身就是对sigmoid的扩展。Tanh几乎在所有场合都优于sigmoid函数，除非要求输出0-1，否则需要考虑sigmoid的时候完全可以用tanh取代。</li>
<li>Relu函数虽然有一半的输出处于0导数，但却并不是说会有一半的神经元处于未激活的状态，因为对于不同的样本可能函数输入会很不相同，有的样本输入大于0，有的样本输入小于0，这样就不能算未激活。</li>
<li>每一个神经元都是一个特征学习器，因此怎样的激活函数更好也可以从这个角度展开分析。所以前馈网络的隐单元更适合relu，而tanh（sigmoid）用在需要限制输出的地方，如循环神经网络中防止梯度爆炸。</li>
</ul>
<p><strong>先验</strong></p>
<ul>
<li>MLP就可以表达各种特征，之所以需要设计、发展各种结构，一方面是直接用MLP会参数爆炸；另一方面是在平衡困难：直接用MLP的话是把困难都推给了优化过程，因为对模型不加限制会使得很难训练到想要的参数，而设计各种结构是通过加入先验信息来限制模型能力，使得模型往想要的方向前进。<ul>
<li>MLP之于神经网络，就如同高斯分布之于分布，是在没有额外先验知识时的选择，当有额外先验知识时，就应该加入先验知识，以使得模型更加符合实际。</li>
</ul>
</li>
<li>学到的参数就是在表征一些模式，神经网络不同的层结构（全连接、卷积…）是不同的特征学习器，它们有着不同的先验假设，适合学习不同的特征。</li>
<li>从计算图上看，权重衰减的正则化也是一种直连，但是并没有通过权重衰减建立与标签信息的联系，只是建立与一些先验信息的联系。</li>
</ul>
<h3 id="优化-1"><a href="#优化-1" class="headerlink" title="优化"></a>优化</h3><p><strong>反向传播</strong></p>
<ul>
<li>反向传播是对多元微分的一种实现。利用动态规划的思想，用空间换时间，存储中间结果，避免链式法则中的大量重复计算。</li>
</ul>
<p><strong>局部极值</strong></p>
<ul>
<li>在极高维的时候，鞍点出现的概率要远远大于极小点，所以神经网络优化不下去很可能不是因为极小值点。</li>
<li>可以绘制梯度范数随着时间的变化：<ul>
<li>如果梯度范数没有缩小到一个很小的值，则问题的原因既不是局部极小值引起的，也不是其他形式的临界点（比如鞍点）引起的。</li>
<li>如果梯度范数缩小到一个很小的值，则问题的原因可能是局部极小值引起的，也可能是其他原因引起的。</li>
</ul>
</li>
<li>当位于函数值较低的区间时，黑塞矩阵的特征值为正的可能性更大。这意味着：<ul>
<li>具有较大函数值的临界点更可能是鞍点，因为此时黑塞矩阵的特征值可能既存在正值、也存在负值。</li>
<li>具有较小函数值的临界点更可能是局部极小值点，因为此时黑塞矩阵的特征值更可能全部为正值。</li>
<li>具有极高函数值的临界点更可能是局部极大值点，因为此时黑塞矩阵的特征值更可能全部为负值。</li>
</ul>
</li>
<li>使用海森矩阵的优化算法需要更大的batch-size，因为海森矩阵的条件数过高，对于偏差的容忍度差。</li>
</ul>
<p><strong>牛顿法</strong></p>
<ul>
<li>梯度下降法不能保证代价函数一定下降，牛顿法更不能。<ul>
<li>梯度下降法利用一阶泰勒展开，下降要求在小领域内；</li>
<li>牛顿法利用二阶泰勒展开，试图直接跳到极值点，在凸二次函数下可以一步达到最优，在其它凸优化情况下也可以较快达到最优，但是在非凸问题时没有保证，它的目标是尽快把每个参数都送到极值点，因此在多参数的时候大概率会跳到鞍点处，因为鞍点出现的概率要远远大于极大值点和极小值点。</li>
<li>所以牛顿法不适合非凸优化。</li>
</ul>
</li>
<li>牛顿法要应用在非凸优化时需要正则化使得海森矩阵正定，这只适合负特征值绝对值较小时，另外海森矩阵的边长等于参数量，所以在参数量极大的深度学习应用中，牛顿法的巨大计算和存储代价也是致命缺点。</li>
</ul>
<p><strong>超参数调优</strong></p>
<ul>
<li>网格搜索；随即搜索：重要的超参数用网格搜索，不重要的超参数用随机选择，以降低复杂度；</li>
<li>动态资源分配：类似于多臂老虎机问题，选择出最优的臂；目的是使得效果不好的组合可以快速被淘汰掉，随着优化的进行，不确定性减小，就可以淘汰掉更多的臂；<ul>
<li>对$n$组超参数组合，进行一定程度优化；</li>
<li>保留效果最好的前一半的组合，淘汰其它；依次循环得到最优的参数组合；</li>
</ul>
</li>
<li>贝叶斯优化：认为超参数存在某种分布，可以利用已经尝试的超参数对分布进行估计，进而预测收益最大的组合，进而减少实验次数；</li>
</ul>
<p><strong>FTRL</strong></p>
<ul>
<li>出发点是在线学习与模型稀疏性；在线学习应对工程场景下大模型、大数据量更新代价大的问题；稀疏性应对特征量极大的场合（推荐系统），减少特征使用量；</li>
<li>直接的解决方案是SGD+L1正则化，但是SGD的随机性无法保证在全局应该稀疏的特征在每次更新的时候都稀疏；</li>
<li>FTRL与SGD+L1正则化几乎等价，可以看成是一种优化版本；</li>
<li>$w_{t+1}=argmin_w(\sum^t_{s=1}g_sw+\frac{1}{2}\sum^t_{s=1}\delta_s||w-w_s||^2_2+\lambda||w||_1)$</li>
</ul>
<h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><ul>
<li>模型是非线性的，也无法直接求解出最优解，所以大的方向是通过梯度下降，贪心地逼近更好的解；各种策略都是服务于这个贪心的过程。</li>
<li>深度学习的方法都显得非常技巧，一个原因是因为深度学习研究的是高度非线性的系统，所以往往无法通过个别高度概括性的工具应对各种问题。所以研究深度学习需要理论结合实际，不能纸上谈兵。</li>
</ul>
<p><strong>学习率</strong></p>
<ul>
<li>基于梯度的优化算法中，每个参数都有一个最佳的学习率范围，但实际中不可能给每个参数都设置一个学习率超参数，而一个学习率又无法满足所有的参数。这个时候就有了很多的优化角度：<ul>
<li>对数据做归一化缩放操作、加批次归一化层以改善参数的一致性；</li>
<li>自适应调整学习率；</li>
<li>动量法自适应调整梯度（动量）等。</li>
</ul>
</li>
<li>第k步的学习率记做$ϵ_k$ ：<ul>
<li>$\sum_k ϵ_k=\infty$：保证无论多远，梯度都可以更新到；</li>
<li>$\sum_k ϵ_k^2&lt;\infty$：保证更新过程稳定收敛；</li>
</ul>
</li>
<li>在随机梯度下降中，学习率固定，因此随着梯度大小变化，参数更新量可能为任意数。而RMSProp动量法通过自适应的方式将参数更新量从无穷区间映射到了大约$[-ϵ,ϵ]$（定性得到的大约区间，不一定）。</li>
<li>batch size大时，优化曲面更加稳定，可以设置更大的学习率；反之batch size小时，学习率要设置的小一些；</li>
<li>周期性得让学习率随轮次减小以适应新的优化情况：<ul>
<li>指数衰减：$\mu=\mu_0e^{-k}$</li>
<li>反比衰减：$\mu=\frac{\mu_0}{1+k}$</li>
</ul>
</li>
<li>大的梯度容易跳出局部极值，但是只能学到简单的pattern，小的梯度不容易跳出局部极值，但是有利于学到复杂的pattern。在训练过程中，学习率一般是要逐步减小的，但是在陷入局部极值的时候增大学习率或许可以帮助跳出。</li>
<li>判断学习率大小，随着轮次，损失函数：<ul>
<li>下降缓慢：学习率太小，梯度下降慢；</li>
<li>下降适中、平稳：学习率合适；</li>
<li>下降快但很快停止下降：学习率偏大，梯度更新震荡，无法进一步下降；</li>
<li>上升：学习率过大，梯度更新发散；</li>
</ul>
</li>
<li>scheduler：对学习率进行管理；<ul>
<li>学习率衰减：有单因子、多因子、多项式、指数、余弦等方式</li>
<li>预热：开始的学习率太大会使得模型发散，太小又会使得整个训练过程变慢，因此可以使用预热的方式，让学习率逐步增加到一定程度，然后再进入衰减阶段。</li>
</ul>
</li>
<li>AdaGrad：将参数历次梯度的斜边的倒数用来自适应调整学习率；梯度大的参数学习率小；很容易还没优化好就停止了；</li>
<li>RMSProp：在AdaGrad的基础上，将累计求和的部分改成了指数滑动，避免自适应学习率快速衰减的问题，自适应学习率可以根据需要变大和变小；</li>
<li>AdaDelta：在RMSProp的基础上，在分子上加入参数变化量的指数滑动斜边，进一步调和，让更新大的参数的学习率可以不太小，缓解自适应学习率的波动；</li>
<li>Adam：在RMSProp的基础上，加入动量法，然后对动量、斜边进行一些调和，改善早期的偏差；</li>
</ul>
<p><strong>早停</strong></p>
<ul>
<li>随着训练的进行，模型基本学习了数据中的有效信息，开始拟合数据中的噪声，此时应该及时停止训练，降低过拟合；</li>
<li>随着训练的进行，泛化/测试误差先下降，后上升，在谷底时即是早停的最好时机；</li>
</ul>
<p><strong>随机/批次梯度下降</strong></p>
<ul>
<li>样本中的信息存在冗余，大量样本都对梯度做出了非常相似的贡献，使用更多样本来估计梯度的方法的收益是低于线性的；</li>
<li>batch随机梯度下降中，只要没有重复使用样本，它就是真实泛化误差梯度的无偏估计；</li>
<li>计算开销更小；</li>
<li>shuffle和小批次提供一定随机性，有利于跨过局部极值<ul>
<li>iteration：更新一次梯度</li>
<li>epoch：所有样本用一遍</li>
</ul>
</li>
</ul>
<p><strong>梯度截断</strong></p>
<ul>
<li>对梯度按阈值截断或按模截断，以避免梯度爆炸；</li>
</ul>
<p><strong>特征scaling</strong></p>
<ul>
<li>对每一个特征，将数据减去该特征的均值并除以标准差，改善不同参数的一致性，使得超曲面更加均匀一些。</li>
</ul>
<p><strong>动量Momentum</strong></p>
<ul>
<li>使用动量更新参数而不是直接用梯度，动量与上一次的动量和现在的梯度相关，或者说现在和以前的梯度共同决定；</li>
<li>通过这种方式可以调和不同参数的更新值，方向相同的梯度会变大以加快更新速度，方向反复变化的梯度会变小以降低震荡；</li>
<li>Nesterov加速梯度：先用历史动量更新参数，然后计算梯度并更新，调整先后顺序；</li>
</ul>
<p><strong>正则化/权重衰减/weight decay</strong></p>
<ul>
<li>通过加入先验知识，避免参数值处于不合理的区间：<ul>
<li>L2正则化：先验假设参数取值服从均值为0的高斯分布，越大的参数被打压得越多；</li>
<li>L1正则化：先验假设参数取值服从均值为0的指数分布，大小参数一起打压，可以带来参数稀疏性；</li>
<li>同时加入L1和L2，调和两种的效果</li>
</ul>
</li>
</ul>
<p><strong>Dropout</strong></p>
<ul>
<li>做法：训练时，在Dropout层按一定概率随机将一部分神经元置0，未置0的神经元值除以保留的概率；</li>
<li>原理类似：bagging+子模型参数共享；</li>
<li>未置0神经元的处理是为了使数据无偏；</li>
</ul>
<p><strong>权值初始化</strong></p>
<ul>
<li>权重初始化应该尽量保证前向传播和反向传播过程中的值得分布不发生变化，分布中的均值大都为0，而方差应该尽量不变，否则深层网络的连乘会导致梯度消失或爆炸，以及饱和等问题。</li>
<li>初始化到0无法学到任何东西，因为导致大量参数的同质，应该采用随机初始化。</li>
<li>Xavier的初始化可以保证线性全连接层传递后的方差不变，但是这样对relu这样的激活函数不好，因为relu有一半的是没有值，所以方差应该大一些；</li>
</ul>
<p><strong>局部正则化/Local Response Normalize</strong></p>
<ul>
<li>$b_i=\frac{a_i}{k+\beta\sum_ia_i}$；</li>
<li>引入竞争，更可解释，同时稳定数据分布，类似Batch Norm等标准化层效果。</li>
</ul>
<p><strong>残差连接</strong></p>
<ul>
<li>深度网络难以训练有许多原因，除了容易梯度爆炸和梯度消失外，还包括参数的依赖，高层参数靠近输出相对容易学习但是高层参数又依赖底层参数，底层参数优化不好，高层参数也无法优化；残差连接可以降低特征学习难度；</li>
<li>利于梯度回流，缓解梯度消失；</li>
</ul>
<p><strong>数据增广</strong></p>
<ul>
<li>可以看成增加模型的等变性，也可以看成抑制数据中的噪声；</li>
<li>图像的数据增广方式包括基本的旋转平移等外，还可以通过风格增广；</li>
</ul>
<p><strong>门结构</strong></p>
<ul>
<li>有助于底层信息直通</li>
</ul>
<p><strong>其它</strong></p>
<ul>
<li>各种类型的网络层与层之间的参数不应该缩减太快，因为后面的后面神经元学习的特征很难有效利用前面的众多特征。</li>
<li>参数共享也是促使模型学习主要模式，同时也可以起到抑制过拟合的作用，因为有限的参数要尽量满足主要特征，就自然忽略了噪声。</li>
</ul>
<h3 id="标准化层"><a href="#标准化层" class="headerlink" title="标准化层"></a>标准化层</h3><ul>
<li>一说可以防止数据分布的变化（内部协变量偏移）带来性能下降；另一说模型的优化曲面发生变化，变得更加平滑，非凸性减弱，使得训练变得容易，即利普希茨系数严格降低；</li>
</ul>
<p><strong>Batch Norm</strong></p>
<ul>
<li><p>Batch Norm：在对一个通道内，对一批次数据的每个特征，减均值除标准差，同时再加入一对可学习的均值方差参数。</p>
<ul>
<li>如果不是卷积网络，Batch Norm也可以是对于单个神经元的;</li>
<li>训练过程使用的减均值除方差是通过一批的训练数据得到，测试的是通过训练过程的指数滑动平均得到;</li>
<li>通过Batch Norm，深度模型更好地收敛训练，即使对于比较大的学习率也有较好的效果;</li>
<li>在batch size较大的时候，Batch分布与整体分布类似，Batch Norm的效果往往好于Other Norm；同时Batch Norm高度依赖于mini batch 的大小。它要求每个mini-batch 都比较大，因此不适合batch size 较小的场景，如：在线学习（batch size=1 ）；</li>
<li>不适合RNN网络，因为不同样本的sequence的长度不同，因此RNN的深度是不固定的。同一个batch中的多个样本会产生不同深度的RNN，因此很难对同一层的样本进行归一化；</li>
</ul>
</li>
<li><p>理解Batch Norm层的一个角度：</p>
<ul>
<li>原来的网络就像一大段代码叠在一起，可读性差（层与层之间相互依赖，难以训练）；</li>
<li>加上Batch Norm层之后就像把以前的代码分割成了一个个子程序，依此调用，程序之间有清晰的接口，因此阅读和调试都更加容易；</li>
<li>这就像是一种解耦操作，让各个层专注自己的任务（特征学习）。</li>
<li>同时也是在改善参数的一致性。</li>
</ul>
</li>
<li><p>在激活函数之前、还是之后进行Batch Norm都可，在激活函数之前进行的更常见；</p>
</li>
<li><p>Batch Norm层前的全连接层一般不需要带偏差项，因为偏差项会被吸走，卷积层可以带。</p>
</li>
</ul>
<p><strong>Layer Norm</strong></p>
<ul>
<li>因为BN是在批次维度进行，所以叫批次标准化，还可以在其他维度进行，得到不同种类的标准化。</li>
<li>Layer Norm不依赖于batch size，适合在线学习，也适合于RNN网络。</li>
</ul>
<p><strong>Instance Norm</strong></p>
<ul>
<li>对于GAN、风格迁移这类任务上，Instance Norm效果要优于BN，因为每张图片自己的风格比较独立，不应该与batch中其它图片产生太大联系。</li>
<li>Instance Norm也不依赖于batch size，适合在线学习，也适合于RNN网络。</li>
</ul>
<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><ul>
<li>在MLP的基础上增加了两个先验：<ul>
<li>局部性连接</li>
<li>参数共享。</li>
</ul>
</li>
<li>这两个先验假设使得卷积网络相对于MLP的参数量大减，泛化能力更加强，可以更好地学习到图像数据中的重要模式（主要指像素之间的变化关系、组合关系，所以非常适合计算视觉领域，当然也可以用在同样具有类似重要特征组合方式的领域）。</li>
<li>卷积和互相关都是由函数到函数的算子，其中卷积包括反转和求和两个过程，互相关只有求和；所以卷积神经网络是披着卷积外衣的互相关网络。</li>
<li>卷积核的大小也会影响卷积核的表达能力，参数量小的卷积核所能产生的可能性也少。</li>
<li>每个卷积核得到一个通道的输出，增加卷积核数量，可以从更多角度进行特征提取</li>
<li>等变性：$f(T(x))=T(f(x))$，卷积层参数共享带来一定的平移等变性，等变性可以看成是对数据噪声的一种抑制，如图片分类中无论物体在图片的中央还是边缘，都应该得到一样的结果，这种位置信息就是和分类无关的噪声；</li>
<li>卷积核参数均值影响特征图亮度，值相加和为1时处理之后的图像与原始图像的亮度相比几乎一致，小于1时减小，大于1时增大；</li>
</ul>
<h4 id="填充-amp-amp-步幅"><a href="#填充-amp-amp-步幅" class="headerlink" title="填充&amp;&amp;步幅"></a>填充&amp;&amp;步幅</h4><p>以二维输入为例，输入的高宽分别为$n_h$，$n_w$，卷积核的高宽分别为$k_h$，$k_w$，填充的高宽分别为$p_h$，$p_w$，步幅的高宽分别为$s_h$，$s_w$，则输出形状为：<br>$$<br>\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor<br>$$<br>当步幅为1时，设置$p_h=k_h-1, p_w=k_w-1$，输出的形状与输入的形状一致。</p>
<h4 id="pooling层"><a href="#pooling层" class="headerlink" title="pooling层"></a>pooling层</h4><ul>
<li>池化层：降低数据量；<ul>
<li>max pooling</li>
<li>mean pooling</li>
<li>全局平均池化：提供平移不变性，丢失很多细节</li>
</ul>
</li>
<li>不变性：$f(T(x))=f(x)$，池化层带来一定不变性；</li>
</ul>
<h4 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h4><ul>
<li>AlexNet：它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</li>
<li>使用重复块的网络（VGG）：它利用许多重复的神经网络块；</li>
<li>网络中的网络（NiN）：它重复使用由卷积层和1*1卷积层（用来代替全连接层）来构建深层网络;增加非线性，降低参数量；</li>
<li>含并行连结的网络（GoogLeNet）：它使用并行连结的网络<ul>
<li>通过多尺度卷积核利用不同窗口大小的卷积层和最大汇聚层来并行抽取信息，学习不同尺度特征；</li>
<li>分支训练：网络分叉，可以将梯度信息更好的传递到底层，防止梯度消失。</li>
</ul>
</li>
<li>残差网络（ResNet）：它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</li>
<li>稠密连接网络（DenseNet）：它的计算成本很高，但给我们带来了更好的效果。</li>
</ul>
<h4 id="其它策略"><a href="#其它策略" class="headerlink" title="其它策略"></a>其它策略</h4><ul>
<li>1*n卷积：降低参数量；</li>
<li>Dropout：类似bagging，集成学习的效果，降低方差；</li>
<li>数据增广：变相增加数据量，抵消不重要信息，加强核心成分，避免模型学偏；</li>
<li>空洞卷积：多尺度+减少参数量；</li>
<li>BN：应对数据分布漂移，避免数据整体进入饱和区，进而缓解梯度消失和梯度爆炸的问题；</li>
</ul>
<h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><ul>
<li>RNN在MLP的基础上加上（不同时刻/序列点）参数共享的先验。</li>
<li>RNN在前向传播和反向传播的时候都有矩阵乘幂，序列略长就必然崩溃，实用性非常低，主要是思想重要。</li>
<li>双向循环神经网络假设上下文都会对当下有影响，因此从两个方向传递信息；时序数据中，后面时刻的数据对前面时刻的数据没有因果关系，但是可以有相关关系；</li>
<li>LSTM在RNN上加入门控逻辑（遗忘、输入、输出），选择性遗忘和更新信息，可以应用在序列不太长的情况；</li>
<li>GRU整合门数量（更新、复位）并进一步减少计算量（不区分cell和h），优化结构。</li>
<li>LSTM、GRU一般不会用很深，往往2层最常见</li>
<li>attention机制可以解决长时间依赖的问题：不再去记录中间信息，而是学习捕获数据间的联系，因此不受长序列信息丢失的影响；</li>
<li>循环神经网络中sigmoid的0～1区间用于模拟门的开闭；tanh用于信息激活函数，线性区间大于sigmoid，同时防止relu可能带来的信息爆炸；</li>
<li>编码-解码架构可以应对seq2seq问题中输入输出序列长度不一致的情况；</li>
</ul>
<h3 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h3><p><strong>图</strong></p>
<ul>
<li>图的组成：顶点，边</li>
<li>有向图：边有方向。</li>
<li>无向图：边没有方向，可以看成有向图的边成对出现。</li>
<li>带权图：边有权重。</li>
</ul>
<p><strong>度</strong></p>
<ul>
<li>有向图度：<ul>
<li>入度：指向自己的边数</li>
<li>出度：自己发出的边数</li>
</ul>
</li>
<li>无向图度：连接边的数量</li>
</ul>
<p><strong>邻接矩阵&amp;临接表</strong></p>
<ul>
<li>邻接矩阵是方阵，宽度等于节点数，元素为1代表节点之间有边连接，邻接矩阵浪费空间；每一行相当于一个节点的onehot编码</li>
<li>无向图的邻接矩阵是对称阵，有向图不一定。</li>
<li>邻接表节省空间但效率低，cache不友好。</li>
</ul>
<p><strong>子图</strong><br>节点和边都是一个大图的子集。</p>
<p><strong>连通性</strong></p>
<ul>
<li>连通图：无向图中所有节点都可以连接在一起。</li>
<li>连通分量：连通子图数称，连通图的连通分量为1。</li>
<li>强连通图：有向图任意节点可以相互到达</li>
<li>弱连通图：有向图不是强连通图，但是作为无向图时是连通图。</li>
</ul>
<p><strong>最短路径</strong></p>
<ul>
<li>两个节点之间的最短距离，可能是边数，如果边权重不一样就是最短的边的和。</li>
<li>图直径：图所有节点的最短路径的最大值</li>
</ul>
<p><strong>节点重要性</strong></p>
<ul>
<li>度中心性：节点度数/(n-1)；即度越高，度中心性越高。</li>
<li>特征向量中心性：对邻接矩阵求特征值、特征向量，最大特征值对应的特征向量就是各个节点的特征向量中心性。<ul>
<li>不只看边数，还可以反映连接节点的重要性。</li>
</ul>
</li>
<li>中介中心性：经过自己的最短路径数/总的最短路径数<ul>
<li>如果有的节点对最短路径可以有多种走法，则分数平均。</li>
</ul>
</li>
<li>连接中心性：(n-1)/该节点到其它节点的最短路径之和。<ul>
<li>即节点越靠中心，分母越小，值越大。</li>
</ul>
</li>
</ul>
<p><strong>PageRank</strong></p>
<ul>
<li>边的PageRank值为源节点的PageRank除以出度，节点的PageRank值等于入边的PageRank值之和；交替计算直至稳定。<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> networkx <span class="token keyword">as</span> nx

<span class="token comment"># 可以从其它的数据结构中导入，如pandas邻接表</span>
G <span class="token operator">=</span> nx<span class="token punctuation">.</span>from_<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  
nx<span class="token punctuation">.</span>degree<span class="token punctuation">(</span>G<span class="token punctuation">)</span>
nx<span class="token punctuation">.</span>connected_components<span class="token punctuation">(</span>G<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
nx<span class="token punctuation">.</span>pagerank<span class="token punctuation">(</span>G<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
<p><strong>拉普拉斯矩阵</strong></p>
<ul>
<li>$L=D-A$，其中$A$为图的邻接矩阵，$D$为图的度矩阵（对角）；对拉普拉斯矩阵进行特征值分解可以得到一系列特征值和对应的特征向量。</li>
</ul>
<p><strong>图卷积</strong></p>
<ul>
<li>卷积是将数据从一个域转换到另一个域中，如信号处理领域中从时域到频域，图像处理中从图片到特征图，图领域中从图到对应特征向量；图像中的每一个特征图是对图像的一种分解，图中的特征向量也是对图连接关系的一种分解。<ul>
<li>DEFFERRARD M, BRESSON X, VANDERGHEYNST P. Convolutional neural networks on graphs with fast localized spectral filtering[C]//Advances in neural information processing systems. 2016: 3844–3852.</li>
</ul>
</li>
<li>在图结构的数据上进行特征提取，数据挖掘；GCN是CNN在图数据上的扩展，将卷积特征提取方式从欧几里得空间迁移到非欧几里得空间。<ul>
<li>BATTAGLIA P W, HAMRICK J B, BAPST V, 等. Relational inductive biases, deep learning, and graph networks[J]. arXiv preprint arXiv:1806.01261, 2018.</li>
<li>BRONSTEIN M M, BRUNA J, LECUN Y, 等. Geometric deep learning: going beyond euclidean data[J]. IEEE Signal Processing Magazine, IEEE, 2017, 34(4): 18–42.</li>
</ul>
</li>
</ul>
<h2 id="数据预处理-amp-特征工程"><a href="#数据预处理-amp-特征工程" class="headerlink" title="数据预处理&amp;特征工程"></a>数据预处理&amp;特征工程</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><ul>
<li>将各个维度的数据变化范围变换到一个相近的区间，便于梯度下降等场景；</li>
<li>有的算法不必要，如决策树；</li>
<li>划分的训练集、测试集等需要使用相同的标准进行处理。</li>
</ul>
<p><strong>z-score标准化</strong><br>中心化（zero-centered）+缩放（scale），减均值除标准差</p>
<p><strong>min-max标准化</strong><br>减$min$，除$|max-min|$。</p>
<h4 id="数据正则化"><a href="#数据正则化" class="headerlink" title="数据正则化"></a>数据正则化</h4><ul>
<li>对每个样本将p范数放缩到1，以便于求样本相似度等运算</li>
<li>标准化是对特征的操作，正则化是对样本的操作</li>
</ul>
<h4 id="唯一属性"><a href="#唯一属性" class="headerlink" title="唯一属性"></a>唯一属性</h4><ul>
<li>有一些特征如id是每个样本都不同，这样的特征对于刻画样本自身属性没有帮助，应该直接删掉；</li>
<li>有的场合，如推荐中，id特征也可以保存一些信息，按需要可以保留。</li>
</ul>
<h4 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h4><ul>
<li>直接使用带缺失值的样本，如决策树类的少量算法可以直接将缺失值作为一种情况进行处理。</li>
<li>直接删除有缺失值的样本，简单，数据纯净，但是浪费了一些信息，在缺失值较多的场合不适用。<ul>
<li>数据量较多是使用的策略，因为其它数据中已经有冗余信息，而缺失数据带有噪声；</li>
<li>数据量非常不足时不适合使用该策略。</li>
</ul>
</li>
<li>缺失值补全，可以尽量全面的利用信息，但是补全方式不恰当时效果适得其反；相当于加入先验。<ul>
<li>均值插补：连续特征用均值，离散特征用众数；</li>
<li>同类均值插补：按类别进行均值插补。</li>
<li>建模预测：建立模型预测，缺点是，如果预测的特征没有关联那预测的结果就没有意义，如果有关联预测出来的信息也是冗余的，也没有意义，所以用的不多。</li>
</ul>
</li>
</ul>
<h3 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h3><p><strong>特征二元化</strong><br>将数值型特征转化成布尔型特征，通过一个阈值超参数划分特征</p>
<p><strong>独热码</strong></p>
<ul>
<li>可以处理非数值特征；</li>
<li>降低单个特征的重要性；</li>
<li>对于有大小关系的数值变量，用独热码表示会丢失信息；</li>
</ul>
<p><strong>离散化</strong></p>
<ul>
<li>分桶</li>
</ul>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>特征选择的时候并不是选择最好的一组特征，因为特征之间往往存在耦合性。</li>
<li>可以使用贪心算法，每次增加一个能给效果提升最大的特征。</li>
</ul>
<p><strong>不进行特征选择的坏处</strong></p>
<ul>
<li>相对样本量小，维度灾难，模型泛化能力弱</li>
<li>计算量大</li>
<li>无关特征误导训练方向</li>
</ul>
<p><strong>稀疏表示和字典学习</strong></p>
<ul>
<li>对样本 $\vec{\mathbf{x}}<em>{i}$, 通过交替寻优的方式学习字典 $\mathbf{B}$ 和稀疏表示$\vec{\alpha}</em>{i}$，优化目标为：$\min_{\vec{\alpha}<em>{i}}||\vec{\mathbf{x}}</em>{i}-\mathbf{B}\vec{\alpha}<em>{i}||</em>{2}^{2}+\lambda\sum_{i=1}^{N}||\vec{\alpha}<em>{i}||</em>{1}$</li>
</ul>
<h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><p>将二分类模型用于多分类问题：</p>
<ul>
<li>一对多，为每一个类别训练一个分类器，非常容易陷入样本不平衡；</li>
<li>一对一，为每一对类别训练一个分类器，计算量太大；</li>
<li>多对多，每次都将若干个类作为正类，若干个其他类作为反类。</li>
</ul>
<h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><ul>
<li>对多的样本欠采样，可能丢失一些重要信息，常用方法是将反类划分成若干个集合供不同学习器使用，这样对每个学习器来看都是欠采样，但是全局来看并不会丢失重要信息。</li>
<li>对少的样本过采样，SMOTE方法：对于一个样本，从其同类近邻中随机选取一个点，在这两个点之间随机插值。</li>
<li>极不平衡时将问题看作单分类问题或异常检测。</li>
</ul>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><ul>
<li>通过一些方式将数据划分为多个簇（类），让簇内的数据尽量相似，簇间的数据尽量不相似。</li>
<li>聚类往往是其它工作的预工作，而不是最终结果。</li>
</ul>
<h3 id="相似度量"><a href="#相似度量" class="headerlink" title="相似度量"></a>相似度量</h3><ul>
<li>很大程度上决定了聚类效果。</li>
<li>不同特征的数值不一定适合直接计算距离。</li>
</ul>
<p><strong>距离基本要求</strong>：非负性、对称性、三角不等式。</p>
<p><strong>p范数</strong>：</p>
<ul>
<li>0范数（非0数）、1范数（曼哈顿距离）、2范数（欧氏距离）、无穷范数（最大值）。</li>
<li>p值越大时，绝对值大的维度起到的作用越大，反之亦然。</li>
<li>各个维度独立计算。</li>
</ul>
<p><strong>马氏距离</strong>：广义距离<br>各个维度不再独立计算，而考虑到它们之间的联系，通过数据的协方差矩阵来联系。<br>也可以不使用协方差矩阵，而通过学习得到联系矩阵。</p>
<h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>对数据分布有一个先验的假设，然后去计算出分布的参数，如K-means、混合高斯分布。求解参数的过程常常是使用EM期望最大的思想，交替寻优得到。</p>
<p><strong>K-means</strong></p>
<blockquote>
<ul>
<li>选取K个样本作为K个簇初始中心，根据距离远近将其他样本<strong>硬</strong>划分到各个簇中；</li>
<li>根据簇中的样本计算的中心/重心作为新的簇中心；</li>
<li>循环执行前两步，至簇中心稳定，常用手肘法判断。</li>
</ul>
</blockquote>
<ul>
<li>K-means可以看成C-means的特殊情况，在区分类别上一硬一软。</li>
</ul>
<p><strong>混合高斯分布</strong><br>是K-means更加一般的情况，每个样本不是硬的属于每个簇，而是以一定概率属于各个簇；每个簇有均值、方差、权重。</p>
<blockquote>
<ul>
<li>初始化K个簇的均值、方差、权重，计算每个样本有多大比重/概率属于各个簇；</li>
<li>根据样本的归属，重新计算各个簇的均值、方差、权重，</li>
<li>循环执行前两步，至取值稳定。</li>
</ul>
</blockquote>
<h3 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h3><p>将数据点看成一张图，然后对图进行切分，使得子图内部连接程度高，子图间连接程度低。</p>
<blockquote>
<p>定义数据点的相似度/边，构建拉普拉斯矩阵$L=D-W$，也可以对拉普拉斯矩阵进行规范化；<br>对拉普拉斯矩阵进行特征分解，取对应特征值最小的几个特征向量，可以构建出数据新的表示；<br>用该表示用其它方法（常见的K-means）对数据进行聚类。</p>
</blockquote>
<ul>
<li>特征分解后，每一个特征向量代表一种连接模式，大特征值的特征向量代表比较普遍的连接模式，区分度较低，因此优先选用小特征值的特征向量。</li>
<li>同PCA对比，这体现了聚类和降维的区别，降维是找共同模式，聚类是找特殊模式。</li>
</ul>
<h3 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h3><h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><h3 id="分布聚类"><a href="#分布聚类" class="headerlink" title="分布聚类"></a>分布聚类</h3><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>EM算法可以看成极大似然估计在数据有缺失时的推广：</p>
<ul>
<li>数据属性完整的时候知道完整的数据信息，直接估计分布的参数，即求极大似然；</li>
<li>缺失数据信息的时候，需要交替寻优，即交替求极大似然和极大概率。</li>
</ul>
<h3 id="分布学习-数据生成"><a href="#分布学习-数据生成" class="headerlink" title="分布学习-数据生成"></a>分布学习-数据生成</h3><p>常见的方式包括VAE和GAN，这两种方式，这两种方式的异同可以从许多角度来看.</p>
<table>
<thead>
<tr>
<th>角度</th>
<th>VAE</th>
<th>GAN</th>
</tr>
</thead>
<tbody><tr>
<td>train stage</td>
<td>one stage</td>
<td>two stage</td>
</tr>
<tr>
<td>distribution learning</td>
<td>对正例样本进行泛化，得到分布，正则化系数越大，泛化程度越高</td>
<td>用生成器得到负例，负例逼近正例共同得到分布</td>
</tr>
<tr>
<td>distribution feature</td>
<td>比较稳定，受限于样本feature，发挥空间相对较小</td>
<td>不确定性大，每次得到的分布可能很不一样，可能得到正样本中完全没有的feature</td>
</tr>
<tr>
<td>优化角度</td>
<td>通过分布变换并重构正样本来使得编码器和解码器学到分布</td>
<td>通过对抗方式，共同进化</td>
</tr>
<tr>
<td>loss组成</td>
<td>重构误差 + 先验分布误差</td>
<td>生成得分误差 &amp; 判别误差</td>
</tr>
<tr>
<td>loss粒度</td>
<td>pointwise loss</td>
<td>分布匹配loss</td>
</tr>
<tr>
<td>对抗部分</td>
<td>重构与正则（泛化）</td>
<td>生成器与判别器</td>
</tr>
<tr>
<td>对抗方式</td>
<td>重构数据与增加泛化性</td>
<td>生成接近正样本的数据与判别出假数据</td>
</tr>
<tr>
<td>生成数据质量</td>
<td>像素维度控制，比较稳定，倾向于局部信息，图片质量低</td>
<td>整体控制，容易跑偏，图片更清晰</td>
</tr>
</tbody></table>
<h4 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h4><p>变分自编码器本质上是希望学习数据的分布，编码器将原始数据分布映射到某个标准分布（如：正太分布、均匀分布），解码器再将标准分布映射到数据分布，最终编码器和解码器就都学习到了原数据分布的信息。其中解码器可以用于生成新数据。</p>
<ul>
<li>变分自编码器不是直接让隐变量Z符合标准正太分布，而是让每个样本生成的均值方差接近标准正太分布（均值接近0，方差接近1）</li>
<li>重参数技巧是为了解决采样过程无法求导</li>
<li>KL散度相当于正则项的作用，让编码器得到的量以及重参数后的隐向量尽量接近标准正太分布。</li>
<li>当decoder还没有训练好时（重构误差远大于KL loss），就会适当降低噪声（方差）这会使KL loss增加，使得拟合起来容易一些（重构误差开始下降）；反之，如果decoder训练得还不错时（重构误差小于KL loss），这时候噪声就会增加（KL loss减少），使得拟合更加困难了（重构误差又开始增加），这时候decoder就要想办法提高它的生成/泛化能力了。</li>
<li>重构的过程是希望没噪声的，而KL loss则希望有高斯噪声的，两者是对立的。所以VAE内部其实是包含了一个对抗的过程，只不过它们两者是混合起来，共同进化的。</li>
<li>变分：对泛函求极值</li>
</ul>
<p><strong>注：</strong><br>泛函：函数到数值的映射，如：KL散度<br>算子：函数到函数的映射，如：梯度</p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p><strong>维度灾难</strong></p>
<ul>
<li>随着特征数的增加，特征空间变得越来越稀疏，需要极多的数据才能有较好的估计效果。即增加维度有利于可分性，但不利于了解数据分布属性。</li>
<li>在样本不变的情况下，特征维度的增加会使得样本的分布越来越稀疏，虽然容易分开，但数据分布的可信度越来越低，越来越难以得到置信结论。</li>
</ul>
<p><strong>降维思路</strong></p>
<ul>
<li>各种降维都是在原空间中找一些重要的方向，使得数据在这些方向上的信息能够尽量代表整体信息；</li>
<li>寻找重要方向的过程往往都涉及矩阵的特征值分解，不同降维方法使用的矩阵不同，因此得到的方向不同。</li>
</ul>
<h3 id="PCA降维"><a href="#PCA降维" class="headerlink" title="PCA降维"></a>PCA降维</h3><ul>
<li>寻找数据方差最大的几个方向保留下来；</li>
<li>使用到的是数据的协方差矩阵。</li>
<li>核心优化目标：$J(w)=w^T S w$</li>
<li>换一个角度：将每个样本放成一行，得到矩阵，做奇异值分解，右奇异矩阵就是各个成分，左奇异矩阵与奇异值的乘积就是各个样本在各个成分上的分量。</li>
</ul>
<p><strong>奇异值分解（SVD）</strong></p>
<ul>
<li>将矩阵分解为左奇异矩阵+奇异值+右奇异矩阵；</li>
<li>是一种能够从非常本质的层面展现矩阵信息的工具；</li>
<li>左奇异矩阵：将原矩阵的行看作特征，列看作样本，得到投影方差降序递减方向向量，也是空间的一组标准正交基。</li>
<li>右奇异矩阵：将原矩阵的列看作特征，行看作样本，得到投影方差降序递减方向向量，也是空间的一组标准正交基。</li>
<li>奇异值：对应向量的方差。</li>
<li>矩阵运算就是做空间转换，例如：右乘向量就是对向量按右奇异矩阵的方向分解，并投影到对应的左奇异矩阵方向，同时乘以相应权重（奇异值）。</li>
</ul>
<h3 id="有监督降维"><a href="#有监督降维" class="headerlink" title="有监督降维"></a>有监督降维</h3><p><strong>LDA</strong></p>
<ul>
<li>在知道数据类别的时候进行降维，目标是使得类间方差尽量大，类内方差尽量小；</li>
<li>使用到的是类间协方差矩阵和类内协方差矩阵。<br>核心优化目标：$J(w)=\frac{w^T S_b w}{w^T S_w w}$</li>
</ul>
<h3 id="非线性降维"><a href="#非线性降维" class="headerlink" title="非线性降维"></a>非线性降维</h3><p><strong>自动编码器</strong></p>
<ul>
<li>通过重构误差最小来自动学习低维表示；</li>
<li>可以看成PCA的推广，当只有一个中间层，且不使用激活函数时，等价于PCA。</li>
</ul>
<p><strong>ISOMAP</strong></p>
<ul>
<li>计算流形上的相似度（最短路径），然后用MDS（多维缩放）来映射到低维；</li>
<li>效果受限于最近邻的计算。</li>
</ul>
<p><strong>LLE</strong><br>获取样本局部嵌入信息，并使低维表示保持这一信息。</p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><ul>
<li>集成学习与深度学习有异曲同工之妙，如：bagging类似于广度网络（dropout机制），boosting类似于深度残差网络，同时每一层都直连到输出。</li>
<li>许多模型如决策树、SVM、神经网络等都可以通过增加参数量等方式持续降低训练误差，但这种训练误差下降一般伴随着方差的明显增大，因为越往后会更多的拟合噪声；而集成学习的策略每次训练新的基学习器都会优先拟合重要特征，每个基学习器的参数量有限。</li>
</ul>
<h3 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h3><ul>
<li>相比深度学习，集成学习boosting的训练过程类似于逐层预训练，在优化上的困难要小得多，因此数据集适合集成学习时，集成学习比深度学习更有优势。同时集成学习的参数量往往要小得多，充分利用了参数的能力，且参数相互制衡，因此往往不容易过拟合。当然boosting不容易过拟合的一个重要保障是要用一些简单的模型作为基模型。</li>
<li>同时集成学习同时利用所有学习器的结果，类似于深度学习中的直连，将所有层都直接和输出连接。</li>
</ul>
<h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>bagging可以抑制过拟合、降低方差，提高模型稳定性，因此适合使用偏差小、方差大的模型（或者说能力比较强的模型）作为子模型。</p>
<h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><ul>
<li>训练新的分类器以降低训练误差，每次调整样本权重；</li>
<li>Adaboost（Adaptive Boosting）集成分类中，错误率越高的分类器，占最终分类器的比重越小，同时对于样本权重的调整越小，反之亦然。</li>
<li>Adaboost每一步都是计算前向分步算法的最优解：<br>$(\beta_m, \gamma_m)=argmin_{\beta,\gamma}\sum^N_{i=1} L(y_i,f_{m-1}(x_i)+\beta B(x_i|\gamma))$</li>
<li>每次增加新的学习器，训练误差严格下降。</li>
</ul>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><ul>
<li>bagging+决策树+自助采样法+随机子空间；</li>
<li>随机子空间：每一棵树随机采样一部分特征来使用，增加多样性，避免模型总是使用最强的几个特征，将数据解释限制在狭窄的范围内；</li>
</ul>
<h3 id="梯度提升树（GBT）"><a href="#梯度提升树（GBT）" class="headerlink" title="梯度提升树（GBT）"></a>梯度提升树（GBT）</h3><p><strong>提升树（BT）</strong>：</p>
<ul>
<li>boosting+决策树；</li>
<li>训练新的决策树来拟合现在模型的残差；</li>
<li>可以看成是将Adaboost中的分类器系数置为1.</li>
</ul>
<p><strong>梯度提升树</strong>：</p>
<ul>
<li>新的决策树拟合负梯度而不是残差，限制更少，应用场景更加广泛，损失函数选择范围更大；</li>
<li>负梯度和残差除了形式不同，也更加灵活，选择不同的损失函数可以调整优化偏好，如：平方损失函数倾向于先降低大的训练误差，绝对值损失函数倾向于带来样本训练误差的稀疏性；</li>
</ul>
<p><strong>梯度提升树的正则化</strong></p>
<ul>
<li>学习率：可以加入学习率控制学习进度；不直接使用新训练的树来更新模型，而是会加上一个较小的学习率来提高泛化误差。</li>
<li>子集采样：从原始数据集中不放回采样一个一个子集，引入随机性，虽然不是自助采样，但也是起到bagging效果；</li>
<li>最小叶节点样本数：限制叶节点至少有的样本数量，低于该数量事直接停止训练，降低过拟合；</li>
<li>Adaboost在训练结束前都可以保证训练误差下降，梯度提升树则不一定。</li>
</ul>
<h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3><p>相对于基本的梯度梯度提升树：</p>
<ul>
<li>将目标函数进行二阶泰勒展开，而非只使用一阶信息；</li>
<li>使用结构风险最小化，考虑节点数量和节点值大小；</li>
</ul>
<h4 id="分解结点"><a href="#分解结点" class="headerlink" title="分解结点"></a>分解结点</h4><p><strong>基本思路</strong><br>循环遍历所有特征的所有分裂点，计算能够带来的增益，选择增益最大的情况对结点进行分裂，如果增益都不大于0则停止。</p>
<h4 id="近似算法（优化）——分桶："><a href="#近似算法（优化）——分桶：" class="headerlink" title="近似算法（优化）——分桶："></a>近似算法（优化）——分桶：</h4><p>减少分裂次数，连续特征按连续值百分位分桶，离散特征按离散值分桶；</p>
<p><strong>全局模式</strong>：</p>
<ul>
<li>在算法开始时，对每个维度分桶一次，后续的分裂都依赖于该分桶并不再更新。</li>
<li>优点是：只需要计算一次，不需要重复计算。</li>
<li>缺点是：在经过多次分裂之后，叶结点的样本有可能在很多全局桶中是空的。</li>
</ul>
<p><strong>局部模式</strong>：</p>
<ul>
<li>除了在算法开始时进行分桶，每次拆分之后再重新分桶。</li>
<li>优点是：每次分桶都能保证各桶中的样本数量都是均匀的。</li>
<li>缺点是：计算量较大。</li>
</ul>
<h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul>
<li>学习率</li>
<li>随机选取特征</li>
</ul>
<h4 id="计算速度提升"><a href="#计算速度提升" class="headerlink" title="计算速度提升"></a>计算速度提升</h4><p><strong>预排序</strong></p>
<ul>
<li>在程序开始的时候对数据在每个特征，按数据的大小进行排序，这样在训练的时候就不用排序，可以避免大量重复劳动，同时因为每个特征之间是独立的，因此在寻找划分点的时候就可以并行执行。</li>
</ul>
<h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3><h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><p><strong>GBT的缺点</strong></p>
<ul>
<li>在构建子决策树时为了获取分裂点，需要在所有特征上扫描所有的样本，从而获得最大的信息增益。当样本的数量很大，或者样本的特征很多时，效率非常低。</li>
<li>同时GBT也无法使用类似mini batch方式进行训练。</li>
</ul>
<p><strong>xgboost的缺点</strong></p>
<ul>
<li>每轮迭代都需要遍历整个数据集多次。</li>
<li>如果把整个训练集装载进内存，则限制了训练数据的大小。如果不把整个训练集装载进内存，则反复读写训练数据会消耗非常大的IO时间。</li>
<li>空间消耗大。预排序（pre-sorted）需要保存数据的feature值，还需要保存feature排序的结果（如排序后的索引，为了后续的快速计算分割点）。因此需要消耗训练数据两倍的内存。</li>
<li>时间消耗大。为了获取分裂点，需要在所有特征上扫描所有的样本，从而获得最大的信息增益，时间消耗大。</li>
<li>对cache优化不友好，造成cache miss 。预排序后，feature对于梯度的访问是一种随机访问，并且不同feature访问的顺序不同，无法对cache进行优化。</li>
</ul>
<p><strong>LightGBM的优点</strong></p>
<ul>
<li>更快的训练效率；</li>
<li>低内存使用；</li>
<li>更高的准确率；</li>
<li>支持并行化学习；</li>
<li>可处理大规模数据。</li>
</ul>
<h4 id="代价优化策略："><a href="#代价优化策略：" class="headerlink" title="代价优化策略："></a>代价优化策略：</h4><p>减少训练样本的数量和减少样本的训练特征数量。</p>
<p><strong>Gradient-based One-Side Sampling(GOSS)</strong><br>基于梯度的采样。该方法用于减少训练样本的数量。</p>
<ul>
<li>传统采样方法采用随机丢弃的策略，而GOSS方法保留梯度较大的样本，随机丢弃梯度较小的样本。</li>
<li>为了不改变原数据的分布，GOSS在保留下来的小梯度样本上乘一个放大系数，以弥补被丢弃的小梯度样本。</li>
</ul>
<p><strong>Exclusive Feature Bundling(EFB)</strong><br>基于互斥特征的特征捆绑。该方法用于减少样本的特征。</p>
<ul>
<li>传统特征选取方法基于PCA的原理，认为许多特征包含重复信息，并以此选择重要特征或使用新特征，但实际场景往往难以使用。</li>
<li>EFB根据特征间的互斥性来将互斥的特征打包成一个新的，信息密度更大的特征。具体的如果对于所有样本，两个特征都不会同时为非零值，即认为两个特征互斥，实际中如果只有少量样本不满足也可以认为互斥。</li>
</ul>
<h4 id="其它优化策略"><a href="#其它优化策略" class="headerlink" title="其它优化策略"></a>其它优化策略</h4><p><strong>直方图</strong></p>
<ul>
<li>优点：<ul>
<li>节省空间：需要存储的信息更少</li>
<li>节省时间：分割点减少，也不用预排序</li>
<li>可能还自带正则化效果</li>
</ul>
</li>
<li>缺点：分割点不是很精确</li>
</ul>
<p><strong>leaf-wise生长策略</strong></p>
<ul>
<li>相对level-wise可以避免很多不必要的分裂；</li>
<li>容易过拟合一些，需要限制最大深度。</li>
</ul>
<p><strong>直方图做差加速</strong></p>
<ul>
<li>通常构造直方图，需要遍历该叶子上的所有数据。但是事实上一个叶子的直方图可以由它的父亲结点的直方图与它兄弟的直方图做差得到。</li>
<li>LightGBM在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</li>
</ul>
<p><strong>直接支持categorical特征</strong></p>
<h2 id="经典机器学习算法"><a href="#经典机器学习算法" class="headerlink" title="经典机器学习算法"></a>经典机器学习算法</h2><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>通过在全局中找到待测样本最近的K个训练样本来得到待测样本的类别或预测值</p>
<ul>
<li>严格来说不算机器学习，因为没有学习的过程，而是记忆所有训练样本，没有得到泛化信息；</li>
<li>预测时的复杂度高</li>
<li>KNN是非参数化的局部模型，消极（懒惰）学习，性能取决于K的大小和距离度量选择。</li>
<li>KNN的模型复杂度和K成反比，K越小越复杂，K越大时平均程度越明显，即正则化力度越大。</li>
<li>KNN中距离度量的选择是一个问题，连续数据常见的欧氏距离、曼哈顿距离、马氏距离等。</li>
</ul>
<p><strong>KD树</strong><br>在原始KNN的基础上加入了学习过程</p>
<ul>
<li>训练：通过垂直于坐标轴的超平面不断对训练样本进行二分的划分，得到一颗KD决策树；</li>
<li>测试：将待测样本的值在KD树上进行比较，逐步找到最近邻的样本；<ul>
<li>平均计算复杂度为$O(log(N))$，大部分时候只需要较少的比较就可以得到最近邻；</li>
<li>当训练样本分布较糟糕时，需要遍历几乎所有节点。</li>
</ul>
</li>
</ul>
<p><strong>压缩KNN</strong><br>从训练样本中找出对于分类/决策比较重要的样本，类似SVM中的支持向量</p>
<ul>
<li>训练：<ul>
<li>选择一个样本放入样本集；</li>
<li>循环所有样本，如果分类正确则不取，分类错误则放入样本集，直到所有样本都分类正确。</li>
</ul>
</li>
<li>测试：使用训练得到的样本集进行测试，而非所有样本。</li>
</ul>
<h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p><strong>优点</strong></p>
<ul>
<li>模型简单；</li>
<li>可解释性强，权重向量直观地表达了各个特征在预测中的重要性；</li>
<li>很多功能强大的非线性模型可以在线性模型的基础上通过引入层级结构和非线性映射得到。</li>
</ul>
<p><strong>线性模型&amp;非线性模型</strong></p>
<ul>
<li>分类模型关键看决策面是否是线性的，逻辑回归所回归的概率值与输入是非线性关系（几率值与输入是指数线性关系），但通过概率值得到的分类决策面是线性的，所以作为分类模型，逻辑回归是线性的。</li>
</ul>
<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><ul>
<li>线性一方面是输入特征与其它参数的数学组合方式，另一方面是它所提供的决策边界（回归线）是线性的。</li>
<li>优势是：简单、解释性强、可以通过一些策略（kernel、层叠）简单转化为非线性模型。</li>
</ul>
<blockquote>
<p>模型：$y= \vec{x}^T\vec{w}$<br>数据：${ X,\vec{y} }$<br>损失函数：$L=(\vec{y}-X^T\vec{w})^2$<br>闭式解：$\vec{w}=(XX^T)^{-1}X\vec{y}$</p>
</blockquote>
<ul>
<li>当数据量小的时候$XX^T$不满秩，没有唯一解，可以加入L1、L2等正则化方式来获得唯一解</li>
<li>也可以用梯度下降来求解。</li>
</ul>
<h4 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h4><ul>
<li>线性回归+sigmoid函数，广义线性模型；</li>
<li>将线性回归的值域从无穷映射到$[0,1]$区间，并将其解释为样本为某一类的概率；因此逻辑回归是概率模型，用于分类问题。</li>
<li>逻辑回归学习的是决策边界，而不是数据分布，因此是判别模型。</li>
</ul>
<h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><ul>
<li>线性回归+符号函数</li>
<li>优化目标是感知准则而非误分类率：<ul>
<li>误分类率：分段常数函数，每一种分类结果的分类边界都不唯一，无法给出优化方向；</li>
<li>感知准则：自适应，适合动态学习；</li>
</ul>
</li>
</ul>
<h3 id="概率图"><a href="#概率图" class="headerlink" title="概率图"></a>概率图</h3><h4 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h4><ul>
<li>贝叶斯定理+特征条件独立假设；</li>
<li>朴素贝叶斯的基础是贝叶斯定理，即通过贝叶斯定理计算样本为各个类别的后验概率；</li>
<li>朴素贝叶斯朴素在特征条件独立假设，通过该假设，极大地降低参数量（将参数量从指数级降为线性级）。</li>
</ul>
<p><strong>贝叶斯公式</strong></p>
<ul>
<li>角度一：条件概率+全概率公式</li>
<li>角度二：用先验概率和具体事件表出后验概率</li>
</ul>
<p><strong>朴素贝叶斯分类器</strong></p>
<ul>
<li>利用贝叶斯定理来实现分类任务，先验得知道各类（输出）的概率以及在各类下事件（输入）的概率，就可以得到在发生事件（输入）时，各类（输出）的概率。</li>
<li>由于数据样本的数量与特征维度往往不匹配，参数量指数级，无法有效得到各类下事件（输入）的概率，因此朴素贝叶斯对条件概率做了特征独立性假设。</li>
<li>这意味着在分类确定的条件下，用于分类的特征是条件独立的。</li>
<li>该假设使得朴素贝叶斯法变得简单，参数量由指数级变为线性级，但是可能牺牲一定的分类准确率。</li>
<li>准确率牺牲的程度依赖实际特征的关联程度。</li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li>性能相当好，它速度快，可以避免维度灾难。</li>
<li>支持大规模数据的并行学习，且天然的支持增量学习。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>输出概率不一定准确；</li>
</ul>
<h4 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h4><ul>
<li>在众多的随机变量中，通过条件独立性假设来降低参数量，得到随机变量的有向无环图；</li>
<li>可以用于因果推断；</li>
<li>有向边表示依赖关系，入度为0的节点事件不受其他随机变量影响，入度不为0的节点概率为条件概率的形式；</li>
</ul>
<h4 id="马尔科夫网络"><a href="#马尔科夫网络" class="headerlink" title="马尔科夫网络"></a>马尔科夫网络</h4><ul>
<li>无向图，可以对关系进行建模，不能用于因果推断</li>
</ul>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>树的每一个根节点通过一个特征对样本进行区分，叶节点的数据归为一类或一个输出值。</p>
<p><strong>信息熵</strong>是用来衡量信息不确定性的指标:</p>
<ul>
<li><p>对于分布$p(y)$， 信息熵为$H(y)=-\sum_y p(y)\log{p(y)}$</p>
</li>
<li><p>在具体数据集$D$上，经验分布熵为$H(D)=-\sum^{K}_{k=1}\frac{N_k}{N}\log{\frac{N_k}{N}}$</p>
</li>
<li><p>也可以用基尼系数$1-\sum_y{p(y)}^2$</p>
</li>
</ul>
<p><strong>条件熵</strong>：加入条件（如某个数据的特征），当条件取值不同时，数据子集有不同的信息熵，其期望就是条件熵。</p>
<ul>
<li><p>$H(y|A)=\sum_A p(A)H(y|A)=-\sum_A[p(A)\sum_{y_A} p(y_A)\log{p(y_A)}]$</p>
</li>
<li><p>经验条件熵：$H(D|A)=-\sum_A[\frac{N_A}{N}\sum^{K_A}_{k=1}\frac{N_k}{N_A}\log{\frac{N_k}{N_A}}]$</p>
</li>
</ul>
<p><strong>信息增益</strong>:</p>
<ul>
<li>通过加入特征使得数据分布更加确定，即该特征为分布带来了信息增益：$g(D,A)=H(D)-H(D|A)$；</li>
<li>ID3使用信息增益作为特征选择策略。</li>
</ul>
<p><strong>信息增益比</strong>:</p>
<ul>
<li>直接使用信息增益会倾向于使用取值较多的特征，如ID特征，信息增益比是在信息增益的基础上用特征取值的信息熵来标准化：$g_r(D,A)=\frac{g(D,A)}{H_A(D)}=\frac{H(D)-H(D|A)}{H_A(D)}$；</li>
<li>C4.5使用信息增益作为特征选择策略。</li>
</ul>
<p><strong>基尼系数</strong></p>
<ul>
<li>$gini(y)=1-\sum_yy^2$：也常用来代替信息熵，趋势以及计算值与信息熵都非常类似；</li>
</ul>
<p><strong>回归树方差</strong>：</p>
<ul>
<li>前面的指标运用于分类问题中的信息不确定度计算，在回归问题中可以使用方差来表示不确定性，对应的有方差、条件方差、方差增益。</li>
</ul>
<p><strong>决策树&amp;贝叶斯定理&amp;朴素贝叶斯分类器</strong></p>
<ul>
<li>决策树的判定方式也可以解释为是用贝叶斯定理（后验概率），与朴素贝叶斯分类器的条件概率特征独立性假设不同的是，决策树依据最优特征选择，使用部分特征来计算后验概率。</li>
</ul>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><ul>
<li>支持向量机：<strong>间隔、对偶、核技巧</strong></li>
<li>SVM支持向量机抛开概率视角，化繁为简，从<strong>几何</strong>的角度来看待问题。</li>
<li>机器学习最终是看<strong>泛化误差</strong>而不是训练误差，<strong>最大化间隔</strong>就是从几何角度尝试最大化泛化误差。</li>
<li>支持向量机的误差包括两个部分：一个是<strong>分界面的不置信度</strong>，几何间隔越小越不置信；另一个是<strong>训练误差</strong>，越多的点被误分类训练误差越大。硬间隔要求后者为0，所以只包含前者。</li>
<li>SVM通过<strong>对偶</strong>问题来求解最优边界，交换最大最小过程。</li>
<li><strong>梅塞尔定理</strong>：只要满足对称和半正定的条件就可以作为核函数。</li>
<li>平稳核函数：值只与两个点的差（相对位置）有关。</li>
<li>径向基核函数：值与两个点的方向也无关，只与差的范数有关，如高斯核函数。</li>
<li>核函数的本质在于相似度计算，找出几何上决定边界的样本点（支持向量），对于新的样本点，就计算它们同支持向量的相似度，进而决定应该属于那个类别。</li>
<li>将核函数用于概率密度函数估计就是<strong>核密度估计方法</strong>。</li>
</ul>
<p><strong>特征空间维度</strong></p>
<ul>
<li>因为核技巧隐藏了映射的细节，只知道内积运算，所以特征空间是希尔伯特空间，维度是无限的。</li>
<li>但是仅从决策函数看，也可以说特征空间的维度是有限的且等于训练样本的数量，每一个支持样本对应一个维度，决策时计算出测试点在各个维度上的得分累加进行分类。</li>
<li>非线性SVM可以看成是一种特殊的神经网络，即输入层对应输入空间，隐藏层对应特征空间，判决分数对应输出层。各层神经元个数分别为输入数据维度、训练样本（或支持向量）维度、1。如果核函数选的合适就是一个广度网络。</li>
<li>另外希尔伯特空间本身就是去掉维度的概念，所以也可以说是没有维度。</li>
</ul>
<p><strong>理论上SVM的目标函数可以使用梯度下降法来训练。但存在三个问题</strong></p>
<ul>
<li>合页损失函数部分不可导。这可以通过sub-gradient descent 来解决。</li>
<li>收敛速度非常慢。</li>
<li>无法得出支持向量和非支持向量的区别。</li>
</ul>
<p><strong>常用核函数</strong></p>
<ul>
<li>多项式核函数；</li>
<li>高斯核函数；</li>
</ul>
<p><strong>支持向量机的优点</strong></p>
<ul>
<li>有严格的数学理论支持，可解释性强。</li>
<li>能找出对任务至关重要的关键样本（即：支持向量）。</li>
<li>采用核技巧之后，可以处理非线性分类/回归任务。</li>
</ul>
<p><strong>支持向量机的缺点</strong></p>
<ul>
<li>训练时间长。</li>
<li>当采用核技巧时，如果需要存储核矩阵，则空间复杂度为平方。</li>
<li>模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。</li>
<li>因此支持向量机目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务。</li>
</ul>
<p><strong>SVM vs KNN</strong></p>
<ul>
<li>使用核函数的SVM同KNN类似，SVM可以看成特殊的KNN，最后预测都是计算待测样本到一些点的距离然后判断类别；不同的是KNN是懒惰的学习，或没有学习，因此预测的时候要从所有的样本计算最近的样本加以判别，而SVM在训练的过程中就找出了支持向量，因此判决过程更简单，SVM是不同支持向量的软的加权，不是离散地、硬的计数。</li>
<li>KNN也有压缩近邻法，也可以看成是在寻找支持向量；方法是将分错的样本留下以保证其它样本都还可以分对，最后的效果是保留下来的样本点主要是决策边界附近的点。</li>
</ul>
<h2 id="深度学习算法"><a href="#深度学习算法" class="headerlink" title="深度学习算法"></a>深度学习算法</h2><h3 id="Graph-embedding"><a href="#Graph-embedding" class="headerlink" title="Graph embedding"></a>Graph embedding</h3><p>从图总提取信息，用一个向量来表示一个节点。</p>
<p><strong>DeepWalk</strong></p>
<ul>
<li>随机游走： 对每个节点在图上进行随机游走，生成图上的序列，通过Word2vector的方式生成embedding。</li>
</ul>
<p><strong>LINE</strong></p>
<ul>
<li>一阶相似性：相互连接的节点相似；一条边的两节点embedding內积求sigmoid作为联合概率分布，不同边的联合概率分布按边权重加权作为loss。</li>
<li>二阶相似性：相邻接点相似的节点相似；已知一个节点时，计算其它节点的条件概率，按边的权重加权作为loss。</li>
<li>将一阶和二阶embedding直接拼接得到最终的embedding。</li>
</ul>
<p><strong>node2vector</strong></p>
<ul>
<li>同质性：相近的节点相似</li>
<li>结构等价性：连接结构相似的节点相似</li>
<li>按一定概率游走，调整参数使模型倾向于学习不同信息；设置p、q的值，控制游走的方向倾向于在附近游走还是在远处游走。  </li>
</ul>
<p><strong>struc2vector</strong></p>
<ul>
<li>两个节点的相似性通过两个节点的n-hop邻居的度的相似性的判断，也就是两个节点的结构。</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">万川</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://notego.top/2021/03/31/ji-qi-xue-xi/">http://notego.top/2021/03/31/ji-qi-xue-xi/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">万川</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%AE%97%E6%B3%95/">
                                    <span class="chip bg-color">算法</span>
                                </a>
                            
                                <a href="/tags/%E6%A8%A1%E5%9E%8B/">
                                    <span class="chip bg-color">模型</span>
                                </a>
                            
                                <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">监督学习</span>
                                </a>
                            
                                <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">无监督学习</span>
                                </a>
                            
                                <a href="/tags/%E7%89%B9%E5%BE%81/">
                                    <span class="chip bg-color">特征</span>
                                </a>
                            
                                <a href="/tags/%E8%BF%87%E6%8B%9F%E5%90%88/">
                                    <span class="chip bg-color">过拟合</span>
                                </a>
                            
                                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">机器学习</span>
                                </a>
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                                <a href="/tags/%E6%A6%82%E7%8E%87/">
                                    <span class="chip bg-color">概率</span>
                                </a>
                            
                                <a href="/tags/%E6%B3%9B%E5%8C%96/">
                                    <span class="chip bg-color">泛化</span>
                                </a>
                            
                                <a href="/tags/%E5%81%8F%E5%B7%AE/">
                                    <span class="chip bg-color">偏差</span>
                                </a>
                            
                                <a href="/tags/%E6%96%B9%E5%B7%AE/">
                                    <span class="chip bg-color">方差</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你好，同行的有缘人</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/03/31/zhuang-zi-bi-ji/">
                    <div class="card-image">
                        
                        <img src="/medias/image/%E3%80%8A%E5%BA%84%E5%AD%90%E3%80%8B.jpg" class="responsive-img" alt="《庄子》笔记">
                        
                        <span class="card-title">《庄子》笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            唯有超越，才能获得真正的自由
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-03-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-category">
                                    读书笔记
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%93%B2%E5%AD%A6/">
                        <span class="chip bg-color">哲学</span>
                    </a>
                    
                    <a href="/tags/%E9%81%93%E5%AE%B6/">
                        <span class="chip bg-color">道家</span>
                    </a>
                    
                    <a href="/tags/%E9%81%93/">
                        <span class="chip bg-color">道</span>
                    </a>
                    
                    <a href="/tags/%E8%87%AA%E7%84%B6/">
                        <span class="chip bg-color">自然</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A9%E4%BA%BA%E5%90%88%E4%B8%80/">
                        <span class="chip bg-color">天人合一</span>
                    </a>
                    
                    <a href="/tags/%E9%80%8D%E9%81%A5/">
                        <span class="chip bg-color">逍遥</span>
                    </a>
                    
                    <a href="/tags/%E5%85%BB%E7%94%9F/">
                        <span class="chip bg-color">养生</span>
                    </a>
                    
                    <a href="/tags/%E8%BE%A9%E8%AF%81/">
                        <span class="chip bg-color">辩证</span>
                    </a>
                    
                    <a href="/tags/%E6%97%A0%E5%BE%85/">
                        <span class="chip bg-color">无待</span>
                    </a>
                    
                    <a href="/tags/%E5%B0%8F%E5%A4%A7%E4%B9%8B%E8%BE%A9/">
                        <span class="chip bg-color">小大之辩</span>
                    </a>
                    
                    <a href="/tags/%E9%B9%8F/">
                        <span class="chip bg-color">鹏</span>
                    </a>
                    
                    <a href="/tags/%E8%B6%85%E8%B6%8A/">
                        <span class="chip bg-color">超越</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/03/31/sql/">
                    <div class="card-image">
                        
                        <img src="/medias/image/SQL.jpg" class="responsive-img" alt="SQL">
                        
                        <span class="card-title">SQL</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-03-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" class="post-category">
                                    编程语言
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/SQL/">
                        <span class="chip bg-color">SQL</span>
                    </a>
                    
                    <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/">
                        <span class="chip bg-color">数据库</span>
                    </a>
                    
                    <a href="/tags/%E4%B8%BB%E9%94%AE/">
                        <span class="chip bg-color">主键</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('20')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 方寸田园<br />'
            + '文章作者: 万川<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h1, h2, h3, h4, h5'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4, h5').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2021-2022</span>
            
            <span id="year">2021</span>
            <a href="/about" target="_blank">万川</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">134.9k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2021";
                    var startMonth = "3";
                    var startDate = "31";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/wanc97" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:wanc97@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=3221927185" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 3221927185" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz=MzI4MzEzNjIxNw==#wechat_redirect" class="tooltipped" target="_blank" data-tooltip="微信联系我: W3221927185" data-position="top" data-delay="50">
        <i class="fab fa-weixin"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

    <!-- 雪花特效 --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</body>

</html>
